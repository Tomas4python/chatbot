{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea64501-001d-4714-93db-6af0f755d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install scikit-learn\n",
    "# !pip install nltk\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90199c4d-e447-46b3-9c7a-e633a3c02f09",
   "metadata": {},
   "source": [
    "# CONVERSATION GENERATOR (CHATBOT) SEQUENCE 2 SEQUENCE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14260f0-4962-42e2-8527-f5189a158c76",
   "metadata": {
    "id": "b0ZiOK0MWE7d"
   },
   "source": [
    "## GPU info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f70056-2151-4051-8974-714b3e237e43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1715927490489,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "bhvFI7ztVLRy",
    "outputId": "e4fb02b2-7ebf-4f3e-b2a8-f9f189a25c47"
   },
   "outputs": [],
   "source": [
    "# gpu_info = !nvidia-smi\n",
    "# gpu_info = '\\n'.join(gpu_info)\n",
    "# if gpu_info.find('failed') >= 0:\n",
    "#   print('Not connected to a GPU')\n",
    "# else:\n",
    "#   print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79a58ec6-07dc-4261-ae09-67ab59ed3fb6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23496,
     "status": "ok",
     "timestamp": 1715927517410,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "RMth1TfbAys6",
    "outputId": "766cb4f5-edfa-4cd7-ad09-825604e4c786"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2d5d74b-5e87-4198-b112-9f91b7f65d0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4075,
     "status": "ok",
     "timestamp": 1715927650106,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "db509470-cb51-4c21-84ca-5f928073d68e",
    "outputId": "95b90920-4333-45db-f52c-0f88f1ec6a5e"
   },
   "outputs": [],
   "source": [
    "# # Check if GPU is available\n",
    "# from tensorflow.python.client import device_lib\n",
    "\n",
    "# def get_gpu_details():\n",
    "#     devices = device_lib.list_local_devices()\n",
    "#     for device in devices:\n",
    "#         if device.device_type == 'GPU':\n",
    "#             print(f\"Device Name: {device.name}\")\n",
    "#             print(f\"Memory Limit: {device.memory_limit} bytes\")\n",
    "#             print(f\"Description: {device.physical_device_desc}\")\n",
    "\n",
    "# get_gpu_details()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72de3458-7b40-491d-a66f-d8355a455243",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1715927650106,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "6648d492-f4e0-40a8-a3ff-012fa0a0d293"
   },
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfe7f8ff-d35e-416c-8252-fc19fc3570e2",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1715927650106,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "8a0af574-6748-467b-8ba6-4da51794c5b9"
   },
   "outputs": [],
   "source": [
    "# !pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50d83013-6a91-4f74-bee7-da28b5c4f085",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1715927650106,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "7e4e2a98-f1d4-4f72-b95e-6a75f457e8e8"
   },
   "outputs": [],
   "source": [
    "# !pip install fastparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0510248e-c233-485a-b707-7a7eb9856939",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35a59de0-817a-4089-9a14-0b45925b6298",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 15 # Length of input and target sequences, padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c8162-54ab-45da-b4f0-2d5472decdd8",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e231807-af60-4571-be49-655d80d8eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, Concatenate, Layer, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4bc7c4b1-9874-48b2-93f3-0b151ccf06f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "TensorFlow Keras version: 3.3.3\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"TensorFlow Keras version:\", tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e56f5d37-9a67-4e51-b0b8-404df9d734a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1715930801381,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "y5srV6bjOpFb",
    "outputId": "b74db1e8-1afe-4348-be1a-23c150492567"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')  # Tokenizer\n",
    "nltk.download('wordnet')  # Lemmatizer\n",
    "nltk.download('stopwords')  # Stopwords\n",
    "nltk.download('omw-1.4') # Ensures multilingual contexts\n",
    "\n",
    "# Stopwords list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "initial_preprocessing = True\n",
    "\n",
    "# Load spaCy's English NLP model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b393a656-df75-4deb-af28-d8ba36f526eb",
   "metadata": {
    "id": "f5150d2f-999d-45ad-b2f2-b9b65b883e89"
   },
   "source": [
    "## Create prepocessing functions for initial text and later response generation preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75edfc9e-3560-4ab7-9992-963b8df9f537",
   "metadata": {
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1715930818326,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "f0e8f237-5274-43ee-8ac2-964dd9214ae5"
   },
   "outputs": [],
   "source": [
    "contractions = {\n",
    "    \"’\": \"'\",\n",
    "    \"‘\": \"'\",\n",
    "    \"“\": '\"',\n",
    "    \"”\": '\"',\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"n't\": \" not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"thats's\": \"that is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"that'll\": \"that will\",\n",
    "    \"there'll\": \"there will\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"ain't\": \"is not\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"daren't\": \"dare not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"usedn't\": \"used not\"\n",
    "}\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    # Normalize Unicode string to NFKD form, remove non-ASCII characters, and then decode it back to a UTF-8 string\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove spaces around apostrophes\n",
    "    text = re.sub(r\"\\s*'\\s*\", \"'\", text)\n",
    "    # Add a space before and after any punctuation mark (., !, or ?)\n",
    "    text = re.sub(r\"\\s*([.!?])\\s*\", r\" \\1 \", text)\n",
    "    # Correct contractions\n",
    "    for contraction, replacement in contractions.items():\n",
    "        text = re.sub(re.escape(contraction), replacement, text)\n",
    "    # Replace any sequence of characters that are not letters, basic punctuation\n",
    "    text = re.sub(r\"[^a-z' ]\", ' ', text) # re.sub(r\"[^a-z.,'!? ]\", ' ', text)\n",
    "    # Replace any sequence of whitespace characters with a single space and remove leading and trailing whitespace\n",
    "    text = re.sub(r\"\\s+\", ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_names(text: str) -> str:\n",
    "    # Use spaCy to detect and remove names from the text\n",
    "    doc = nlp(text)\n",
    "    filtered_text = ' '.join([token.text for token in doc if token.ent_type_ != 'PERSON']) # Takes really long time, exlude from chatbot input preprocessing\n",
    "    return filtered_text\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # Normalize text\n",
    "    text = normalize_text(text)\n",
    "    # Remove names using spaCy's NER\n",
    "    if initial_preprocessing:\n",
    "        text = remove_names(text)\n",
    "    # # Remove punctuation\n",
    "    # text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stopwords and tokenize\n",
    "    # words = word_tokenize(text) # More intelligent splitting\n",
    "    # filtered_words = [word for word in words if word not in stop_words]\n",
    "    # # Lemmatize words\n",
    "    # lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    # Add <SOS> and <EOS> tokens, and join the list into a single string\n",
    "    # return ' '.join(['sofs'] + lemmatized_words + ['eofs'])\n",
    "        # Trim the text to the desired length\n",
    "    words = text.split()[:max_length]\n",
    "    trimmed_text = ' '.join(words)  # Consider to remove trimming, if you want pad later on max length\n",
    "    return trimmed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13a57e-6fc6-48ad-8326-becd2e579083",
   "metadata": {},
   "source": [
    "### Load data\n",
    "https://huggingface.co/datasets/daily_dialog/tree/refs%2Fconvert%2Fparquet/default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17b8c9fb-fe6c-4bca-a255-45e7c9a9a831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of data/0000.parquet:\n",
      "                                              dialog  \\\n",
      "0  [Say , Jim , how about going for a few beers a...   \n",
      "1  [Can you do push-ups ? ,  Of course I can . It...   \n",
      "2  [Can you study with the radio on ? ,  No , I l...   \n",
      "3  [Are you all right ? ,  I will be all right so...   \n",
      "4  [Hey John , nice skates . Are they new ? ,  Ye...   \n",
      "\n",
      "                              act                         emotion  \n",
      "0  [3, 4, 2, 2, 2, 3, 4, 1, 3, 4]  [0, 0, 0, 0, 0, 0, 4, 4, 4, 4]  \n",
      "1              [2, 1, 2, 2, 1, 1]              [0, 0, 6, 0, 0, 0]  \n",
      "2                 [2, 1, 2, 1, 1]                 [0, 0, 0, 0, 0]  \n",
      "3                    [2, 1, 1, 1]                    [0, 0, 0, 0]  \n",
      "4     [2, 1, 2, 1, 1, 2, 1, 3, 4]     [0, 0, 0, 0, 0, 6, 0, 6, 0]   \n",
      "\n",
      "Contents of data/default_validation_0000.parquet:\n",
      "                                              dialog  \\\n",
      "0  [Good morning , sir . Is there a bank near her...   \n",
      "1  [Good afternoon . This is Michelle Li speaking...   \n",
      "2  [What qualifications should a reporter have ? ...   \n",
      "3  [Hi , good morning , Miss ? what can I help yo...   \n",
      "4  [Excuse me , ma'am . Can you tell me where the...   \n",
      "\n",
      "                                                act  \\\n",
      "0                             [2, 1, 3, 2, 1, 2, 1]   \n",
      "1                    [2, 1, 1, 1, 1, 2, 3, 2, 3, 4]   \n",
      "2                                      [2, 1, 2, 1]   \n",
      "3  [2, 3, 2, 2, 1, 2, 1, 2, 1, 1, 1, 3, 2, 1, 3, 4]   \n",
      "4                          [3, 4, 2, 1, 2, 1, 1, 1]   \n",
      "\n",
      "                                            emotion  \n",
      "0                             [0, 0, 0, 0, 0, 0, 0]  \n",
      "1                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "2                                      [0, 0, 0, 0]  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]  \n",
      "4                          [0, 0, 0, 0, 0, 0, 4, 4]   \n",
      "\n",
      "Contents of data/default_test_0000.parquet:\n",
      "                                              dialog  \\\n",
      "0  [Hey man , you wanna buy some weed ? ,  Some w...   \n",
      "1  [The taxi drivers are on strike again . ,  Wha...   \n",
      "2  [We've managed to reduce our energy consumptio...   \n",
      "3  [Believe it or not , tea is the most popular b...   \n",
      "4  [What are your personal weaknesses ? ,  I ’ m ...   \n",
      "\n",
      "                                          act  \\\n",
      "0        [3, 2, 3, 4, 3, 4, 3, 2, 3, 4, 2, 3]   \n",
      "1                                [1, 2, 1, 1]   \n",
      "2                       [1, 2, 1, 2, 1, 2, 1]   \n",
      "3  [1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 3, 4, 3]   \n",
      "4                    [2, 1, 2, 1, 2, 1, 2, 1]   \n",
      "\n",
      "                                      emotion  \n",
      "0        [0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0]  \n",
      "1                                [0, 0, 0, 0]  \n",
      "2                       [0, 0, 0, 0, 0, 0, 0]  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 4]  \n",
      "4                    [0, 0, 0, 0, 0, 0, 0, 4]   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_parquet_files(file_paths: Dict[str, str]) -> Dict[str, pd.DataFrame]:\n",
    "    dataframes = {}\n",
    "    for key, file_path in file_paths.items():\n",
    "        try:\n",
    "            df = pd.read_parquet(file_path)\n",
    "            dataframes[key] = df\n",
    "            print(f\"Contents of {file_path}:\")\n",
    "            print(df.head(), \"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while loading {file_path}: {e}\")\n",
    "    return dataframes\n",
    "\n",
    "file_paths = {\n",
    "    'train': 'data/0000.parquet',\n",
    "    'validation': 'data/default_validation_0000.parquet',\n",
    "    'test': 'data/default_test_0000.parquet'\n",
    "}\n",
    "\n",
    "dataframes = load_parquet_files(file_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631dd47f-5120-435e-b7ae-27fde4ee31df",
   "metadata": {},
   "source": [
    "### Review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d739cdf-8425-416e-a55a-3e502c4b70d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation', 'test'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c79dba0-1db0-4f10-a3e2-65098b1c9d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11118 1000 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialog</th>\n",
       "      <th>act</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Say , Jim , how about going for a few beers a...</td>\n",
       "      <td>[3, 4, 2, 2, 2, 3, 4, 1, 3, 4]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 4, 4, 4, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Can you do push-ups ? ,  Of course I can . It...</td>\n",
       "      <td>[2, 1, 2, 2, 1, 1]</td>\n",
       "      <td>[0, 0, 6, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Can you study with the radio on ? ,  No , I l...</td>\n",
       "      <td>[2, 1, 2, 1, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Are you all right ? ,  I will be all right so...</td>\n",
       "      <td>[2, 1, 1, 1]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Hey John , nice skates . Are they new ? ,  Ye...</td>\n",
       "      <td>[2, 1, 2, 1, 1, 2, 1, 3, 4]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 6, 0, 6, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11113</th>\n",
       "      <td>[Hello , I bought a pen in your shop just befo...</td>\n",
       "      <td>[1, 1, 1, 2, 3, 2, 1, 4, 1]</td>\n",
       "      <td>[0, 4, 0, 0, 0, 0, 0, 0, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11114</th>\n",
       "      <td>[Do you have any seats available ? ,  Yes . Th...</td>\n",
       "      <td>[2, 1, 2, 1, 3, 4]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11115</th>\n",
       "      <td>[Uncle Ben , how did the Forbidden City get th...</td>\n",
       "      <td>[2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 3, 4]</td>\n",
       "      <td>[0, 0, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11116</th>\n",
       "      <td>[May I help you , sir ? ,  I want a pair of lo...</td>\n",
       "      <td>[2, 3, 4, 3]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11117</th>\n",
       "      <td>[Could I have the check , please ? ,  Okay . I...</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>[0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11118 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  dialog  \\\n",
       "0      [Say , Jim , how about going for a few beers a...   \n",
       "1      [Can you do push-ups ? ,  Of course I can . It...   \n",
       "2      [Can you study with the radio on ? ,  No , I l...   \n",
       "3      [Are you all right ? ,  I will be all right so...   \n",
       "4      [Hey John , nice skates . Are they new ? ,  Ye...   \n",
       "...                                                  ...   \n",
       "11113  [Hello , I bought a pen in your shop just befo...   \n",
       "11114  [Do you have any seats available ? ,  Yes . Th...   \n",
       "11115  [Uncle Ben , how did the Forbidden City get th...   \n",
       "11116  [May I help you , sir ? ,  I want a pair of lo...   \n",
       "11117  [Could I have the check , please ? ,  Okay . I...   \n",
       "\n",
       "                                                    act  \\\n",
       "0                        [3, 4, 2, 2, 2, 3, 4, 1, 3, 4]   \n",
       "1                                    [2, 1, 2, 2, 1, 1]   \n",
       "2                                       [2, 1, 2, 1, 1]   \n",
       "3                                          [2, 1, 1, 1]   \n",
       "4                           [2, 1, 2, 1, 1, 2, 1, 3, 4]   \n",
       "...                                                 ...   \n",
       "11113                       [1, 1, 1, 2, 3, 2, 1, 4, 1]   \n",
       "11114                                [2, 1, 2, 1, 3, 4]   \n",
       "11115  [2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 3, 4]   \n",
       "11116                                      [2, 3, 4, 3]   \n",
       "11117                                            [3, 4]   \n",
       "\n",
       "                                                emotion  \n",
       "0                        [0, 0, 0, 0, 0, 0, 4, 4, 4, 4]  \n",
       "1                                    [0, 0, 6, 0, 0, 0]  \n",
       "2                                       [0, 0, 0, 0, 0]  \n",
       "3                                          [0, 0, 0, 0]  \n",
       "4                           [0, 0, 0, 0, 0, 6, 0, 6, 0]  \n",
       "...                                                 ...  \n",
       "11113                       [0, 4, 0, 0, 0, 0, 0, 0, 4]  \n",
       "11114                                [0, 0, 0, 0, 0, 4]  \n",
       "11115  [0, 0, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0]  \n",
       "11116                                      [0, 0, 0, 0]  \n",
       "11117                                            [0, 0]  \n",
       "\n",
       "[11118 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = dataframes['train']\n",
    "val_df = dataframes['validation']\n",
    "test_df = dataframes['test']\n",
    "print(len(train_df),len(val_df), len(test_df))\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4defcd4b-38db-4f28-8cbc-3a27bd2db229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Can you tell me where the pots and pans are ? '\n",
      " ' Pots and pans are right over there . ' ' Oh , thank you . '\n",
      " ' Could I interest you in our store credit card ? '\n",
      " ' No , thanks . I already have credit cards . '\n",
      " ' But our credit card saves you 10 percent . '\n",
      " \" That's a nice discount . \"\n",
      " ' Here . Let me give you an application form . '\n",
      " \" Thank you , but I'm just browsing today . \"\n",
      " ' Okay . Enjoy your browsing . ']\n",
      "['Here is the fish counter . Look at the lobsters and crabs . Shall we have some ? '\n",
      " \" I'm allergic to these things , you know . \"\n",
      " ' Sorry , I forgot . I don ’ t like seafood , neither . '\n",
      " ' Let ’ s go over there and get some milk , a couple dozen eggs and some orange juice . '\n",
      " \" Let's get frozen juice . It is really good . We ’ Ve got enough food . Let ’ s go over to the check-out stand . \"\n",
      " ' OK . But just let me pick up a bottle of cooking wine and oil as we go by . ']\n",
      "['Good morning , may I speak with Professor Clark , please ? '\n",
      " ' You are speaking with Professor Clark . '\n",
      " ' Professor , I am Kalina from your morning literature class . '\n",
      " ' Yes , how can I help you ? '\n",
      " ' I ran my car into a tree yesterday and need to miss a few days of school . '\n",
      " ' Oh , my God ! I hope you are all right . '\n",
      " ' I have a concussion , but I will be OK . '\n",
      " ' How much school will you miss ? '\n",
      " ' I only need to take this week off . '\n",
      " ' I appreciate you calling and telling me that you won ’ t be in class . See you next week ! ']\n",
      "['Hello , Parker . How ’ s everything ? ' ' Can ’ t complain . And you ? '\n",
      " ' Business is booming . I understand you want to meet up with me next week . How ’ s your schedule looking ? '\n",
      " ' Let me see . I can come out and see you first thing Wednesday . '\n",
      " ' Great . ']\n",
      "['How come it is slow as a snail today ? '\n",
      " ' You mean the network connection ? '\n",
      " ' Yes , I wanted to look for some information on the company page just now . It took me almost one minute to open it . Then there is no response to any click . '\n",
      " ' I have the same question . I can ’ t send out mails . We ’ d better call the IT department and ask them to check it immediately . '\n",
      " ' Ok . ']\n",
      "['Honey , I need to have a talk with you . '\n",
      " ' Dad , I have to do my homework . '\n",
      " \" No , honey , why didn't you go to cram school last night ? \"\n",
      " \" Dad , I don't want to talk about it now . \"\n",
      " \" Honey , if you don't want to go to cram school , you should tell me the reason why . \"\n",
      " \" I'm sorry , dad . But I would rather stay at school than go to cram school . \"]\n",
      "[\"What's wrong with you , young man ? \"\n",
      " ' Doctor , I have a bad cough and a headache . '\n",
      " ' Do you have a fever ? ' \" I don't know , but I feel terrible . \"\n",
      " \" Let me examine you . Don't worry . It's nothing serious . \"\n",
      " ' Do you think I should lie in bed ? '\n",
      " ' Yes , stay in bed and drink a lot of water . Your fever will be gone in a day or two . '\n",
      " ' OK . Do you think I can play football tomorrow ? '\n",
      " ' Of course not . You need a good rest . ' \" OK , I'll listen to you . \"]\n",
      "['I want something sweet after dinner . ' ' What do you have in mind ? '\n",
      " ' A dessert sounds nice . ' ' What kind are you thinking of getting ? '\n",
      " ' I want to get some pie . ' ' What kind of pie do you want ? '\n",
      " ' I have no idea . ' ' Do you want to know what kind of pie I like ? '\n",
      " ' Sure , what kind do you like ? ' ' I love apple pie . '\n",
      " ' Oh , I love apple pie too . ' ' There you go . Problem solved . ']\n",
      "[\"Let's go now . \" \" I'll be with you in a minute . \"]\n"
     ]
    }
   ],
   "source": [
    "for dialogue in test_df['dialog'][101: 110]:\n",
    "    print(dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3388fda7-6bb1-47a7-9859-29e32e2f1ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    11118.000000\n",
      "mean         7.840439\n",
      "std          4.007963\n",
      "min          2.000000\n",
      "25%          4.000000\n",
      "50%          7.000000\n",
      "75%         10.000000\n",
      "max         35.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Cheking lengths of dialogs\n",
    "lengths = []\n",
    "for dialogue in train_df['dialog']:\n",
    "    length = len(dialogue)\n",
    "    lengths.append(length)\n",
    "\n",
    "lengths_series = pd.Series(lengths)\n",
    "print(lengths_series.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89db0e3-aa1c-4889-9e39-aabc148ee2ca",
   "metadata": {},
   "source": [
    "### Clean data\n",
    "For the first such project I will not use additional data provided such as 'act' and 'emotion'. I will use my own data split, therefore I will concatinate all data and take only dialogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "069c8ab6-7013-44c0-88ac-e194286af9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Say , Jim , how about going for a few beers a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Can you do push-ups ? ,  Of course I can . It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Can you study with the radio on ? ,  No , I l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Are you all right ? ,  I will be all right so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Hey John , nice skates . Are they new ? ,  Ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13113</th>\n",
       "      <td>[Frank ’ s getting married , do you believe th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13114</th>\n",
       "      <td>[OK . Come back into the classroom , class . ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13115</th>\n",
       "      <td>[Do you have any hobbies ? ,  Yes , I like col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13116</th>\n",
       "      <td>[Jenny , what's wrong with you ? Why do you ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13117</th>\n",
       "      <td>[What a nice day ! ,  yes . How about going ou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13118 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  dialog\n",
       "0      [Say , Jim , how about going for a few beers a...\n",
       "1      [Can you do push-ups ? ,  Of course I can . It...\n",
       "2      [Can you study with the radio on ? ,  No , I l...\n",
       "3      [Are you all right ? ,  I will be all right so...\n",
       "4      [Hey John , nice skates . Are they new ? ,  Ye...\n",
       "...                                                  ...\n",
       "13113  [Frank ’ s getting married , do you believe th...\n",
       "13114  [OK . Come back into the classroom , class . ,...\n",
       "13115  [Do you have any hobbies ? ,  Yes , I like col...\n",
       "13116  [Jenny , what's wrong with you ? Why do you ke...\n",
       "13117  [What a nice day ! ,  yes . How about going ou...\n",
       "\n",
       "[13118 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dialogs = pd.concat([train_df['dialog'], val_df['dialog'], test_df['dialog']], ignore_index=True)\n",
    "data = pd.DataFrame({'dialog': all_dialogs})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e04d54-13e8-444c-a08b-a16880b5abb1",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d06d6cf2-c98b-433b-9746-939d4b24ebfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of single dialog: <class 'numpy.ndarray'>\n",
      "The type of the sentence within dialog: <class 'str'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 13118 entries, 0 to 13117\n",
      "Series name: dialog\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "13118 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 102.6+ KB\n"
     ]
    }
   ],
   "source": [
    "print(f\"The type of single dialog: {type(data['dialog'][0])}\")\n",
    "print(f\"The type of the sentence within dialog: {type(data['dialog'][0][0])}\")\n",
    "data['dialog'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed01a1ad-359f-4db9-b2b5-9f810676cf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dialogs are ndarrays, my preprocessing funcions are for strings\n",
    "def preprocess_text_array(arr):\n",
    "    dialog = arr.tolist()\n",
    "    return [preprocess_text(text) for text in dialog]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b4a97ad-9000-42e9-ab96-8ee621bda8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialog</th>\n",
       "      <th>preprocessed_dialog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Say , Jim , how about going for a few beers a...</td>\n",
       "      <td>[say how about going for a few beers after din...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Can you do push-ups ? ,  Of course I can . It...</td>\n",
       "      <td>[can you do push ups, of course i can it is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Can you study with the radio on ? ,  No , I l...</td>\n",
       "      <td>[can you study with the radio on, no i listen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Are you all right ? ,  I will be all right so...</td>\n",
       "      <td>[are you all right, i will be all right soon i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Hey John , nice skates . Are they new ? ,  Ye...</td>\n",
       "      <td>[skates are they new, yeah i just got them i s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13113</th>\n",
       "      <td>[Frank ’ s getting married , do you believe th...</td>\n",
       "      <td>[getting married do you believe this, is he re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13114</th>\n",
       "      <td>[OK . Come back into the classroom , class . ,...</td>\n",
       "      <td>[ok come back into the classroom class, does t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13115</th>\n",
       "      <td>[Do you have any hobbies ? ,  Yes , I like col...</td>\n",
       "      <td>[do you have any hobbies, yes i like collectin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13116</th>\n",
       "      <td>[Jenny , what's wrong with you ? Why do you ke...</td>\n",
       "      <td>[what is wrong with you why do you keep weepin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13117</th>\n",
       "      <td>[What a nice day ! ,  yes . How about going ou...</td>\n",
       "      <td>[what a nice day, yes how about going out and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13118 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  dialog  \\\n",
       "0      [Say , Jim , how about going for a few beers a...   \n",
       "1      [Can you do push-ups ? ,  Of course I can . It...   \n",
       "2      [Can you study with the radio on ? ,  No , I l...   \n",
       "3      [Are you all right ? ,  I will be all right so...   \n",
       "4      [Hey John , nice skates . Are they new ? ,  Ye...   \n",
       "...                                                  ...   \n",
       "13113  [Frank ’ s getting married , do you believe th...   \n",
       "13114  [OK . Come back into the classroom , class . ,...   \n",
       "13115  [Do you have any hobbies ? ,  Yes , I like col...   \n",
       "13116  [Jenny , what's wrong with you ? Why do you ke...   \n",
       "13117  [What a nice day ! ,  yes . How about going ou...   \n",
       "\n",
       "                                     preprocessed_dialog  \n",
       "0      [say how about going for a few beers after din...  \n",
       "1      [can you do push ups, of course i can it is a ...  \n",
       "2      [can you study with the radio on, no i listen ...  \n",
       "3      [are you all right, i will be all right soon i...  \n",
       "4      [skates are they new, yeah i just got them i s...  \n",
       "...                                                  ...  \n",
       "13113  [getting married do you believe this, is he re...  \n",
       "13114  [ok come back into the classroom class, does t...  \n",
       "13115  [do you have any hobbies, yes i like collectin...  \n",
       "13116  [what is wrong with you why do you keep weepin...  \n",
       "13117  [what a nice day, yes how about going out and ...  \n",
       "\n",
       "[13118 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[:, \"preprocessed_dialog\"] = data.loc[:, \"dialog\"].apply(preprocess_text_array)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6073ee61-beb8-4c5f-b169-64660f0abfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['please excuse me but i really have to be going', 'yes of course it was nice to see you', 'it was nice to see you too and please give my regards to mrs robbins']\n",
      "['excuse me is this seat taken', 'i am afraid so']\n",
      "['what do you think of the coming match', 'winning is a piece of cake to me', 'you are bragging again']\n",
      "['what would you reckon the taxing increases', 'well the state will benefit a lot i suppose', 'but what do most people think about it', 'ah it s hard to say']\n",
      "['are you still coming to my place for dinner tomorrow night', 'of course is the dinner still on', 'yes i was just wondering how you and your roommate were planning on coming to', 'we were planning on walking both ways since the weather is still nice', \"that 's what i thought you would do listen i live in a bit of\", 'it can not be that bad', 'i wish it was not but there is actually a lot of crime and prostitution', 'really i never would have guessed the criminals must only come out in the evenings', \"do me a favor and take a taxi it 'd make me feel a lot\", 'ok we will how do you get around in the evenings', 'when i first moved in i walked everywhere but within a week i had my', 'has anything else happened to you', 'nothing else has happened to me but i have seen quite a few fights on', 'well we will be careful thanks for letting me know']\n",
      "['wonders whether likes him or not', 'why does not he ask her', 'he is too scared to ask her', 'he is a chicken guy']\n",
      "['i do not understand why some parents keep beefing and complaining about their daughters not', \"yeah li na 's mother has been building a fire under her since her neighbour\", 'if i were na i would ask her if she had done that', 'she is as meek as a lamb she never goes against anyone or anything she']\n",
      "['where is i can not find him anywhere', 'have not you heard that he is in prison', 'what beg your pardon', 'is in prison now he was copped outstealing', 'i just can not believe my ears']\n",
      "['what do you need', 'i need to use the internet', 'you have your library card right', 'yes i do', 'there is a wait right now to use the computers', 'that s fine', 'would you please write your name on this list', 'then what', 'i will call you when a computer is free', 'how do i log on to the computer', 'use the number on the back of your library card', 'thanks i ll be sitting over there']\n"
     ]
    }
   ],
   "source": [
    "for dialogue in data['preprocessed_dialog'][121: 130]:\n",
    "    print(dialogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1662a9-d4df-47e1-aa98-9327309fa488",
   "metadata": {},
   "source": [
    "### Pairing messages - input with responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f7c7c5c-5ded-472c-a5dd-1823a6085050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create input-response pairs\n",
    "def create_pairs(dialogues):\n",
    "    input_responses = []\n",
    "    for dialogue in dialogues:\n",
    "        for i in range(len(dialogue) - 1):\n",
    "            input_responses.append((dialogue[i], dialogue[i + 1]))\n",
    "    return input_responses\n",
    "\n",
    "# Create input-response pairs\n",
    "pairs = create_pairs(data['preprocessed_dialog'])\n",
    "\n",
    "# Convert pairs to DataFrame\n",
    "pairs_df = pd.DataFrame(pairs, columns=['input', 'response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bd0c93a-101d-4ef5-af34-9324b1be7a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>say how about going for a few beers after dinner</td>\n",
       "      <td>you know that is tempting but is really not go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you know that is tempting but is really not go...</td>\n",
       "      <td>what do you mean it will help us to relax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what do you mean it will help us to relax</td>\n",
       "      <td>do you really think so i do not it will just m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>do you really think so i do not it will just m...</td>\n",
       "      <td>i guess you are right but what shall we do i d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i guess you are right but what shall we do i d...</td>\n",
       "      <td>i suggest a walk over to the gym where we can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89856</th>\n",
       "      <td>why not go again to celebrate out one year ann...</td>\n",
       "      <td>are you kidding can you afford it do you think...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89857</th>\n",
       "      <td>are you kidding can you afford it do you think...</td>\n",
       "      <td>never mind that i will take care of it are you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89858</th>\n",
       "      <td>never mind that i will take care of it are you...</td>\n",
       "      <td>yeah i think so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89859</th>\n",
       "      <td>yeah i think so</td>\n",
       "      <td>ok i will make the arrangements it will be great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89860</th>\n",
       "      <td>ok i will make the arrangements it will be great</td>\n",
       "      <td>wonderful i will start packing our suitcases</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89861 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input  \\\n",
       "0       say how about going for a few beers after dinner   \n",
       "1      you know that is tempting but is really not go...   \n",
       "2              what do you mean it will help us to relax   \n",
       "3      do you really think so i do not it will just m...   \n",
       "4      i guess you are right but what shall we do i d...   \n",
       "...                                                  ...   \n",
       "89856  why not go again to celebrate out one year ann...   \n",
       "89857  are you kidding can you afford it do you think...   \n",
       "89858  never mind that i will take care of it are you...   \n",
       "89859                                    yeah i think so   \n",
       "89860   ok i will make the arrangements it will be great   \n",
       "\n",
       "                                                response  \n",
       "0      you know that is tempting but is really not go...  \n",
       "1              what do you mean it will help us to relax  \n",
       "2      do you really think so i do not it will just m...  \n",
       "3      i guess you are right but what shall we do i d...  \n",
       "4      i suggest a walk over to the gym where we can ...  \n",
       "...                                                  ...  \n",
       "89856  are you kidding can you afford it do you think...  \n",
       "89857  never mind that i will take care of it are you...  \n",
       "89858                                    yeah i think so  \n",
       "89859   ok i will make the arrangements it will be great  \n",
       "89860       wonderful i will start packing our suitcases  \n",
       "\n",
       "[89861 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac0374a5-80f3-4d8f-9f8f-234190f4e2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    89861.000000\n",
      "mean         9.574799\n",
      "std          4.314846\n",
      "min          0.000000\n",
      "25%          6.000000\n",
      "50%          9.000000\n",
      "75%         15.000000\n",
      "max         15.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Checking length of sentences\n",
    "lengths = []\n",
    "for sentence in pairs_df['input']:\n",
    "    length = len(sentence.split())\n",
    "    lengths.append(length)\n",
    "\n",
    "lengths_series = pd.Series(lengths)\n",
    "print(lengths_series.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2abf70bf-f25f-4102-9793-a6a8c7f244f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_monologues = pairs_df[pairs_df['input'].str.split().str.len() > 30]\n",
    "# print(long_monologues)\n",
    "# print(pairs_df['input'][89769])\n",
    "# len(long_monologues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eed6d9-5b38-439b-8a95-bd835bb4d588",
   "metadata": {},
   "source": [
    "### Initialize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d16fef4-a5db-45e2-99c5-a71e9fecfcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of <START> token: 1\n",
      "Index of <END> token: 2\n",
      "Index of <OOV> token: 3\n",
      "15384\n",
      "10000\n",
      "\n",
      "Top 15 most frequent words:\n",
      " [('i', 86051), ('you', 73409), ('the', 52476), ('to', 44476), ('a', 39594), ('is', 37170), ('it', 36511), ('that', 24652), ('have', 23424), ('do', 23403), ('and', 21134), ('not', 20913), ('of', 18558), ('are', 18413), ('what', 17623)]\n",
      "\n",
      "Last 100 words:\n",
      " [('wong', 1), ('glitches', 1), ('communicational', 1), ('assiduously', 1), ('rarest', 1), ('momma', 1), ('codes', 1), ('dongle', 1), ('multitasking', 1), ('constipation', 1), ('recarpeted', 1), ('uncite', 1), ('dutton', 1), ('goodnight', 1), ('reunification', 1), ('yearning', 1), ('informing', 1), ('robson', 1), ('gaston', 1), ('transition', 1), ('macchiato', 1), ('backers', 1), ('montezuma', 1), ('thans', 1), ('ultra', 1), ('brushed', 1), ('titanium', 1), ('kaohsiung', 1), ('mousaka', 1), ('adjustments', 1), ('floral', 1), ('apologise', 1), ('distributed', 1), ('judgement', 1), ('transported', 1), ('trucks', 1), ('donut', 1), ('outsmart', 1), ('thrilled', 1), ('tienda', 1), ('maroon', 1), ('familiarise', 1), ('chunks', 1), ('upfront', 1), ('cctv', 1), ('discreet', 1), ('demographics', 1), ('byeb', 1), ('formalities', 1), ('chute', 1), ('draws', 1), ('neighborhoods', 1), ('redouble', 1), ('transplant', 1), ('fossil', 1), ('fuels', 1), ('hydro', 1), ('distractions', 1), ('gustave', 1), ('hells', 1), ('canvas', 1), ('crescive', 1), ('symbolize', 1), ('longevity', 1), ('lpt', 1), ('oversee', 1), ('evades', 1), ('detection', 1), ('await', 1), ('trional', 1), ('fortnightly', 1), ('trifle', 1), ('overtake', 1), ('aider', 1), ('smashing', 1), ('actives', 1), ('copywriters', 1), ('translations', 1), ('aspirins', 1), ('crashes', 1), ('streeter', 1), ('kleenex', 1), ('noticeable', 1), ('innovation', 1), ('coordination', 1), ('sealed', 1), ('grownups', 1), ('telecommuting', 1), ('lifelike', 1), ('simulated', 1), ('incarnate', 1), ('extraordinaire', 1), ('engages', 1), ('interact', 1), ('apes', 1), ('wined', 1), ('dined', 1), ('scuba', 1), ('flavorings', 1), ('hopeless', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>') # , filters=' ') - default filters remove punctuation\n",
    "tokenizer.fit_on_texts(pairs_df['input'].tolist() + pairs_df['response'].tolist())\n",
    "\n",
    "# Define special tokens\n",
    "start_token = '<START>'\n",
    "end_token = '<END>'\n",
    "\n",
    "old_word_index = len(tokenizer.word_index)\n",
    "\n",
    "# Add special tokens to the tokenizer and ensure they are within the top 10,000 words\n",
    "tokenizer.word_index = {k: (i+3) for i, (k, v) in enumerate(tokenizer.word_index.items()) if i < old_word_index}\n",
    "tokenizer.word_index[start_token] = 1\n",
    "tokenizer.word_index[end_token] = 2\n",
    "tokenizer.word_index[tokenizer.oov_token] = 3\n",
    "\n",
    "# Verify the indices\n",
    "print(\"Index of <START> token:\", tokenizer.word_index['<START>'])\n",
    "print(\"Index of <END> token:\", tokenizer.word_index['<END>'])\n",
    "print(\"Index of <OOV> token:\", tokenizer.word_index['<OOV>'])\n",
    "\n",
    "print(len(tokenizer.word_index))\n",
    "print(tokenizer.num_words)\n",
    "\n",
    "sorted_word_counts = OrderedDict(sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "print(f\"\\nTop 15 most frequent words:\\n {list(sorted_word_counts.items())[:15]}\")\n",
    "print(f\"\\nLast 100 words:\\n {list(sorted_word_counts.items())[-100:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a276c-6264-4bee-a0fb-067f04b6b4c7",
   "metadata": {},
   "source": [
    "### Filter rare words - 10000 vocabulary OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9dd5de7a-fe72-4754-92cc-76b214d7fe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words before filtering: 15381\n",
      "Total words after filtering: 10416\n",
      "\n",
      "Top 15 most frequent words:\n",
      " [('i', 86051), ('you', 73409), ('the', 52476), ('to', 44476), ('a', 39594), ('is', 37170), ('it', 36511), ('that', 24652), ('have', 23424), ('do', 23403), ('and', 21134), ('not', 20913), ('of', 18558), ('are', 18413), ('what', 17623)]\n",
      "\n",
      "Last 100 words:\n",
      " [('solomon', 3), ('believer', 3), ('truant', 3), ('clarity', 3), ('blanca', 3), ('bellhop', 3), ('frappuccino', 3), ('trail', 3), ('flames', 3), ('sanitary', 3), ('unfit', 3), ('allison', 3), ('biannually', 3), ('pamphlets', 3), ('hasty', 3), ('expire', 3), ('overcoming', 3), ('banked', 3), ('undue', 3), ('scaring', 3), ('stylus', 3), ('advisor', 3), ('realise', 3), ('occasional', 3), ('lithium', 3), ('eva', 3), ('bluemingdails', 3), ('spectator', 3), ('fang', 3), ('insight', 3), ('spotless', 3), ('prosperous', 3), ('deduct', 3), ('al', 3), ('booster', 3), ('stefan', 3), ('pedicure', 3), ('discoveries', 3), ('acidic', 3), ('judging', 3), ('ation', 3), ('bristles', 3), ('bidders', 3), ('ahem', 3), ('arranging', 3), ('inventors', 3), ('convertible', 3), ('turnaround', 3), ('handcrafts', 3), ('interrupted', 3), ('firmly', 3), ('precision', 3), ('corresponding', 3), ('speedy', 3), (\"i'li\", 3), ('ministry', 3), ('weed', 3), ('scented', 3), ('talker', 3), ('worship', 3), ('kiwis', 3), ('typewriters', 3), ('metropolis', 3), ('certainty', 3), ('mentality', 3), ('ratings', 3), ('usher', 3), ('scottish', 3), ('herbal', 3), ('confiscated', 3), ('clears', 3), ('juices', 3), ('technoledge', 3), ('bicycles', 3), ('possession', 3), ('poem', 3), ('tutoring', 3), ('lifts', 3), ('fortunes', 3), ('groves', 3), ('carter', 3), ('chessboard', 3), ('hurdle', 3), ('aesthetics', 3), ('pronounces', 3), ('beliefs', 3), ('warned', 3), ('habitable', 3), ('exhilarating', 3), ('pitches', 3), ('whiter', 3), ('stimulate', 3), ('smiles', 3), ('salesmen', 3), ('visits', 3), ('cartridge', 3), ('mcbride', 3), ('pullover', 3), ('cushion', 3), ('banged', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Just reviewing what are the last words in vocabulary - do they still usable and recognizable\n",
    "# Set a frequency threshold\n",
    "threshold = 3\n",
    "\n",
    "# Filter out rare words\n",
    "filtered_words = {word: count for word, count in sorted_word_counts.items() if count >= threshold}\n",
    "\n",
    "# Display the number of words before and after filtering\n",
    "print(f\"Total words before filtering: {len(sorted_word_counts)}\")\n",
    "print(f\"Total words after filtering: {len(filtered_words)}\")\n",
    "# Display the sorted word counts\n",
    "print(f\"\\nTop 15 most frequent words:\\n {list(filtered_words.items())[:15]}\")\n",
    "print(f\"\\nLast 100 words:\\n {list(filtered_words.items())[-100:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb4a376-b87e-4630-a88c-12d22b9d3a2b",
   "metadata": {},
   "source": [
    "### Save the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca8be1de-b1f7-4d93-897b-6ef1908fc475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved to C:\\Users\\tomui\\Desktop\\capstone_project\\data\\tokenizer_dd.pickle\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Determine the directory where the tokenizer will be saved\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Save the tokenizer using pickle\n",
    "tokenizer_path = os.path.join(data_dir, 'tokenizer_dd.pickle')\n",
    "with open(tokenizer_path, 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(f\"Tokenizer saved to {tokenizer_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5deb6d2-695b-4025-b6b1-72428d051053",
   "metadata": {},
   "source": [
    "### Load the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5428c87-4ce6-480a-bb32-48f7a6063e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from file\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "tokenizer_path = os.path.join(data_dir, 'tokenizer_dd.pickle')\n",
    "with open(tokenizer_path, 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9df411-ff49-4a67-802e-bd0dbca3ba3f",
   "metadata": {},
   "source": [
    "### Converting to indices and input-target sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df1d74ff-4be8-4b14-8083-ddbce2032e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input data shape: (89861, 15)\n",
      "Decoder input data shape: (89861, 16)\n",
      "Decoder output data shape: (89861, 16)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and pad the encoder input\n",
    "input_sequences = tokenizer.texts_to_sequences(pairs_df['input'].tolist())\n",
    "max_len_input = max(len(seq) for seq in input_sequences)  # Leve for later possibility pad without trimming in preprocessing stage\n",
    "encoder_input_data = pad_sequences(input_sequences, maxlen=max_len_input, padding='pre', truncating='post')\n",
    "\n",
    "# Tokenize the decoder input and output\n",
    "output_sequences = tokenizer.texts_to_sequences(pairs_df['response'].tolist())\n",
    "\n",
    "# Add start and end tokens\n",
    "start_token_index = tokenizer.word_index[start_token]\n",
    "end_token_index = tokenizer.word_index[end_token] \n",
    "decoder_input_sequences = [[start_token_index] + seq for seq in output_sequences]\n",
    "decoder_output_sequences = [seq + [end_token_index] for seq in output_sequences]\n",
    "\n",
    "# Pad the decoder input sequences\n",
    "max_len_output = max(len(seq) for seq in decoder_input_sequences)  # Leve for later possibility pad without trimming in preprocessing stage\n",
    "decoder_input_data = pad_sequences(decoder_input_sequences, maxlen=max_len_output, padding='pre', truncating='post')\n",
    "\n",
    "# Pad the decoder output sequences\n",
    "decoder_output_data = pad_sequences(decoder_output_sequences, maxlen=max_len_output, padding='pre', truncating='post')\n",
    "\n",
    "print(f'Encoder input data shape: {encoder_input_data.shape}')\n",
    "print(f'Decoder input data shape: {decoder_input_data.shape}')\n",
    "print(f'Decoder output data shape: {decoder_output_data.shape}')\n",
    "\n",
    "# Store numpy arrays directly in the DataFrame\n",
    "pairs_df['encoder_input_data'] = encoder_input_data.tolist()\n",
    "pairs_df['decoder_input_data'] = decoder_input_data.tolist()\n",
    "pairs_df['decoder_output_data'] = decoder_output_data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "685d7313-700b-4e5f-a1c4-34d33b8e0155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>response</th>\n",
       "      <th>encoder_input_data</th>\n",
       "      <th>decoder_input_data</th>\n",
       "      <th>decoder_output_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>say how about going for a few beers after dinner</td>\n",
       "      <td>you know that is tempting but is really not go...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 138, 33, 37, 75, 20, 8, 206, 3...</td>\n",
       "      <td>[0, 0, 1, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 4...</td>\n",
       "      <td>[0, 0, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 47, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you know that is tempting but is really not go...</td>\n",
       "      <td>what do you mean it will help us to relax</td>\n",
       "      <td>[0, 0, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 47, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 18, 13, 5, 161, 10, 23, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 18, 13, 5, 161, 10, 23, 101, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what do you mean it will help us to relax</td>\n",
       "      <td>do you really think so i do not it will just m...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 18, 13, 5, 161, 10, 23, 101, 9...</td>\n",
       "      <td>[1, 13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, ...</td>\n",
       "      <td>[13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, 102...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>do you really think so i do not it will just m...</td>\n",
       "      <td>i guess you are right but what shall we do i d...</td>\n",
       "      <td>[13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, 102...</td>\n",
       "      <td>[1, 4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4,...</td>\n",
       "      <td>[4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i guess you are right but what shall we do i d...</td>\n",
       "      <td>i suggest a walk over to the gym where we can ...</td>\n",
       "      <td>[4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4, 13...</td>\n",
       "      <td>[1, 4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 2...</td>\n",
       "      <td>[4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 21, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89856</th>\n",
       "      <td>why not go again to celebrate out one year ann...</td>\n",
       "      <td>are you kidding can you afford it do you think...</td>\n",
       "      <td>[88, 15, 59, 204, 7, 1602, 84, 56, 203, 1776, ...</td>\n",
       "      <td>[1, 17, 5, 618, 21, 5, 974, 10, 13, 5, 43, 22,...</td>\n",
       "      <td>[17, 5, 618, 21, 5, 974, 10, 13, 5, 43, 22, 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89857</th>\n",
       "      <td>are you kidding can you afford it do you think...</td>\n",
       "      <td>never mind that i will take care of it are you...</td>\n",
       "      <td>[17, 5, 618, 21, 5, 974, 10, 13, 5, 43, 22, 21...</td>\n",
       "      <td>[0, 1, 174, 211, 11, 4, 23, 72, 351, 16, 10, 1...</td>\n",
       "      <td>[0, 174, 211, 11, 4, 23, 72, 351, 16, 10, 17, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89858</th>\n",
       "      <td>never mind that i will take care of it are you...</td>\n",
       "      <td>yeah i think so</td>\n",
       "      <td>[0, 174, 211, 11, 4, 23, 72, 351, 16, 10, 17, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 112, 4, 4...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 112, 4, 43, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89859</th>\n",
       "      <td>yeah i think so</td>\n",
       "      <td>ok i will make the arrangements it will be great</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 112, 4, 43, 36]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 71, 4, 23, 102, 6, 3756, 10...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 71, 4, 23, 102, 6, 3756, 10, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89860</th>\n",
       "      <td>ok i will make the arrangements it will be great</td>\n",
       "      <td>wonderful i will start packing our suitcases</td>\n",
       "      <td>[0, 0, 0, 0, 0, 71, 4, 23, 102, 6, 3756, 10, 2...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 418, 4, 23, 271, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 418, 4, 23, 271, 1710...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89861 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input  \\\n",
       "0       say how about going for a few beers after dinner   \n",
       "1      you know that is tempting but is really not go...   \n",
       "2              what do you mean it will help us to relax   \n",
       "3      do you really think so i do not it will just m...   \n",
       "4      i guess you are right but what shall we do i d...   \n",
       "...                                                  ...   \n",
       "89856  why not go again to celebrate out one year ann...   \n",
       "89857  are you kidding can you afford it do you think...   \n",
       "89858  never mind that i will take care of it are you...   \n",
       "89859                                    yeah i think so   \n",
       "89860   ok i will make the arrangements it will be great   \n",
       "\n",
       "                                                response  \\\n",
       "0      you know that is tempting but is really not go...   \n",
       "1              what do you mean it will help us to relax   \n",
       "2      do you really think so i do not it will just m...   \n",
       "3      i guess you are right but what shall we do i d...   \n",
       "4      i suggest a walk over to the gym where we can ...   \n",
       "...                                                  ...   \n",
       "89856  are you kidding can you afford it do you think...   \n",
       "89857  never mind that i will take care of it are you...   \n",
       "89858                                    yeah i think so   \n",
       "89859   ok i will make the arrangements it will be great   \n",
       "89860       wonderful i will start packing our suitcases   \n",
       "\n",
       "                                      encoder_input_data  \\\n",
       "0      [0, 0, 0, 0, 0, 138, 33, 37, 75, 20, 8, 206, 3...   \n",
       "1      [0, 0, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 47, ...   \n",
       "2      [0, 0, 0, 0, 0, 18, 13, 5, 161, 10, 23, 101, 9...   \n",
       "3      [13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, 102...   \n",
       "4      [4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4, 13...   \n",
       "...                                                  ...   \n",
       "89856  [88, 15, 59, 204, 7, 1602, 84, 56, 203, 1776, ...   \n",
       "89857  [17, 5, 618, 21, 5, 974, 10, 13, 5, 43, 22, 21...   \n",
       "89858  [0, 174, 211, 11, 4, 23, 72, 351, 16, 10, 17, ...   \n",
       "89859  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 112, 4, 43, 36]   \n",
       "89860  [0, 0, 0, 0, 0, 71, 4, 23, 102, 6, 3756, 10, 2...   \n",
       "\n",
       "                                      decoder_input_data  \\\n",
       "0      [0, 0, 1, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 4...   \n",
       "1      [0, 0, 0, 0, 0, 1, 18, 13, 5, 161, 10, 23, 101...   \n",
       "2      [1, 13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, ...   \n",
       "3      [1, 4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4,...   \n",
       "4      [1, 4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 2...   \n",
       "...                                                  ...   \n",
       "89856  [1, 17, 5, 618, 21, 5, 974, 10, 13, 5, 43, 22,...   \n",
       "89857  [0, 1, 174, 211, 11, 4, 23, 72, 351, 16, 10, 1...   \n",
       "89858  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 112, 4, 4...   \n",
       "89859  [0, 0, 0, 0, 0, 1, 71, 4, 23, 102, 6, 3756, 10...   \n",
       "89860  [0, 0, 0, 0, 0, 0, 0, 0, 1, 418, 4, 23, 271, 1...   \n",
       "\n",
       "                                     decoder_output_data  \n",
       "0      [0, 0, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 47, ...  \n",
       "1      [0, 0, 0, 0, 0, 18, 13, 5, 161, 10, 23, 101, 9...  \n",
       "2      [13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, 102...  \n",
       "3      [4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4, 13...  \n",
       "4      [4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 21, ...  \n",
       "...                                                  ...  \n",
       "89856  [17, 5, 618, 21, 5, 974, 10, 13, 5, 43, 22, 21...  \n",
       "89857  [0, 174, 211, 11, 4, 23, 72, 351, 16, 10, 17, ...  \n",
       "89858  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 112, 4, 43, ...  \n",
       "89859  [0, 0, 0, 0, 0, 71, 4, 23, 102, 6, 3756, 10, 2...  \n",
       "89860  [0, 0, 0, 0, 0, 0, 0, 0, 418, 4, 23, 271, 1710...  \n",
       "\n",
       "[89861 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43feafb6-2b5a-4710-b070-258258f63e30",
   "metadata": {},
   "source": [
    "### Checking if conversion was successfull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26d4a424-9e01-44d3-a6a5-4df7eb46bf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Encoder Input: no so it is usually boring to join my friends in the afternoon at a \n",
      "Reconstructed Input: no so it is usually boring to join my friends in the afternoon at a\n",
      "\n",
      "Original Decoder Input: what kind of things would you like to see on the menu \n",
      "Reconstructed Text: <START> what kind of things would you like to see on the menu\n",
      "\n",
      "Original Decoder Output: what kind of things would you like to see on the menu \n",
      "Reconstructed Text: what kind of things would you like to see on the menu <END>\n",
      "\n",
      "Original Encoder Input: what kind of things would you like to see on the menu \n",
      "Reconstructed Input: what kind of things would you like to see on the menu\n",
      "\n",
      "Original Decoder Input: maybe a fruit salad and a few different hot sandwiches at least \n",
      "Reconstructed Text: <START> maybe a fruit salad and a few different hot sandwiches at least\n",
      "\n",
      "Original Decoder Output: maybe a fruit salad and a few different hot sandwiches at least \n",
      "Reconstructed Text: maybe a fruit salad and a few different hot sandwiches at least <END>\n",
      "\n",
      "Original Encoder Input: maybe a fruit salad and a few different hot sandwiches at least \n",
      "Reconstructed Input: maybe a fruit salad and a few different hot sandwiches at least\n",
      "\n",
      "Original Decoder Input: that should not be too difficult since this is a small neighborhood maybe they will \n",
      "Reconstructed Text: <START> that should not be too difficult since this is a small neighborhood maybe they will\n",
      "\n",
      "Original Decoder Output: that should not be too difficult since this is a small neighborhood maybe they will \n",
      "Reconstructed Text: that should not be too difficult since this is a small neighborhood maybe they will <END>\n",
      "\n",
      "Original Encoder Input: that should not be too difficult since this is a small neighborhood maybe they will \n",
      "Reconstructed Input: that should not be too difficult since this is a small neighborhood maybe they will\n",
      "\n",
      "Original Decoder Input: let us try it \n",
      "Reconstructed Text: <START> let us try it\n",
      "\n",
      "Original Decoder Output: let us try it \n",
      "Reconstructed Text: let us try it <END>\n",
      "\n",
      "Original Encoder Input: darling i have news for you and his wife evelyn are going to have a \n",
      "Reconstructed Input: darling i have news for you and his wife evelyn are going to have a\n",
      "\n",
      "Original Decoder Input: really i thought his wife couldn t have a baby \n",
      "Reconstructed Text: <START> really i thought his wife couldn t have a baby\n",
      "\n",
      "Original Decoder Output: really i thought his wife couldn t have a baby \n",
      "Reconstructed Text: really i thought his wife couldn t have a baby <END>\n"
     ]
    }
   ],
   "source": [
    "def sequences_to_text(sequence):\n",
    "    index_to_word = {index: word for word, index in tokenizer.word_index.items()}\n",
    "    # Directly map sequence of indices back to words\n",
    "    return ' '.join(index_to_word.get(idx, '') for idx in sequence if idx != 0)\n",
    "\n",
    "# Print original and reverse-tokenized text for entries\n",
    "for index, row in pairs_df[1130:1135].iterrows():\n",
    "    print(\"\\nOriginal Encoder Input:\", row['input'], \n",
    "          \"\\nReconstructed Input:\", sequences_to_text(row['encoder_input_data']))\n",
    "    print(\"\\nOriginal Decoder Input:\", row['response'], \n",
    "          \"\\nReconstructed Text:\", sequences_to_text(row['decoder_input_data']))\n",
    "    print(\"\\nOriginal Decoder Output:\", row['response'], \n",
    "          \"\\nReconstructed Text:\", sequences_to_text(row['decoder_output_data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f96c6b5-a666-499a-b578-9639350909e3",
   "metadata": {},
   "source": [
    "### Saving the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3dddbeaa-4b10-44e1-a6f9-f3f2056ce421",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving the DataFrame\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "file_path_parquet = os.path.join(data_dir, 'training_df_dd.parquet')\n",
    "pairs_df.to_parquet(file_path_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e0ec68-2723-436d-a101-ad18df9f0a63",
   "metadata": {},
   "source": [
    "### Loading the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8036e60-f3a3-4beb-9f96-da6496797413",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>response</th>\n",
       "      <th>encoder_input_data</th>\n",
       "      <th>decoder_input_data</th>\n",
       "      <th>decoder_output_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>say how about going for a few beers after dinner</td>\n",
       "      <td>you know that is tempting but is really not go...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 138, 33, 37, 75, 20, 8, 206, 3...</td>\n",
       "      <td>[0, 0, 1, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 4...</td>\n",
       "      <td>[0, 0, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 47, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you know that is tempting but is really not go...</td>\n",
       "      <td>what do you mean it will help us to relax</td>\n",
       "      <td>[0, 0, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 47, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 18, 13, 5, 161, 10, 23, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 18, 13, 5, 161, 10, 23, 101, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what do you mean it will help us to relax</td>\n",
       "      <td>do you really think so i do not it will just m...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 18, 13, 5, 161, 10, 23, 101, 9...</td>\n",
       "      <td>[1, 13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, ...</td>\n",
       "      <td>[13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, 102...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>do you really think so i do not it will just m...</td>\n",
       "      <td>i guess you are right but what shall we do i d...</td>\n",
       "      <td>[13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, 102...</td>\n",
       "      <td>[1, 4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4,...</td>\n",
       "      <td>[4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i guess you are right but what shall we do i d...</td>\n",
       "      <td>i suggest a walk over to the gym where we can ...</td>\n",
       "      <td>[4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4, 13...</td>\n",
       "      <td>[1, 4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 2...</td>\n",
       "      <td>[4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 21, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i suggest a walk over to the gym where we can ...</td>\n",
       "      <td>that 's a good idea i hear mary and sally ofte...</td>\n",
       "      <td>[4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 21, ...</td>\n",
       "      <td>[1, 11, 38, 8, 47, 179, 4, 237, 441, 14, 3323,...</td>\n",
       "      <td>[11, 38, 8, 47, 179, 4, 237, 441, 14, 3323, 30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>that 's a good idea i hear mary and sally ofte...</td>\n",
       "      <td>sounds great to me if they are willing we coul...</td>\n",
       "      <td>[11, 38, 8, 47, 179, 4, 237, 441, 14, 3323, 30...</td>\n",
       "      <td>[1, 154, 99, 7, 26, 57, 54, 17, 1083, 22, 79, ...</td>\n",
       "      <td>[154, 99, 7, 26, 57, 54, 17, 1083, 22, 79, 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sounds great to me if they are willing we coul...</td>\n",
       "      <td>good let us go now</td>\n",
       "      <td>[154, 99, 7, 26, 57, 54, 17, 1083, 22, 79, 200...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 47, 74, 93, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 47, 74, 93, 59,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>good let us go now</td>\n",
       "      <td>all right</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 47, 74, 93, 59,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 50,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>can you do push ups</td>\n",
       "      <td>of course i can it is a piece of cake believe ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 5, 13, 1635...</td>\n",
       "      <td>[1, 16, 125, 4, 21, 10, 9, 8, 773, 16, 899, 25...</td>\n",
       "      <td>[16, 125, 4, 21, 10, 9, 8, 773, 16, 899, 254, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0   say how about going for a few beers after dinner   \n",
       "1  you know that is tempting but is really not go...   \n",
       "2          what do you mean it will help us to relax   \n",
       "3  do you really think so i do not it will just m...   \n",
       "4  i guess you are right but what shall we do i d...   \n",
       "5  i suggest a walk over to the gym where we can ...   \n",
       "6  that 's a good idea i hear mary and sally ofte...   \n",
       "7  sounds great to me if they are willing we coul...   \n",
       "8                                 good let us go now   \n",
       "9                                can you do push ups   \n",
       "\n",
       "                                            response  \\\n",
       "0  you know that is tempting but is really not go...   \n",
       "1          what do you mean it will help us to relax   \n",
       "2  do you really think so i do not it will just m...   \n",
       "3  i guess you are right but what shall we do i d...   \n",
       "4  i suggest a walk over to the gym where we can ...   \n",
       "5  that 's a good idea i hear mary and sally ofte...   \n",
       "6  sounds great to me if they are willing we coul...   \n",
       "7                                 good let us go now   \n",
       "8                                          all right   \n",
       "9  of course i can it is a piece of cake believe ...   \n",
       "\n",
       "                                  encoder_input_data  \\\n",
       "0  [0, 0, 0, 0, 0, 138, 33, 37, 75, 20, 8, 206, 3...   \n",
       "1  [0, 0, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 47, ...   \n",
       "2  [0, 0, 0, 0, 0, 18, 13, 5, 161, 10, 23, 101, 9...   \n",
       "3  [13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, 102...   \n",
       "4  [4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4, 13...   \n",
       "5  [4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 21, ...   \n",
       "6  [11, 38, 8, 47, 179, 4, 237, 441, 14, 3323, 30...   \n",
       "7  [154, 99, 7, 26, 57, 54, 17, 1083, 22, 79, 200...   \n",
       "8  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 47, 74, 93, 59,...   \n",
       "9  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 5, 13, 1635...   \n",
       "\n",
       "                                  decoder_input_data  \\\n",
       "0  [0, 0, 1, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 4...   \n",
       "1  [0, 0, 0, 0, 0, 1, 18, 13, 5, 161, 10, 23, 101...   \n",
       "2  [1, 13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, ...   \n",
       "3  [1, 4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4,...   \n",
       "4  [1, 4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 2...   \n",
       "5  [1, 11, 38, 8, 47, 179, 4, 237, 441, 14, 3323,...   \n",
       "6  [1, 154, 99, 7, 26, 57, 54, 17, 1083, 22, 79, ...   \n",
       "7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 47, 74, 93, ...   \n",
       "8  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 50,...   \n",
       "9  [1, 16, 125, 4, 21, 10, 9, 8, 773, 16, 899, 25...   \n",
       "\n",
       "                                 decoder_output_data  \n",
       "0  [0, 0, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 47, ...  \n",
       "1  [0, 0, 0, 0, 0, 18, 13, 5, 161, 10, 23, 101, 9...  \n",
       "2  [13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, 102...  \n",
       "3  [4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4, 13...  \n",
       "4  [4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 21, ...  \n",
       "5  [11, 38, 8, 47, 179, 4, 237, 441, 14, 3323, 30...  \n",
       "6  [154, 99, 7, 26, 57, 54, 17, 1083, 22, 79, 200...  \n",
       "7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 47, 74, 93, 59,...  \n",
       "8  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 53...  \n",
       "9  [16, 125, 4, 21, 10, 9, 8, 773, 16, 899, 254, ...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the DataFrame\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "file_path_parquet = os.path.join(data_dir, 'training_df_dd.parquet')\n",
    "training_data_final = pd.read_parquet(file_path_parquet)\n",
    "\n",
    "training_data_final.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad6aab19-4df8-461d-9dc1-393d1157a83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15384\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.word_index))\n",
    "print(tokenizer.num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3fbffc-0d50-455a-8d1a-2608dad745e3",
   "metadata": {},
   "source": [
    "### Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39ff9d2f-e6b8-4c47-aeb5-5af9071ad308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80874, 15)\n",
      "(8987, 15)\n",
      "(80874, 16)\n",
      "(8987, 16)\n",
      "(80874, 16)\n",
      "(8987, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data = np.array(training_data_final['encoder_input_data'].tolist())\n",
    "decoder_input_data = np.array(training_data_final['decoder_input_data'].tolist())\n",
    "decoder_output_data = np.array(training_data_final['decoder_output_data'].tolist())\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "encoder_input_train, encoder_input_val, decoder_input_train, decoder_input_val, decoder_output_train, decoder_output_val = train_test_split(\n",
    "    encoder_input_data, decoder_input_data, decoder_output_data, test_size=0.1, random_state=22)\n",
    "print(encoder_input_train.shape)\n",
    "print(encoder_input_val.shape)\n",
    "print(decoder_input_train.shape)\n",
    "print(decoder_input_val.shape)\n",
    "print(decoder_output_train.shape)\n",
    "print(decoder_output_val.shape)\n",
    "encoder_input_data = encoder_input_data.astype('int32')\n",
    "decoder_input_data = decoder_input_data.astype('int32')\n",
    "decoder_output_data = decoder_output_data.astype('int32')\n",
    "decoder_output_data.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076347eb-91fb-4830-a259-27e6241faa67",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5feee9c-140d-4bf3-bd4b-fd7f576ce7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,077,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">641,600</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>),      │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">6,154,000</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281,600</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">6,169,385</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">15385</span>)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m) │  \u001b[38;5;34m3,077,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m),     │    \u001b[38;5;34m641,600\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m),      │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m) │  \u001b[38;5;34m6,154,000\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │  \u001b[38;5;34m1,281,600\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ \u001b[38;5;34m400\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ \u001b[38;5;34m400\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m400\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m6,169,385\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│                     │ \u001b[38;5;34m15385\u001b[0m)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,323,585</span> (66.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,323,585\u001b[0m (66.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,323,585</span> (66.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,323,585\u001b[0m (66.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define model parameters\n",
    "latent_dim = 200\n",
    "num_encoder_tokens = len(tokenizer.word_index) + 1\n",
    "num_decoder_tokens = len(tokenizer.word_index) + 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding = Embedding(num_encoder_tokens, latent_dim, mask_zero=True)(encoder_inputs)\n",
    "# Make the LSTM layer bidirectional\n",
    "encoder_lstm = Bidirectional(LSTM(latent_dim, return_state=True, dropout=0.2, recurrent_dropout=0.2, kernel_regularizer=l2(0.01)))  # , recurrent_dropout=0.2) Removed recurrent_dropout for cuDNN compatibility\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedding)\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Update latent_dim to match the concatenated states\n",
    "latent_dim *= 2\n",
    "\n",
    "# Define decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(num_decoder_tokens, latent_dim, mask_zero=True)\n",
    "decoder_embedded = decoder_embedding(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.2, recurrent_dropout=0.2, kernel_regularizer=l2(0.01))  # , recurrent_dropout=0.2) Removed recurrent_dropout for cuDNN compatibility\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedded, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Define checkpoint callback\n",
    "checkpoint = ModelCheckpoint('data/seq2seq_dd_model_best_val_loss.keras', save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6a4f9-b6f4-4a00-a92b-dcb869e5dda7",
   "metadata": {},
   "source": [
    "### Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9067870-1a68-4a69-bdd4-d8abc6cce21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the directory and file path\n",
    "# data_dir = os.path.join(os.getcwd(), 'data')\n",
    "# file_path_h5 = os.path.join(data_dir, 'seq2seq_dd_model_9 ep_val_loss.h5')\n",
    "\n",
    "# # Load the model\n",
    "# model = load_model(file_path_h5)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe71e88b-5724-4c14-ada4-e7f7f2ceb383",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e86cf1fe-85e1-4142-8c54-04d178b499e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488ms/step - loss: 6.7836\n",
      "Epoch 1: val_loss improved from inf to 4.92725, saving model to data/seq2seq_dd_model_best_val_loss.keras\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m644s\u001b[0m 503ms/step - loss: 6.7826 - val_loss: 4.9272\n",
      "Epoch 2/20\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 496ms/step - loss: 4.8349\n",
      "Epoch 2: val_loss improved from 4.92725 to 4.63086, saving model to data/seq2seq_dd_model_best_val_loss.keras\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m645s\u001b[0m 511ms/step - loss: 4.8349 - val_loss: 4.6309\n",
      "Epoch 3/20\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542ms/step - loss: 4.5554\n",
      "Epoch 3: val_loss improved from 4.63086 to 4.46843, saving model to data/seq2seq_dd_model_best_val_loss.keras\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m706s\u001b[0m 559ms/step - loss: 4.5554 - val_loss: 4.4684\n",
      "Epoch 4/20\n",
      "\u001b[1m 456/1264\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m7:22\u001b[0m 547ms/step - loss: 4.3809"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      3\u001b[0m     [encoder_input_train, decoder_input_train],\n\u001b[0;32m      4\u001b[0m     decoder_output_train[:, :, np\u001b[38;5;241m.\u001b[39mnewaxis],  \u001b[38;5;66;03m# Sparse categorical crossentropy expects a 3D target array\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m      6\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m      7\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m([encoder_input_val, decoder_input_val], decoder_output_val[:, :, np\u001b[38;5;241m.\u001b[39mnewaxis]),\n\u001b[0;32m      8\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[checkpoint]\n\u001b[0;32m      9\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freh_tensorflow\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freh_tensorflow\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freh_tensorflow\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freh_tensorflow\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freh_tensorflow\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freh_tensorflow\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freh_tensorflow\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freh_tensorflow\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freh_tensorflow\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freh_tensorflow\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1501\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1502\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1503\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1504\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1505\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1506\u001b[0m   )\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\freh_tensorflow\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    [encoder_input_train, decoder_input_train],\n",
    "    decoder_output_train[:, :, np.newaxis],  # Sparse categorical crossentropy expects a 3D target array\n",
    "    batch_size=64,\n",
    "    epochs=20,\n",
    "    validation_data=([encoder_input_val, decoder_input_val], decoder_output_val[:, :, np.newaxis]),\n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda22363-777c-4bac-b55c-b8ba47fd7175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c39826-1839-4bf9-9f32-9fc66393e49c",
   "metadata": {},
   "source": [
    "### Generating Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba8439-f79d-4b17-b107-f96d61e72dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoder model for inference\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Define decoder model for inference\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_inference_inputs = Input(shape=(None,))\n",
    "decoder_embedding_inference = decoder_embedding(decoder_inference_inputs)\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding_inference, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inference_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# Save token index mappings\n",
    "target_token_index = tokenizer.word_index\n",
    "reverse_target_token_index = {v: k for k, v in target_token_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171a9727-3484-4968-a1e8-b26e3dd695d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate responses\n",
    "def generate_response(input_seq: np.ndarray, max_decoder_seq_length: int) -> str:\n",
    "    # Encode the input sequence to get the internal states\n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "\n",
    "    # Generate empty target sequence of length 1 with only the start token\n",
    "    target_seq = np.ones((1, 1)) # Was np.zeros((1, 1))\n",
    "    target_seq[0, 0] = tokenizer.word_index['<START>']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
    "\n",
    "        # Sample a token and add the corresponding character to the decoded sentence\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_token_index[sampled_token_index]\n",
    "        decoded_sentence += ' ' + sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length or find stop token\n",
    "        if (sampled_char == '<END>' or len(decoded_sentence.split()) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1)\n",
    "        target_seq = np.ones((1, 1)) # Was np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence.strip().replace('<START>', '').replace('<END>', '').strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0f56fc-ba3e-40fd-b965-ba93098dda02",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b68f62-8d82-4ea3-9061-cb798fcef3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_preprocessing = False  # Expects spaCy to detect and remove names from the text\n",
    "max_length = 15\n",
    "\n",
    "# Define ten examples to test the model\n",
    "test_examples = [\n",
    "    \"How are you doing today?\",\n",
    "    \"What is your name?\",\n",
    "    \"Can you help me with my homework?\",\n",
    "    \"What is the weather like?\",\n",
    "    \"Tell me a joke.\",\n",
    "    \"Who is the president of the United States?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Do you like pizza?\",\n",
    "    \"What is your favorite color?\",\n",
    "    \"Goodbye!\"\n",
    "]\n",
    "\n",
    "# Preprocess input text\n",
    "input_text = [preprocess_text(text) for text in test_examples]\n",
    "# print(f\"Preprocessed text: {input_text}\")\n",
    "\n",
    "# Tokenize and pad the test examples\n",
    "test_sequences = tokenizer.texts_to_sequences(input_text)\n",
    "# print(f\"Tokenizer sequences: {test_sequences}\")\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=max_length, padding='pre', truncating='post')\n",
    "# print(f\"Padded sequences: {padded_test_sequences}\")\n",
    "\n",
    "# Generate responses\n",
    "for test_seq in padded_test_sequences:\n",
    "    input_seq = np.array([test_seq])\n",
    "    response = generate_response(input_seq, max_length)\n",
    "    print(f\"Input: {test_examples[padded_test_sequences.tolist().index(test_seq.tolist())]}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf3485-ef0e-4ced-a681-daaa0eca5012",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e2505-888d-4941-b041-3ae9dc061fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "# Saving .keras\n",
    "keras_file_path = os.path.join(data_dir, 's2s_model.keras')\n",
    "model.save(keras_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2069f0f9-6f2f-48b7-bc83-803df9ec8f3f",
   "metadata": {},
   "source": [
    "### Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822f5419-ee91-466f-988a-16f5731eab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def beam_search_decode(input_seq, beam_width=3, max_decoder_seq_length=15):\n",
    "    # Encode the input sequence to get the internal states\n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "\n",
    "    # Initialize the beams\n",
    "    start_token_index = tokenizer.word_index['<START>']\n",
    "    end_token_index = tokenizer.word_index['<END>']\n",
    "    beams = [(np.array([[start_token_index]]), states_value, 0.0)]  # (sequence, states, cumulative_probability)\n",
    "\n",
    "    for _ in range(max_decoder_seq_length):\n",
    "        all_candidates = []\n",
    "        for seq, states, score in beams:\n",
    "            if seq[0, -1] == end_token_index:\n",
    "                # If the beam already ended with the end token, add it to the candidates\n",
    "                all_candidates.append((seq, states, score))\n",
    "                continue\n",
    "            \n",
    "            # Predict the next token\n",
    "            output_tokens, h, c = decoder_model.predict([seq[:, -1:]] + states, verbose=0)\n",
    "            # Get the top beam_width predictions\n",
    "            top_k_indices = np.argsort(output_tokens[0, -1, :])[-beam_width:]\n",
    "            \n",
    "            # Create new beams for each prediction\n",
    "            for idx in top_k_indices:\n",
    "                new_seq = np.hstack([seq, np.array([[idx]])])\n",
    "                new_score = score + np.log(output_tokens[0, -1, idx])  # Use log to prevent underflow\n",
    "                all_candidates.append((new_seq, [h, c], new_score))\n",
    "        \n",
    "        # Select the top beam_width beams\n",
    "        beams = sorted(all_candidates, key=lambda x: x[2], reverse=True)[:beam_width]\n",
    "\n",
    "        # Check if all beams end with the end token\n",
    "        if all(seq[0, -1] == end_token_index for seq, _, _ in beams):\n",
    "            break\n",
    "\n",
    "    # Choose the best beam (highest score)\n",
    "    best_seq, _, _ = beams[0]\n",
    "    decoded_sentence = ' '.join([reverse_target_token_index[idx] for idx in best_seq[0] if idx != start_token_index and idx != end_token_index])\n",
    "    return decoded_sentence\n",
    "\n",
    "# Generate responses with beam search\n",
    "for test_seq in padded_test_sequences:\n",
    "    input_seq = np.array([test_seq])\n",
    "    response = beam_search_decode(input_seq, beam_width=3, max_decoder_seq_length=max_length)\n",
    "    print(f\"Input: {test_examples[padded_test_sequences.tolist().index(test_seq.tolist())]}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffb6119-08ec-4908-a626-d46c5a84c078",
   "metadata": {},
   "source": [
    "### Chatbot Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2f5c1f-4eab-47e1-8bf4-78f40bab2f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    print(\"Chatbot is ready! Type 'exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Argama: Goodbye!\")\n",
    "            print(\"Beamara: Goodbye!\")\n",
    "            break\n",
    "        input_text = preprocess_text(user_input)\n",
    "        input_sequence = [tokenizer.texts_to_sequences([input_text])[0]]\n",
    "        padded_input_sequence = pad_sequences(input_sequence, maxlen=max_length, padding='pre', truncating='post')\n",
    "        response = generate_response(np.array(padded_input_sequence), max_length)\n",
    "        response_2 = beam_search_decode(np.array(padded_input_sequence), beam_width=3, max_decoder_seq_length=max_length)\n",
    "        print(f\"Argama: {response}\")\n",
    "        print(f\"Beamara: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e38f011-2e17-4a9e-9ecc-5addaf88f125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "TensorFlow Keras version: 3.3.3\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"TensorFlow Keras version:\", tf.keras.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
