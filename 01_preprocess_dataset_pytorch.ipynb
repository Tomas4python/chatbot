{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f0fb967-b515-4054-9c06-c75938fa7c9f",
   "metadata": {},
   "source": [
    "# CAPSTONE PROJECT - CHATBOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5fa74bc-fba1-44b1-a9a1-c994c4d91e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe3579f-4161-47c3-8237-d609725e0964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16178972-a2e7-4e96-92a8-46f15515490a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Device: NVIDIA GeForce GTX 1660 Ti with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c593d8-dfc1-420a-8f40-7947c049dc69",
   "metadata": {},
   "source": [
    "## DATA\n",
    "### Data sources\n",
    "https://convokit.cornell.edu/documentation/movie.html <br>\n",
    "https://www.cs.cornell.edu/~cristian/Chameleons_in_imagined_conversations.html "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1328c9a6-4d8e-497b-b764-a9ab3eb9f243",
   "metadata": {},
   "source": [
    "### Install ConvoKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d5d481c-79d5-45d7-8883-aa169633c231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install convokit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a957699a-a417-4860-aee7-581bce869888",
   "metadata": {},
   "source": [
    "### Load data from source and save to 'data' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d1157ce-ff4a-440c-bfa4-7d6f0261e902",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from convokit import Corpus, download\n",
    "# import os\n",
    "\n",
    "# # Directory where to save the corpus\n",
    "# data_dir = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "# # Ensure the directory exists\n",
    "# if not os.path.exists(data_dir):\n",
    "#     os.makedirs(data_dir)\n",
    "\n",
    "# # Downloading and saving the corpus\n",
    "# corpus = Corpus(filename=download(\"movie-corpus\", data_dir=data_dir))\n",
    "\n",
    "# # Saving the corpus to the 'data' folder\n",
    "# corpus_path = os.path.join(data_dir, \"movie_corpus\")\n",
    "# corpus.dump(corpus_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9de684-120b-478a-8e60-345a3b392703",
   "metadata": {
    "tags": []
   },
   "source": [
    "Downloading movie-corpus to C:\\Users\\tomui\\Desktop\\capstone_project\\data\\movie-corpus  \n",
    "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0b8ca3-4757-49f0-826a-7a0a3824824a",
   "metadata": {},
   "source": [
    "### Load data from 'data' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8169d5a6-b021-4752-81ee-ed2f81b322e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Speakers: 9035\n",
      "Number of Utterances: 304713\n",
      "Number of Conversations: 83097\n"
     ]
    }
   ],
   "source": [
    "from convokit import Corpus\n",
    "import os\n",
    "\n",
    "# Directory where to load the corpus\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "# Load the corpus from the specified folder\n",
    "loaded_corpus = Corpus(filename=os.path.join(data_dir, \"movie_corpus\"))\n",
    "loaded_corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee27d1da-9ff5-468e-824b-6a02727e1535",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convokit.model.corpus.Corpus"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loaded_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568a5418-4b10-482f-9c59-685028b784df",
   "metadata": {},
   "source": [
    "### Data Structure and Organization\n",
    "```plaintext\n",
    "data/\n",
    "└── movie_corpus/\n",
    "    ├── conversations.json\n",
    "    ├── corpus.json\n",
    "    ├── index.json\n",
    "    ├── speakers.json\n",
    "    └── utterances.jsonl\n",
    "```\n",
    "\n",
    "Description [here](https://convokit.cornell.edu/documentation/movie.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92813c-7f90-4b46-bc21-9e814d5d82bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Choice for exploration\n",
    "The files I need from ConvoKit corpus for my chatbot project depend on the specific functionalities I want to implement in my chatbot. I'll most likely need `utterances.json` because it contains the dialogue data. This is what I'll use to train chatbot to understand and generate human-like responses.\n",
    "\n",
    "Description from source:  \n",
    "> \"Utterance-level information <br>\n",
    "> For each utterance, we provide:\n",
    "> - id: index of the utterance\n",
    "> - speaker: the speaker who authored the utterance\n",
    "> - conversation_id: id of the first utterance in the conversation this utterance belongs to\n",
    "> - reply_to: id of the utterance to which this utterance replies to (None if the utterance is not a reply)\n",
    "> - timestamp: time of the utterance\n",
    "> - text: textual content of the utterance\n",
    "> \n",
    "> Metadata for utterances include:\n",
    "> - movie_idx: index of the movie from which this utterance occurs\n",
    "> - parsed: parsed version of the utterance text, represented as a SpaCy Doc\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55bf77c-945b-4e5e-a56d-f8186c4d023c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Understanding data from `utterances.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d6acc31-6281-4a4c-8f06-40673e42bf61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint as pp\n",
    "\n",
    "# Initialize a list to hold all the utterances\n",
    "utterances = []\n",
    "\n",
    "# Open the file and read line by line\n",
    "with open(os.path.join(data_dir, 'movie_corpus', 'utterances.jsonl'), 'r') as file:\n",
    "    \n",
    "    for line in file:\n",
    "        utterance = json.loads(line)\n",
    "        utterances.append(utterance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c26682ea-4ebb-4705-9933-b91553312d2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(utterances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aac7a307-5edf-4bd6-a360-6def6f7523cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 304713 lines\n",
      "\n",
      "[{'conversation_id': 'L1044',\n",
      "  'id': 'L1045',\n",
      "  'meta': {'movie_id': 'm0',\n",
      "           'parsed': [{'rt': 1,\n",
      "                       'toks': [{'dep': 'nsubj',\n",
      "                                 'dn': [],\n",
      "                                 'tag': 'PRP',\n",
      "                                 'tok': 'They',\n",
      "                                 'up': 1},\n",
      "                                {'dep': 'ROOT',\n",
      "                                 'dn': [0, 2, 3],\n",
      "                                 'tag': 'VBP',\n",
      "                                 'tok': 'do'},\n",
      "                                {'dep': 'neg',\n",
      "                                 'dn': [],\n",
      "                                 'tag': 'RB',\n",
      "                                 'tok': 'not',\n",
      "                                 'up': 1},\n",
      "                                {'dep': 'punct',\n",
      "                                 'dn': [],\n",
      "                                 'tag': '.',\n",
      "                                 'tok': '!',\n",
      "                                 'up': 1}]}]},\n",
      "  'reply-to': 'L1044',\n",
      "  'speaker': 'u0',\n",
      "  'text': 'They do not!',\n",
      "  'timestamp': None,\n",
      "  'vectors': []},\n",
      " {'conversation_id': 'L1044',\n",
      "  'id': 'L1044',\n",
      "  'meta': {'movie_id': 'm0',\n",
      "           'parsed': [{'rt': 1,\n",
      "                       'toks': [{'dep': 'nsubj',\n",
      "                                 'dn': [],\n",
      "                                 'tag': 'PRP',\n",
      "                                 'tok': 'They',\n",
      "                                 'up': 1},\n",
      "                                {'dep': 'ROOT',\n",
      "                                 'dn': [0, 2, 3],\n",
      "                                 'tag': 'VBP',\n",
      "                                 'tok': 'do'},\n",
      "                                {'dep': 'dobj',\n",
      "                                 'dn': [],\n",
      "                                 'tag': 'TO',\n",
      "                                 'tok': 'to',\n",
      "                                 'up': 1},\n",
      "                                {'dep': 'punct',\n",
      "                                 'dn': [],\n",
      "                                 'tag': '.',\n",
      "                                 'tok': '!',\n",
      "                                 'up': 1}]}]},\n",
      "  'reply-to': None,\n",
      "  'speaker': 'u2',\n",
      "  'text': 'They do to!',\n",
      "  'timestamp': None,\n",
      "  'vectors': []},\n",
      " {'conversation_id': 'L984',\n",
      "  'id': 'L985',\n",
      "  'meta': {'movie_id': 'm0',\n",
      "           'parsed': [{'rt': 1,\n",
      "                       'toks': [{'dep': 'nsubj',\n",
      "                                 'dn': [],\n",
      "                                 'tag': 'PRP',\n",
      "                                 'tok': 'I',\n",
      "                                 'up': 1},\n",
      "                                {'dep': 'ROOT',\n",
      "                                 'dn': [0, 2, 3],\n",
      "                                 'tag': 'VBP',\n",
      "                                 'tok': 'hope'},\n",
      "                                {'dep': 'advmod',\n",
      "                                 'dn': [],\n",
      "                                 'tag': 'RB',\n",
      "                                 'tok': 'so',\n",
      "                                 'up': 1},\n",
      "                                {'dep': 'punct',\n",
      "                                 'dn': [],\n",
      "                                 'tag': '.',\n",
      "                                 'tok': '.',\n",
      "                                 'up': 1}]}]},\n",
      "  'reply-to': 'L984',\n",
      "  'speaker': 'u0',\n",
      "  'text': 'I hope so.',\n",
      "  'timestamp': None,\n",
      "  'vectors': []}]\n"
     ]
    }
   ],
   "source": [
    "print(f'There are a total of {len(utterances)} lines\\n')\n",
    "pp(utterances[: 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eac7ca7-8b69-4c5c-abd3-204b45d827bd",
   "metadata": {},
   "source": [
    "### Understanding data from other dataset json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65a6a94c-4467-4488-a2cb-8de7840acc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# with open(os.path.join(data_dir, 'movie_corpus', 'conversations.json'), 'r') as file:\n",
    "#     conversations = json.load(file)\n",
    "# with open(os.path.join(data_dir, 'movie_corpus', 'corpus.json'), 'r') as file:\n",
    "#     conversations = json.load(file)\n",
    "# with open(os.path.join(data_dir, 'movie_corpus', 'index.json'), 'r') as file:\n",
    "#     conversations = json.load(file)\n",
    "# with open(os.path.join(data_dir, 'movie_corpus', 'speakers.json'), 'r') as file:\n",
    "#     conversations = json.load(file)\n",
    "\n",
    "# print(type(conversations))\n",
    "\n",
    "# print(f'There are a total of {len(conversations)} keys in the dictionary\\n')\n",
    "# first_three_items = list(conversations.items())[:3]\n",
    "# pp(first_three_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55568df2-4bb1-4526-907a-455e217fe2d4",
   "metadata": {},
   "source": [
    "## Decision regarding data\n",
    "\n",
    "In developing the chatbot I made the decision to collect only data from the utterances.json file to ensure the chatbot can effectively manage and understand multi-turn conversations. The essential data elements to be gathered include `'text'` for generating responses, `'conversation_id'` for tracking the flow of conversations, and `'reply_to'` for understanding response sequences within the dialogue. While initially, the chatbot will not utilize complex NLP features like parsed linguistic data, the architecture will allow for the integration of these advanced features in the future. While initially I will collect `'parsed'` and `'toks'` information from the utterances.json file, the decision on whether to use this pre-parsed data directly, generate similar data anew, or conduct comparisons between the two will be made later as the project evolves. This approach ensures flexibility in utilizing advanced NLP features as required, maintaining the adaptability of the architecture for future enhancements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a7d75-7c31-4ebc-ae31-904ad911e3a6",
   "metadata": {},
   "source": [
    "## Converting utterances data to DataFrame\n",
    "Pandas provides a powerful and easy-to-use interface for data manipulation, filtering, transformation, and analysis, and integration with Python Ecosystem: seamless integration with other Python libraries for data analysis, machine learning (e.g., scikit-learn, TensorFlow), and visualization (e.g., Matplotlib, Seaborn), as well fast processing for datasets that fit comfortably in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5336f550-8d49-4abe-86e0-4517a993cf71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Flatten the data\n",
    "def flatten_data(data):\n",
    "    flattened_data = []\n",
    "    for entry in data:\n",
    "        flat_entry = {\n",
    "            'id': entry['id'],\n",
    "            'conversation_id': entry['conversation_id'],\n",
    "            'text': entry['text'],\n",
    "            'speaker': entry['speaker'],\n",
    "            'reply_to': entry.get('reply-to'),\n",
    "            'timestamp': entry['timestamp'],\n",
    "            'movie_id': entry['meta']['movie_id'],\n",
    "        }\n",
    "        # Handle nested parsed data\n",
    "        for parsed in entry['meta']['parsed']:\n",
    "            for idx, tok in enumerate(parsed['toks']):\n",
    "                flat_entry[f'tok_{idx}_token'] = tok['tok']\n",
    "                flat_entry[f'tok_{idx}_tag'] = tok['tag']\n",
    "                flat_entry[f'tok_{idx}_dep'] = tok['dep']\n",
    "                # Add other fields from tokens as needed\n",
    "        flattened_data.append(flat_entry)\n",
    "    return flattened_data\n",
    "\n",
    "# Convert to DataFrame\n",
    "flattened_data = flatten_data(utterances)\n",
    "df = pd.DataFrame(flattened_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08e5e09e-d6e0-45d9-b9d8-767c441470be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>tok_0_token</th>\n",
       "      <th>tok_0_tag</th>\n",
       "      <th>tok_0_dep</th>\n",
       "      <th>...</th>\n",
       "      <th>tok_121_dep</th>\n",
       "      <th>tok_122_token</th>\n",
       "      <th>tok_122_tag</th>\n",
       "      <th>tok_122_dep</th>\n",
       "      <th>tok_123_token</th>\n",
       "      <th>tok_123_tag</th>\n",
       "      <th>tok_123_dep</th>\n",
       "      <th>tok_124_token</th>\n",
       "      <th>tok_124_tag</th>\n",
       "      <th>tok_124_dep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1045</td>\n",
       "      <td>L1044</td>\n",
       "      <td>They do not!</td>\n",
       "      <td>u0</td>\n",
       "      <td>L1044</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>They</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "      <td>They do to!</td>\n",
       "      <td>u2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>They</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L985</td>\n",
       "      <td>L984</td>\n",
       "      <td>I hope so.</td>\n",
       "      <td>u0</td>\n",
       "      <td>L984</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "      <td>She okay?</td>\n",
       "      <td>u2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>She</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L925</td>\n",
       "      <td>L924</td>\n",
       "      <td>Let's go.</td>\n",
       "      <td>u0</td>\n",
       "      <td>L924</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>Let</td>\n",
       "      <td>VB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 382 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id conversation_id          text speaker reply_to timestamp movie_id  \\\n",
       "0  L1045           L1044  They do not!      u0    L1044      None       m0   \n",
       "1  L1044           L1044   They do to!      u2     None      None       m0   \n",
       "2   L985            L984    I hope so.      u0     L984      None       m0   \n",
       "3   L984            L984     She okay?      u2     None      None       m0   \n",
       "4   L925            L924     Let's go.      u0     L924      None       m0   \n",
       "\n",
       "  tok_0_token tok_0_tag tok_0_dep  ... tok_121_dep tok_122_token tok_122_tag  \\\n",
       "0        They       PRP     nsubj  ...         NaN           NaN         NaN   \n",
       "1        They       PRP     nsubj  ...         NaN           NaN         NaN   \n",
       "2           I       PRP     nsubj  ...         NaN           NaN         NaN   \n",
       "3         She       PRP     nsubj  ...         NaN           NaN         NaN   \n",
       "4         Let        VB      ROOT  ...         NaN           NaN         NaN   \n",
       "\n",
       "  tok_122_dep tok_123_token tok_123_tag tok_123_dep tok_124_token tok_124_tag  \\\n",
       "0         NaN           NaN         NaN         NaN           NaN         NaN   \n",
       "1         NaN           NaN         NaN         NaN           NaN         NaN   \n",
       "2         NaN           NaN         NaN         NaN           NaN         NaN   \n",
       "3         NaN           NaN         NaN         NaN           NaN         NaN   \n",
       "4         NaN           NaN         NaN         NaN           NaN         NaN   \n",
       "\n",
       "  tok_124_dep  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  \n",
       "\n",
       "[5 rows x 382 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show DataFrame to check structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2845afc3-b7dc-403a-ade7-79a83c710e00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 304713 entries, 0 to 304712\n",
      "Columns: 382 entries, id to tok_124_dep\n",
      "dtypes: object(382)\n",
      "memory usage: 888.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37724f59-ec6d-4493-b7eb-ec280663b221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                      0\n",
      "conversation_id         0\n",
      "text                    0\n",
      "speaker                 0\n",
      "reply_to            83097\n",
      "timestamp          304713\n",
      "movie_id                0\n",
      "tok_0_token           267\n",
      "tok_0_tag             267\n",
      "tok_0_dep             267\n",
      "tok_1_token           625\n",
      "tok_1_tag             625\n",
      "tok_1_dep             625\n",
      "tok_2_token         21729\n",
      "tok_2_tag           21729\n",
      "tok_2_dep           21729\n",
      "tok_3_token         38515\n",
      "tok_3_tag           38515\n",
      "tok_3_dep           38515\n",
      "tok_4_token         63316\n",
      "tok_4_tag           63316\n",
      "tok_4_dep           63316\n",
      "tok_5_token         92404\n",
      "tok_5_tag           92404\n",
      "tok_5_dep           92404\n",
      "tok_6_token        121962\n",
      "tok_6_tag          121962\n",
      "tok_6_dep          121962\n",
      "tok_7_token        149962\n",
      "tok_7_tag          149962\n",
      "tok_7_dep          149962\n",
      "tok_8_token        175281\n",
      "tok_8_tag          175281\n",
      "tok_8_dep          175281\n",
      "tok_9_token        196627\n",
      "tok_9_tag          196627\n",
      "tok_9_dep          196627\n",
      "tok_10_token       214433\n",
      "tok_10_tag         214433\n",
      "tok_10_dep         214433\n",
      "tok_11_token       229294\n",
      "tok_11_tag         229294\n",
      "tok_11_dep         229294\n",
      "tok_12_token       241453\n",
      "tok_12_tag         241453\n",
      "tok_12_dep         241453\n",
      "tok_13_token       251574\n",
      "tok_13_tag         251574\n",
      "tok_13_dep         251574\n",
      "tok_14_token       260081\n",
      "tok_14_tag         260081\n",
      "tok_14_dep         260081\n",
      "tok_15_token       267106\n",
      "tok_15_tag         267106\n",
      "tok_15_dep         267106\n",
      "tok_16_token       273061\n",
      "tok_16_tag         273061\n",
      "tok_16_dep         273061\n",
      "tok_17_token       278121\n",
      "tok_17_tag         278121\n",
      "tok_17_dep         278121\n",
      "tok_18_token       282396\n",
      "tok_18_tag         282396\n",
      "tok_18_dep         282396\n",
      "tok_19_token       285961\n",
      "tok_19_tag         285961\n",
      "tok_19_dep         285961\n",
      "tok_20_token       288804\n",
      "tok_20_tag         288804\n",
      "tok_20_dep         288804\n",
      "tok_21_token       291267\n",
      "tok_21_tag         291267\n",
      "tok_21_dep         291267\n",
      "tok_22_token       293375\n",
      "tok_22_tag         293375\n",
      "tok_22_dep         293375\n",
      "tok_23_token       295125\n",
      "tok_23_tag         295125\n",
      "tok_23_dep         295125\n",
      "tok_24_token       296564\n",
      "tok_24_tag         296564\n",
      "tok_24_dep         296564\n",
      "tok_25_token       297736\n",
      "tok_25_tag         297736\n",
      "tok_25_dep         297736\n",
      "tok_26_token       298756\n",
      "tok_26_tag         298756\n",
      "tok_26_dep         298756\n",
      "tok_27_token       299618\n",
      "tok_27_tag         299618\n",
      "tok_27_dep         299618\n",
      "tok_28_token       300391\n",
      "tok_28_tag         300391\n",
      "tok_28_dep         300391\n",
      "tok_29_token       301006\n",
      "tok_29_tag         301006\n",
      "tok_29_dep         301006\n",
      "tok_30_token       301511\n",
      "tok_30_tag         301511\n",
      "tok_30_dep         301511\n",
      "tok_31_token       301953\n",
      "tok_31_tag         301953\n",
      "tok_31_dep         301953\n",
      "tok_32_token       302347\n",
      "tok_32_tag         302347\n",
      "tok_32_dep         302347\n",
      "tok_33_token       302679\n",
      "tok_33_tag         302679\n",
      "tok_33_dep         302679\n",
      "tok_34_token       302947\n",
      "tok_34_tag         302947\n",
      "tok_34_dep         302947\n",
      "tok_35_token       303177\n",
      "tok_35_tag         303177\n",
      "tok_35_dep         303177\n",
      "tok_36_token       303366\n",
      "tok_36_tag         303366\n",
      "tok_36_dep         303366\n",
      "tok_37_token       303540\n",
      "tok_37_tag         303540\n",
      "tok_37_dep         303540\n",
      "tok_38_token       303700\n",
      "tok_38_tag         303700\n",
      "tok_38_dep         303700\n",
      "tok_39_token       303841\n",
      "tok_39_tag         303841\n",
      "tok_39_dep         303841\n",
      "tok_40_token       303935\n",
      "tok_40_tag         303935\n",
      "tok_40_dep         303935\n",
      "tok_41_token       304030\n",
      "tok_41_tag         304030\n",
      "tok_41_dep         304030\n",
      "tok_42_token       304127\n",
      "tok_42_tag         304127\n",
      "tok_42_dep         304127\n",
      "tok_43_token       304190\n",
      "tok_43_tag         304190\n",
      "tok_43_dep         304190\n",
      "tok_44_token       304252\n",
      "tok_44_tag         304252\n",
      "tok_44_dep         304252\n",
      "tok_45_token       304309\n",
      "tok_45_tag         304309\n",
      "tok_45_dep         304309\n",
      "tok_46_token       304355\n",
      "tok_46_tag         304355\n",
      "tok_46_dep         304355\n",
      "tok_47_token       304400\n",
      "tok_47_tag         304400\n",
      "tok_47_dep         304400\n",
      "tok_48_token       304429\n",
      "tok_48_tag         304429\n",
      "tok_48_dep         304429\n",
      "tok_49_token       304459\n",
      "tok_49_tag         304459\n",
      "tok_49_dep         304459\n",
      "tok_50_token       304484\n",
      "tok_50_tag         304484\n",
      "tok_50_dep         304484\n",
      "tok_51_token       304510\n",
      "tok_51_tag         304510\n",
      "tok_51_dep         304510\n",
      "tok_52_token       304531\n",
      "tok_52_tag         304531\n",
      "tok_52_dep         304531\n",
      "tok_53_token       304550\n",
      "tok_53_tag         304550\n",
      "tok_53_dep         304550\n",
      "tok_54_token       304569\n",
      "tok_54_tag         304569\n",
      "tok_54_dep         304569\n",
      "tok_55_token       304582\n",
      "tok_55_tag         304582\n",
      "tok_55_dep         304582\n",
      "tok_56_token       304594\n",
      "tok_56_tag         304594\n",
      "tok_56_dep         304594\n",
      "tok_57_token       304606\n",
      "tok_57_tag         304606\n",
      "tok_57_dep         304606\n",
      "tok_58_token       304615\n",
      "tok_58_tag         304615\n",
      "tok_58_dep         304615\n",
      "tok_59_token       304623\n",
      "tok_59_tag         304623\n",
      "tok_59_dep         304623\n",
      "tok_60_token       304636\n",
      "tok_60_tag         304636\n",
      "tok_60_dep         304636\n",
      "tok_61_token       304640\n",
      "tok_61_tag         304640\n",
      "tok_61_dep         304640\n",
      "tok_62_token       304646\n",
      "tok_62_tag         304646\n",
      "tok_62_dep         304646\n",
      "tok_63_token       304654\n",
      "tok_63_tag         304654\n",
      "tok_63_dep         304654\n",
      "tok_64_token       304664\n",
      "tok_64_tag         304664\n",
      "tok_64_dep         304664\n",
      "tok_65_token       304669\n",
      "tok_65_tag         304669\n",
      "tok_65_dep         304669\n",
      "tok_66_token       304675\n",
      "tok_66_tag         304675\n",
      "tok_66_dep         304675\n",
      "tok_67_token       304678\n",
      "tok_67_tag         304678\n",
      "tok_67_dep         304678\n",
      "tok_68_token       304680\n",
      "tok_68_tag         304680\n",
      "tok_68_dep         304680\n",
      "tok_69_token       304686\n",
      "tok_69_tag         304686\n",
      "tok_69_dep         304686\n",
      "tok_70_token       304690\n",
      "tok_70_tag         304690\n",
      "tok_70_dep         304690\n",
      "tok_71_token       304692\n",
      "tok_71_tag         304692\n",
      "tok_71_dep         304692\n",
      "tok_72_token       304694\n",
      "tok_72_tag         304694\n",
      "tok_72_dep         304694\n",
      "tok_73_token       304696\n",
      "tok_73_tag         304696\n",
      "tok_73_dep         304696\n",
      "tok_74_token       304697\n",
      "tok_74_tag         304697\n",
      "tok_74_dep         304697\n",
      "tok_75_token       304698\n",
      "tok_75_tag         304698\n",
      "tok_75_dep         304698\n",
      "tok_76_token       304702\n",
      "tok_76_tag         304702\n",
      "tok_76_dep         304702\n",
      "tok_77_token       304702\n",
      "tok_77_tag         304702\n",
      "tok_77_dep         304702\n",
      "tok_78_token       304705\n",
      "tok_78_tag         304705\n",
      "tok_78_dep         304705\n",
      "tok_79_token       304706\n",
      "tok_79_tag         304706\n",
      "tok_79_dep         304706\n",
      "tok_80_token       304707\n",
      "tok_80_tag         304707\n",
      "tok_80_dep         304707\n",
      "tok_81_token       304707\n",
      "tok_81_tag         304707\n",
      "tok_81_dep         304707\n",
      "tok_82_token       304707\n",
      "tok_82_tag         304707\n",
      "tok_82_dep         304707\n",
      "tok_83_token       304707\n",
      "tok_83_tag         304707\n",
      "tok_83_dep         304707\n",
      "tok_84_token       304707\n",
      "tok_84_tag         304707\n",
      "tok_84_dep         304707\n",
      "tok_85_token       304708\n",
      "tok_85_tag         304708\n",
      "tok_85_dep         304708\n",
      "tok_86_token       304708\n",
      "tok_86_tag         304708\n",
      "tok_86_dep         304708\n",
      "tok_87_token       304709\n",
      "tok_87_tag         304709\n",
      "tok_87_dep         304709\n",
      "tok_88_token       304709\n",
      "tok_88_tag         304709\n",
      "tok_88_dep         304709\n",
      "tok_89_token       304710\n",
      "tok_89_tag         304710\n",
      "tok_89_dep         304710\n",
      "tok_90_token       304710\n",
      "tok_90_tag         304710\n",
      "tok_90_dep         304710\n",
      "tok_91_token       304711\n",
      "tok_91_tag         304711\n",
      "tok_91_dep         304711\n",
      "tok_92_token       304711\n",
      "tok_92_tag         304711\n",
      "tok_92_dep         304711\n",
      "tok_93_token       304711\n",
      "tok_93_tag         304711\n",
      "tok_93_dep         304711\n",
      "tok_94_token       304711\n",
      "tok_94_tag         304711\n",
      "tok_94_dep         304711\n",
      "tok_95_token       304711\n",
      "tok_95_tag         304711\n",
      "tok_95_dep         304711\n",
      "tok_96_token       304711\n",
      "tok_96_tag         304711\n",
      "tok_96_dep         304711\n",
      "tok_97_token       304711\n",
      "tok_97_tag         304711\n",
      "tok_97_dep         304711\n",
      "tok_98_token       304711\n",
      "tok_98_tag         304711\n",
      "tok_98_dep         304711\n",
      "tok_99_token       304711\n",
      "tok_99_tag         304711\n",
      "tok_99_dep         304711\n",
      "tok_100_token      304711\n",
      "tok_100_tag        304711\n",
      "tok_100_dep        304711\n",
      "tok_101_token      304712\n",
      "tok_101_tag        304712\n",
      "tok_101_dep        304712\n",
      "tok_102_token      304712\n",
      "tok_102_tag        304712\n",
      "tok_102_dep        304712\n",
      "tok_103_token      304712\n",
      "tok_103_tag        304712\n",
      "tok_103_dep        304712\n",
      "tok_104_token      304712\n",
      "tok_104_tag        304712\n",
      "tok_104_dep        304712\n",
      "tok_105_token      304712\n",
      "tok_105_tag        304712\n",
      "tok_105_dep        304712\n",
      "tok_106_token      304712\n",
      "tok_106_tag        304712\n",
      "tok_106_dep        304712\n",
      "tok_107_token      304712\n",
      "tok_107_tag        304712\n",
      "tok_107_dep        304712\n",
      "tok_108_token      304712\n",
      "tok_108_tag        304712\n",
      "tok_108_dep        304712\n",
      "tok_109_token      304712\n",
      "tok_109_tag        304712\n",
      "tok_109_dep        304712\n",
      "tok_110_token      304712\n",
      "tok_110_tag        304712\n",
      "tok_110_dep        304712\n",
      "tok_111_token      304712\n",
      "tok_111_tag        304712\n",
      "tok_111_dep        304712\n",
      "tok_112_token      304712\n",
      "tok_112_tag        304712\n",
      "tok_112_dep        304712\n",
      "tok_113_token      304712\n",
      "tok_113_tag        304712\n",
      "tok_113_dep        304712\n",
      "tok_114_token      304712\n",
      "tok_114_tag        304712\n",
      "tok_114_dep        304712\n",
      "tok_115_token      304712\n",
      "tok_115_tag        304712\n",
      "tok_115_dep        304712\n",
      "tok_116_token      304712\n",
      "tok_116_tag        304712\n",
      "tok_116_dep        304712\n",
      "tok_117_token      304712\n",
      "tok_117_tag        304712\n",
      "tok_117_dep        304712\n",
      "tok_118_token      304712\n",
      "tok_118_tag        304712\n",
      "tok_118_dep        304712\n",
      "tok_119_token      304712\n",
      "tok_119_tag        304712\n",
      "tok_119_dep        304712\n",
      "tok_120_token      304712\n",
      "tok_120_tag        304712\n",
      "tok_120_dep        304712\n",
      "tok_121_token      304712\n",
      "tok_121_tag        304712\n",
      "tok_121_dep        304712\n",
      "tok_122_token      304712\n",
      "tok_122_tag        304712\n",
      "tok_122_dep        304712\n",
      "tok_123_token      304712\n",
      "tok_123_tag        304712\n",
      "tok_123_dep        304712\n",
      "tok_124_token      304712\n",
      "tok_124_tag        304712\n",
      "tok_124_dep        304712\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Temporarily adjust display settings to show all columns\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(df.isnull().sum())\n",
    "#print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6acbac4-a8a0-4380-9083-b672443d8459",
   "metadata": {},
   "source": [
    "## Saving the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e9f5895-8d1a-4900-9e50-8c6062ce1348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving the DataFrame\n",
    "file_path_parquet = os.path.join(data_dir, 'utterances.parquet')\n",
    "df.to_parquet(file_path_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df029d37-dc2e-4173-87ce-7c7f4d48ccc2",
   "metadata": {},
   "source": [
    "`utterances.jsonl` - 351 404 KB, `utterances.parquet` - 28 409 KB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30907873-7514-495a-90aa-1e9f247cf987",
   "metadata": {},
   "source": [
    "## Loading the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93a10f80-f191-48a5-99f0-4930f607c96b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>tok_0_token</th>\n",
       "      <th>tok_0_tag</th>\n",
       "      <th>tok_0_dep</th>\n",
       "      <th>...</th>\n",
       "      <th>tok_121_dep</th>\n",
       "      <th>tok_122_token</th>\n",
       "      <th>tok_122_tag</th>\n",
       "      <th>tok_122_dep</th>\n",
       "      <th>tok_123_token</th>\n",
       "      <th>tok_123_tag</th>\n",
       "      <th>tok_123_dep</th>\n",
       "      <th>tok_124_token</th>\n",
       "      <th>tok_124_tag</th>\n",
       "      <th>tok_124_dep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1045</td>\n",
       "      <td>L1044</td>\n",
       "      <td>They do not!</td>\n",
       "      <td>u0</td>\n",
       "      <td>L1044</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>They</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "      <td>They do to!</td>\n",
       "      <td>u2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>They</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L985</td>\n",
       "      <td>L984</td>\n",
       "      <td>I hope so.</td>\n",
       "      <td>u0</td>\n",
       "      <td>L984</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "      <td>She okay?</td>\n",
       "      <td>u2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>She</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L925</td>\n",
       "      <td>L924</td>\n",
       "      <td>Let's go.</td>\n",
       "      <td>u0</td>\n",
       "      <td>L924</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>Let</td>\n",
       "      <td>VB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>L924</td>\n",
       "      <td>L924</td>\n",
       "      <td>Wow</td>\n",
       "      <td>u2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>Wow</td>\n",
       "      <td>UH</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>L872</td>\n",
       "      <td>L870</td>\n",
       "      <td>Okay -- you're gonna need to learn how to lie.</td>\n",
       "      <td>u0</td>\n",
       "      <td>L871</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>Okay</td>\n",
       "      <td>UH</td>\n",
       "      <td>intj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>L871</td>\n",
       "      <td>L870</td>\n",
       "      <td>No</td>\n",
       "      <td>u2</td>\n",
       "      <td>L870</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>No</td>\n",
       "      <td>UH</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "      <td>I'm kidding.  You know how sometimes you just ...</td>\n",
       "      <td>u0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>And</td>\n",
       "      <td>CC</td>\n",
       "      <td>cc</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>L869</td>\n",
       "      <td>L866</td>\n",
       "      <td>Like my fear of wearing pastels?</td>\n",
       "      <td>u0</td>\n",
       "      <td>L868</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>Like</td>\n",
       "      <td>IN</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>L868</td>\n",
       "      <td>L866</td>\n",
       "      <td>The \"real you\".</td>\n",
       "      <td>u2</td>\n",
       "      <td>L867</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>L867</td>\n",
       "      <td>L866</td>\n",
       "      <td>What good stuff?</td>\n",
       "      <td>u0</td>\n",
       "      <td>L866</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>What</td>\n",
       "      <td>WDT</td>\n",
       "      <td>det</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>L866</td>\n",
       "      <td>L866</td>\n",
       "      <td>I figured you'd get to the good stuff eventually.</td>\n",
       "      <td>u2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>L865</td>\n",
       "      <td>L862</td>\n",
       "      <td>Thank God!  If I had to hear one more story ab...</td>\n",
       "      <td>u2</td>\n",
       "      <td>L864</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>If</td>\n",
       "      <td>IN</td>\n",
       "      <td>mark</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>L864</td>\n",
       "      <td>L862</td>\n",
       "      <td>Me.  This endless ...blonde babble. I'm like, ...</td>\n",
       "      <td>u0</td>\n",
       "      <td>L863</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>L863</td>\n",
       "      <td>L862</td>\n",
       "      <td>What crap?</td>\n",
       "      <td>u2</td>\n",
       "      <td>L862</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>What</td>\n",
       "      <td>WDT</td>\n",
       "      <td>det</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>L862</td>\n",
       "      <td>L862</td>\n",
       "      <td>do you listen to this crap?</td>\n",
       "      <td>u0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>do</td>\n",
       "      <td>VBP</td>\n",
       "      <td>aux</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>L861</td>\n",
       "      <td>L860</td>\n",
       "      <td>No...</td>\n",
       "      <td>u2</td>\n",
       "      <td>L860</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>No</td>\n",
       "      <td>UH</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>L860</td>\n",
       "      <td>L860</td>\n",
       "      <td>Then Guillermo says, \"If you go any lighter, y...</td>\n",
       "      <td>u0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>Then</td>\n",
       "      <td>RB</td>\n",
       "      <td>advmod</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>L699</td>\n",
       "      <td>L696</td>\n",
       "      <td>You always been this selfish?</td>\n",
       "      <td>u2</td>\n",
       "      <td>L698</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>You</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 382 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id conversation_id                                               text  \\\n",
       "0   L1045           L1044                                       They do not!   \n",
       "1   L1044           L1044                                        They do to!   \n",
       "2    L985            L984                                         I hope so.   \n",
       "3    L984            L984                                          She okay?   \n",
       "4    L925            L924                                          Let's go.   \n",
       "5    L924            L924                                                Wow   \n",
       "6    L872            L870     Okay -- you're gonna need to learn how to lie.   \n",
       "7    L871            L870                                                 No   \n",
       "8    L870            L870  I'm kidding.  You know how sometimes you just ...   \n",
       "9    L869            L866                   Like my fear of wearing pastels?   \n",
       "10   L868            L866                                    The \"real you\".   \n",
       "11   L867            L866                                   What good stuff?   \n",
       "12   L866            L866  I figured you'd get to the good stuff eventually.   \n",
       "13   L865            L862  Thank God!  If I had to hear one more story ab...   \n",
       "14   L864            L862  Me.  This endless ...blonde babble. I'm like, ...   \n",
       "15   L863            L862                                         What crap?   \n",
       "16   L862            L862                        do you listen to this crap?   \n",
       "17   L861            L860                                              No...   \n",
       "18   L860            L860  Then Guillermo says, \"If you go any lighter, y...   \n",
       "19   L699            L696                      You always been this selfish?   \n",
       "\n",
       "   speaker reply_to timestamp movie_id tok_0_token tok_0_tag tok_0_dep  ...  \\\n",
       "0       u0    L1044      None       m0        They       PRP     nsubj  ...   \n",
       "1       u2     None      None       m0        They       PRP     nsubj  ...   \n",
       "2       u0     L984      None       m0           I       PRP     nsubj  ...   \n",
       "3       u2     None      None       m0         She       PRP     nsubj  ...   \n",
       "4       u0     L924      None       m0         Let        VB      ROOT  ...   \n",
       "5       u2     None      None       m0         Wow        UH      ROOT  ...   \n",
       "6       u0     L871      None       m0        Okay        UH      intj  ...   \n",
       "7       u2     L870      None       m0          No        UH      ROOT  ...   \n",
       "8       u0     None      None       m0         And        CC        cc  ...   \n",
       "9       u0     L868      None       m0        Like        IN      ROOT  ...   \n",
       "10      u2     L867      None       m0         The        DT       det  ...   \n",
       "11      u0     L866      None       m0        What       WDT       det  ...   \n",
       "12      u2     None      None       m0           I       PRP     nsubj  ...   \n",
       "13      u2     L864      None       m0          If        IN      mark  ...   \n",
       "14      u0     L863      None       m0           I       PRP     nsubj  ...   \n",
       "15      u2     L862      None       m0        What       WDT       det  ...   \n",
       "16      u0     None      None       m0          do       VBP       aux  ...   \n",
       "17      u2     L860      None       m0          No        UH      ROOT  ...   \n",
       "18      u0     None      None       m0        Then        RB    advmod  ...   \n",
       "19      u2     L698      None       m0         You       PRP     nsubj  ...   \n",
       "\n",
       "   tok_121_dep tok_122_token tok_122_tag tok_122_dep tok_123_token  \\\n",
       "0         None          None        None        None          None   \n",
       "1         None          None        None        None          None   \n",
       "2         None          None        None        None          None   \n",
       "3         None          None        None        None          None   \n",
       "4         None          None        None        None          None   \n",
       "5         None          None        None        None          None   \n",
       "6         None          None        None        None          None   \n",
       "7         None          None        None        None          None   \n",
       "8         None          None        None        None          None   \n",
       "9         None          None        None        None          None   \n",
       "10        None          None        None        None          None   \n",
       "11        None          None        None        None          None   \n",
       "12        None          None        None        None          None   \n",
       "13        None          None        None        None          None   \n",
       "14        None          None        None        None          None   \n",
       "15        None          None        None        None          None   \n",
       "16        None          None        None        None          None   \n",
       "17        None          None        None        None          None   \n",
       "18        None          None        None        None          None   \n",
       "19        None          None        None        None          None   \n",
       "\n",
       "   tok_123_tag tok_123_dep tok_124_token tok_124_tag tok_124_dep  \n",
       "0         None        None          None        None        None  \n",
       "1         None        None          None        None        None  \n",
       "2         None        None          None        None        None  \n",
       "3         None        None          None        None        None  \n",
       "4         None        None          None        None        None  \n",
       "5         None        None          None        None        None  \n",
       "6         None        None          None        None        None  \n",
       "7         None        None          None        None        None  \n",
       "8         None        None          None        None        None  \n",
       "9         None        None          None        None        None  \n",
       "10        None        None          None        None        None  \n",
       "11        None        None          None        None        None  \n",
       "12        None        None          None        None        None  \n",
       "13        None        None          None        None        None  \n",
       "14        None        None          None        None        None  \n",
       "15        None        None          None        None        None  \n",
       "16        None        None          None        None        None  \n",
       "17        None        None          None        None        None  \n",
       "18        None        None          None        None        None  \n",
       "19        None        None          None        None        None  \n",
       "\n",
       "[20 rows x 382 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the DataFrame\n",
    "file_path_parquet = os.path.join(data_dir, 'utterances.parquet')\n",
    "df_loaded_parquet = pd.read_parquet(file_path_parquet)\n",
    "\n",
    "df_loaded_parquet.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2977c6-4473-451b-a36e-bb2c7a6b65f0",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab66833-1b80-4eaa-913b-ff5c938cca74",
   "metadata": {},
   "source": [
    "### Leaving only necessary data for initial stage of the project\n",
    "Id, conversation_id for tracking the flow of conversations and reply_to for understanding the sequence within the dialogue, and conversation text ofcourse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed419e17-c695-4d32-a6f9-8ad8ac93e68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>reply_to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They do not!</td>\n",
       "      <td>L1045</td>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They do to!</td>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I hope so.</td>\n",
       "      <td>L985</td>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>She okay?</td>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Let's go.</td>\n",
       "      <td>L925</td>\n",
       "      <td>L924</td>\n",
       "      <td>L924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wow</td>\n",
       "      <td>L924</td>\n",
       "      <td>L924</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Okay -- you're gonna need to learn how to lie.</td>\n",
       "      <td>L872</td>\n",
       "      <td>L870</td>\n",
       "      <td>L871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>No</td>\n",
       "      <td>L871</td>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I'm kidding.  You know how sometimes you just ...</td>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Like my fear of wearing pastels?</td>\n",
       "      <td>L869</td>\n",
       "      <td>L866</td>\n",
       "      <td>L868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The \"real you\".</td>\n",
       "      <td>L868</td>\n",
       "      <td>L866</td>\n",
       "      <td>L867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What good stuff?</td>\n",
       "      <td>L867</td>\n",
       "      <td>L866</td>\n",
       "      <td>L866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I figured you'd get to the good stuff eventually.</td>\n",
       "      <td>L866</td>\n",
       "      <td>L866</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Thank God!  If I had to hear one more story ab...</td>\n",
       "      <td>L865</td>\n",
       "      <td>L862</td>\n",
       "      <td>L864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Me.  This endless ...blonde babble. I'm like, ...</td>\n",
       "      <td>L864</td>\n",
       "      <td>L862</td>\n",
       "      <td>L863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What crap?</td>\n",
       "      <td>L863</td>\n",
       "      <td>L862</td>\n",
       "      <td>L862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>do you listen to this crap?</td>\n",
       "      <td>L862</td>\n",
       "      <td>L862</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>No...</td>\n",
       "      <td>L861</td>\n",
       "      <td>L860</td>\n",
       "      <td>L860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Then Guillermo says, \"If you go any lighter, y...</td>\n",
       "      <td>L860</td>\n",
       "      <td>L860</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>You always been this selfish?</td>\n",
       "      <td>L699</td>\n",
       "      <td>L696</td>\n",
       "      <td>L698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>But</td>\n",
       "      <td>L698</td>\n",
       "      <td>L696</td>\n",
       "      <td>L697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Then that's all you had to say.</td>\n",
       "      <td>L697</td>\n",
       "      <td>L696</td>\n",
       "      <td>L696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Well, no...</td>\n",
       "      <td>L696</td>\n",
       "      <td>L696</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>You never wanted to go out with 'me, did you?</td>\n",
       "      <td>L695</td>\n",
       "      <td>L693</td>\n",
       "      <td>L694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I was?</td>\n",
       "      <td>L694</td>\n",
       "      <td>L693</td>\n",
       "      <td>L693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I looked for you back at the party, but you al...</td>\n",
       "      <td>L693</td>\n",
       "      <td>L693</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Tons</td>\n",
       "      <td>L663</td>\n",
       "      <td>L662</td>\n",
       "      <td>L662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Have fun tonight?</td>\n",
       "      <td>L662</td>\n",
       "      <td>L662</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>I believe we share an art instructor</td>\n",
       "      <td>L578</td>\n",
       "      <td>L577</td>\n",
       "      <td>L577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>You know Chastity?</td>\n",
       "      <td>L577</td>\n",
       "      <td>L577</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text     id conversation_id  \\\n",
       "0                                        They do not!  L1045           L1044   \n",
       "1                                         They do to!  L1044           L1044   \n",
       "2                                          I hope so.   L985            L984   \n",
       "3                                           She okay?   L984            L984   \n",
       "4                                           Let's go.   L925            L924   \n",
       "5                                                 Wow   L924            L924   \n",
       "6      Okay -- you're gonna need to learn how to lie.   L872            L870   \n",
       "7                                                  No   L871            L870   \n",
       "8   I'm kidding.  You know how sometimes you just ...   L870            L870   \n",
       "9                    Like my fear of wearing pastels?   L869            L866   \n",
       "10                                    The \"real you\".   L868            L866   \n",
       "11                                   What good stuff?   L867            L866   \n",
       "12  I figured you'd get to the good stuff eventually.   L866            L866   \n",
       "13  Thank God!  If I had to hear one more story ab...   L865            L862   \n",
       "14  Me.  This endless ...blonde babble. I'm like, ...   L864            L862   \n",
       "15                                         What crap?   L863            L862   \n",
       "16                        do you listen to this crap?   L862            L862   \n",
       "17                                              No...   L861            L860   \n",
       "18  Then Guillermo says, \"If you go any lighter, y...   L860            L860   \n",
       "19                      You always been this selfish?   L699            L696   \n",
       "20                                                But   L698            L696   \n",
       "21                    Then that's all you had to say.   L697            L696   \n",
       "22                                        Well, no...   L696            L696   \n",
       "23      You never wanted to go out with 'me, did you?   L695            L693   \n",
       "24                                             I was?   L694            L693   \n",
       "25  I looked for you back at the party, but you al...   L693            L693   \n",
       "26                                               Tons   L663            L662   \n",
       "27                                  Have fun tonight?   L662            L662   \n",
       "28               I believe we share an art instructor   L578            L577   \n",
       "29                                 You know Chastity?   L577            L577   \n",
       "\n",
       "   reply_to  \n",
       "0     L1044  \n",
       "1      None  \n",
       "2      L984  \n",
       "3      None  \n",
       "4      L924  \n",
       "5      None  \n",
       "6      L871  \n",
       "7      L870  \n",
       "8      None  \n",
       "9      L868  \n",
       "10     L867  \n",
       "11     L866  \n",
       "12     None  \n",
       "13     L864  \n",
       "14     L863  \n",
       "15     L862  \n",
       "16     None  \n",
       "17     L860  \n",
       "18     None  \n",
       "19     L698  \n",
       "20     L697  \n",
       "21     L696  \n",
       "22     None  \n",
       "23     L694  \n",
       "24     L693  \n",
       "25     None  \n",
       "26     L662  \n",
       "27     None  \n",
       "28     L577  \n",
       "29     None  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations = df_loaded_parquet[['text', 'id', 'conversation_id', 'reply_to']]\n",
    "conversations.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4690dc51-4ad3-494b-af82-a0cbf300bc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries where 'id' equals 'conversation_id': 83097\n",
      "Total entries where 'reply_to' is None: 83097\n"
     ]
    }
   ],
   "source": [
    "# Cheking if counts of None are the same with 'id' == 'conversation_id'\n",
    "print(\"Total entries where 'id' equals 'conversation_id':\", (df['id'] == df['conversation_id']).sum())\n",
    "print(\"Total entries where 'reply_to' is None:\", df['reply_to'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38235458-3018-42ef-b554-d4c79acd5caa",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Looks like the dataset is well-structured and prepared for further processing: <br>\n",
    "Conversation_id and id:  <br>\n",
    "When conversation_id and id are the same and there's no reply_to, this indicates the start of a new conversation, this allows to understand where each conversation begins.  <br>\n",
    "Counts of None in reply_to:  <br>\n",
    "The count of None in the reply_to field matches the number of conversations (83,097). This confirms that each conversation starts with a message that does not reply to any previous message, this is the first message in the thread.  <br>\n",
    "Data Cleanliness:  <br>\n",
    "The alignment of these counts and the consistency of data formatting suggest that dataset is clean and structured. Each message within the dataset is correctly linked to its conversation, and the flow of conversations is well-defined.  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50da00ca-77b9-47f7-a5cc-e6ad457c318a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 304713 entries, 0 to 304712\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   text             304713 non-null  object\n",
      " 1   id               304713 non-null  object\n",
      " 2   conversation_id  304713 non-null  object\n",
      " 3   reply_to         221616 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 9.3+ MB\n"
     ]
    }
   ],
   "source": [
    "conversations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee973182-677d-4de8-a117-fc7fc7ac35ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     304713\n",
       "unique    265774\n",
       "top        What?\n",
       "freq        1684\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations.text.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048d07c7-d160-4ef1-bc97-ced5d1bc22a0",
   "metadata": {},
   "source": [
    "## Analyze text of conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "352cb5a8-53bf-4255-b52d-4c211fbb8724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1b9c18b-ae4f-46fa-b148-2d04666182c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\tomui\\AppData\\Local\\Temp\\ipykernel_177364\\2271790802.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conversations.loc[:, 'msg_length'] = conversations.loc[:, 'text'].apply(len)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average message length (characters): 55.25953930419772\n",
      "Average message length (words): 13.721094931952361\n",
      "Min message length (characters): 0\n",
      "Max message length (characters): 3046\n",
      "Standard deviation (characters): 64.06661834805733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomui\\AppData\\Local\\Temp\\ipykernel_177364\\2271790802.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conversations.loc[:, 'word_count'] = conversations.loc[:, 'text'].apply(lambda x: len(word_tokenize(x)))\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist, word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "# Ensure that the punkt tokenizer is available\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Basic statistics\n",
    "conversations.loc[:, 'msg_length'] = conversations.loc[:, 'text'].apply(len)\n",
    "conversations.loc[:, 'word_count'] = conversations.loc[:, 'text'].apply(lambda x: len(word_tokenize(x)))\n",
    "\n",
    "\n",
    "print(\"Average message length (characters):\", np.mean(conversations['msg_length']))\n",
    "print(\"Average message length (words):\", np.mean(conversations['word_count']))\n",
    "print(\"Min message length (characters):\", np.min(conversations['msg_length']))\n",
    "print(\"Max message length (characters):\", np.max(conversations['msg_length']))\n",
    "print(\"Standard deviation (characters):\", np.std(conversations['msg_length']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726bf30c-44d5-41f4-b113-0ab3aabe91e8",
   "metadata": {},
   "source": [
    "### Word frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "524fc0f8-89b8-4098-a5eb-cc145fce7dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words: [('.', 332912), (',', 170188), ('you', 148400), ('i', 140952), ('?', 110240), ('the', 99132), ('to', 80649), ('a', 70839), (\"'s\", 66538), ('it', 66076), (\"n't\", 55224), ('...', 50796), ('do', 47049), ('that', 46582), ('and', 45934), ('of', 39338), ('!', 37866), ('what', 37719), ('in', 34129), ('me', 32203), ('is', 31639), ('we', 29291), ('he', 27408), ('--', 26662), ('this', 24616), ('for', 23415), ('have', 22934), (\"'m\", 22578), (\"'re\", 21717), ('know', 21657), ('was', 21407), ('your', 20962), ('my', 20824), ('not', 19883), ('on', 19560), ('no', 19425), ('be', 19414), ('are', 17600), ('but', 17321), ('with', 17249), ('they', 16942), ('just', 15853), ('all', 15392), ('like', 15007), (\"'ll\", 14613), ('did', 14547), ('there', 14446), ('get', 14152), ('about', 14000), ('so', 13447)]\n"
     ]
    }
   ],
   "source": [
    "all_words = ' '.join(conversations['text']).lower()\n",
    "words = word_tokenize(all_words)\n",
    "freq_dist = FreqDist(words)\n",
    "print(\"Most common words:\", freq_dist.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba737cb-fb87-4e16-bd4d-e3d5df9af995",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5cda7ce-2c7e-478f-bf5b-85ad496f1dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentiment (polarity): 0.04174547982992158\n",
      "Sentiment distribution: count    304713.000000\n",
      "mean          0.041745\n",
      "std           0.246197\n",
      "min          -1.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.013889\n",
      "max           1.000000\n",
      "Name: sentiment, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomui\\AppData\\Local\\Temp\\ipykernel_177364\\4254834136.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conversations.loc[:, 'sentiment'] = conversations.loc[:, 'text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n"
     ]
    }
   ],
   "source": [
    "conversations.loc[:, 'sentiment'] = conversations.loc[:, 'text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "print(\"Average sentiment (polarity):\", np.mean(conversations['sentiment']))\n",
    "print(\"Sentiment distribution:\", conversations['sentiment'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b190a4ec-cffb-471f-917f-3aa8a1eb98c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zero-length messages: 267\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>msg_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>153452</th>\n",
       "      <td></td>\n",
       "      <td>L128985</td>\n",
       "      <td>L128985</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154160</th>\n",
       "      <td></td>\n",
       "      <td>L127954</td>\n",
       "      <td>L127954</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153539</th>\n",
       "      <td></td>\n",
       "      <td>L128742</td>\n",
       "      <td>L128742</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153661</th>\n",
       "      <td></td>\n",
       "      <td>L128579</td>\n",
       "      <td>L128562</td>\n",
       "      <td>L128578</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154007</th>\n",
       "      <td></td>\n",
       "      <td>L128722</td>\n",
       "      <td>L128721</td>\n",
       "      <td>L128721</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153331</th>\n",
       "      <td></td>\n",
       "      <td>L129413</td>\n",
       "      <td>L129409</td>\n",
       "      <td>L129412</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101507</th>\n",
       "      <td></td>\n",
       "      <td>L541062</td>\n",
       "      <td>L541061</td>\n",
       "      <td>L541061</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154244</th>\n",
       "      <td></td>\n",
       "      <td>L129469</td>\n",
       "      <td>L129463</td>\n",
       "      <td>L129468</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213440</th>\n",
       "      <td></td>\n",
       "      <td>L352107</td>\n",
       "      <td>L352107</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154010</th>\n",
       "      <td></td>\n",
       "      <td>L128708</td>\n",
       "      <td>L128708</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text       id conversation_id reply_to  msg_length  word_count  \\\n",
       "153452       L128985         L128985     None           0           0   \n",
       "154160       L127954         L127954     None           0           0   \n",
       "153539       L128742         L128742     None           0           0   \n",
       "153661       L128579         L128562  L128578           0           0   \n",
       "154007       L128722         L128721  L128721           0           0   \n",
       "153331       L129413         L129409  L129412           0           0   \n",
       "101507       L541062         L541061  L541061           0           0   \n",
       "154244       L129469         L129463  L129468           0           0   \n",
       "213440       L352107         L352107     None           0           0   \n",
       "154010       L128708         L128708     None           0           0   \n",
       "\n",
       "        sentiment  \n",
       "153452        0.0  \n",
       "154160        0.0  \n",
       "153539        0.0  \n",
       "153661        0.0  \n",
       "154007        0.0  \n",
       "153331        0.0  \n",
       "101507        0.0  \n",
       "154244        0.0  \n",
       "213440        0.0  \n",
       "154010        0.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating zero characters messages\n",
    "zero_length_messages = conversations[conversations['text'].apply(len) == 0]\n",
    "print(\"Number of zero-length messages:\", len(zero_length_messages))\n",
    "zero_length_messages.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde0265a-8eb2-4d16-b041-c898d500ea15",
   "metadata": {},
   "source": [
    "Same id and conversation_id with None in reply_to - these messages likely represent the start of a conversation. Removing them could impact the structure of the conversation as it might remove the entry point for a conversational thread.\n",
    "Different id and conversation_id with a specific reply_to - these are responses within a conversation. Their removal might disrupt the sequence, making it difficult to follow the flow of the conversation.\n",
    "Messages with a specific reply_to - these indicate replies within the conversation sequence. Removing these could create gaps in the conversation history. I've decided to leave zero text conversations for now, besides thera are only 267 of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6dc250b-588e-45ae-be67-0d49c8a41bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of long messages: 3151\n"
     ]
    }
   ],
   "source": [
    "# Calculating long messages\n",
    "long_messages = conversations[conversations['msg_length'] > 300]\n",
    "print(\"Number of long messages:\", len(long_messages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e2e503e-cb43-44d0-9a91-cb0c67b633b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is what I call my secret place 'cause I come out here when I feel like bein' by myself. I used to come here with Karen Cross. She's kind of like my girlfriend, or used to be. She says she likes Jerry Maroney now. But I'm gonna get her back 'cause I love her. We used to come here and hold hands and talk and read books to each other with a flashlight. She didn't want to have anything to do with me in front of other people 'cause I don't have any money. Well, mama and me, I mean. She seemed to like me a whole lot when we were out here though. She said she loved me, too. Out here. Settin' right on that stump you're on. See, her daddy's a dentist so they're rich. So's Jerry Maroney's daddy. He owns the ice plant. Was your folks well off?\n"
     ]
    }
   ],
   "source": [
    "# Print sample long messages\n",
    "sampled_text = long_messages.sample(1)['text'].iloc[0]\n",
    "print(sampled_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c4217-6c54-47a0-994e-c9ed8bf57e2c",
   "metadata": {},
   "source": [
    "Decided to leave for now long messages - considering to use advanced NLP models such as BERT or GPT (from the transformer family), which are adept at understanding context over longer stretches of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee967afc-2af9-40b2-91c7-1ededc3643a4",
   "metadata": {},
   "source": [
    "## Conversations text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb176ba-116b-4e03-a983-39066c0244d9",
   "metadata": {},
   "source": [
    "### Normalize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6b4d4f4-2c73-404d-8886-aef6c698056a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no, ninety ninety percent of them are full of baloney. they're into the power trip, not the damage. what scares me is that this guy is so sophisticated he could blow up whatever he wants, then disappear. the worst of the bunch, they love the challenge of creating the wildest device ever... and they love the carnage.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Convert to lowercase\n",
    "conversations.loc[:, 'text'] = conversations.loc[:, 'text'].str.lower().str.strip()\n",
    "# Function to apply the regex and normalization transformations row-wise\n",
    "def normalize_text(text):\n",
    "    # Remove non-alphanumeric characters except for basic punctuation\n",
    "    text = re.sub(r\"[^a-z0-9.',!? ]\", ' ', text)\n",
    "    # Replace numbers with a special token\n",
    "    text = re.sub(r'\\d+', '<num>', text)\n",
    "    # Normalize accented characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    return text\n",
    "\n",
    "# Apply the normalization function to each row in the 'text' column\n",
    "conversations.loc[:, 'text'] = conversations.loc[:, 'text'].apply(normalize_text)\n",
    "long_messages = conversations[conversations['msg_length'] > 300]\n",
    "sampled_text = long_messages.sample(1)['text'].iloc[0]\n",
    "print(sampled_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de1fab-d8ae-4c74-bee6-42862a5dcfdf",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6f74020-cf03-48cc-a39a-1711a31e8998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "# conversations.loc[:, 'text'] = conversations.loc[:, 'text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "# long_messages = conversations[conversations['msg_length'] > 300]\n",
    "# sampled_text = long_messages.sample(1)['text'].iloc[0]\n",
    "# print(sampled_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4354401-9e48-4b03-a10d-16a8628661e1",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7c54d05-d740-40f9-9048-905c7ca9d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# conversations.loc[:, 'text'] = conversations['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "# long_messages = conversations[conversations['msg_length'] > 300]\n",
    "# sampled_text = long_messages.sample(1)['text'].iloc[0]\n",
    "# print(sampled_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00370116-00cc-4f88-b20d-b175ab0639de",
   "metadata": {},
   "source": [
    "## Analyze again text of conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9241cf4-d4ba-401b-b31f-9f52a33948af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Basic statistics\n",
    "# conversations.loc[:, 'msg_length'] = conversations['text'].apply(len)\n",
    "# conversations.loc[:, 'word_count'] = conversations['text'].apply(lambda x: len(word_tokenize(x)))\n",
    "\n",
    "\n",
    "# print(\"Average message length (characters):\", np.mean(conversations['msg_length']))\n",
    "# print(\"Average message length (words):\", np.mean(conversations['word_count']))\n",
    "# print(\"Min message length (characters):\", np.min(conversations['msg_length']))\n",
    "# print(\"Max message length (characters):\", np.max(conversations['msg_length']))\n",
    "# print(\"Standard deviation (characters):\", np.std(conversations['msg_length']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6d1f17-991c-44cf-ae52-bfd9a4340f07",
   "metadata": {},
   "source": [
    "### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "132d1c00-a51d-4923-a07e-64d19aa5bfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomui\\AppData\\Local\\Temp\\ipykernel_177364\\1695850789.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conversations.loc[:, 'tokens'] = conversations.loc[:, 'text'].apply(word_tokenize)\n"
     ]
    }
   ],
   "source": [
    "conversations.loc[:, 'tokens'] = conversations.loc[:, 'text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c14ed20-528d-4365-a4ea-a45334f731c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>msg_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>or czechoslovakia.  the slavs have been fighti...</td>\n",
       "      <td>L2897</td>\n",
       "      <td>L2895</td>\n",
       "      <td>L2896</td>\n",
       "      <td>163</td>\n",
       "      <td>29</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>[or, czechoslovakia, ., the, slavs, have, been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>eastern europe.  like what?  romania? hungary?</td>\n",
       "      <td>L2896</td>\n",
       "      <td>L2895</td>\n",
       "      <td>L2895</td>\n",
       "      <td>46</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[eastern, europe, ., like, what, ?, romania, ?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>maybe it's a ritual thing or someone trying to...</td>\n",
       "      <td>L2895</td>\n",
       "      <td>L2895</td>\n",
       "      <td>None</td>\n",
       "      <td>228</td>\n",
       "      <td>48</td>\n",
       "      <td>-0.216667</td>\n",
       "      <td>[maybe, it, 's, a, ritual, thing, or, someone,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>look, i'm not even sure she has anything to do...</td>\n",
       "      <td>L2893</td>\n",
       "      <td>L2892</td>\n",
       "      <td>L2892</td>\n",
       "      <td>192</td>\n",
       "      <td>44</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>[look, ,, i, 'm, not, even, sure, she, has, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>what would you call her?</td>\n",
       "      <td>L2892</td>\n",
       "      <td>L2892</td>\n",
       "      <td>None</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[what, would, you, call, her, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>who says she's a suspect?</td>\n",
       "      <td>L2891</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2890</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[who, says, she, 's, a, suspect, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>maybe you don't care about that either.  prett...</td>\n",
       "      <td>L2890</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2889</td>\n",
       "      <td>78</td>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[maybe, you, do, n't, care, about, that, eithe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>hmmmm.</td>\n",
       "      <td>L2889</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2888</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[hmmmm, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>pretty.</td>\n",
       "      <td>L2888</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2887</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>[pretty, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>the super said he'd seen her before but she di...</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2887</td>\n",
       "      <td>None</td>\n",
       "      <td>61</td>\n",
       "      <td>15</td>\n",
       "      <td>0.234848</td>\n",
       "      <td>[the, super, said, he, 'd, seen, her, before, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     id  \\\n",
       "1200  or czechoslovakia.  the slavs have been fighti...  L2897   \n",
       "1201     eastern europe.  like what?  romania? hungary?  L2896   \n",
       "1202  maybe it's a ritual thing or someone trying to...  L2895   \n",
       "1203  look, i'm not even sure she has anything to do...  L2893   \n",
       "1204                           what would you call her?  L2892   \n",
       "1205                          who says she's a suspect?  L2891   \n",
       "1206  maybe you don't care about that either.  prett...  L2890   \n",
       "1207                                             hmmmm.  L2889   \n",
       "1208                                            pretty.  L2888   \n",
       "1209  the super said he'd seen her before but she di...  L2887   \n",
       "\n",
       "     conversation_id reply_to  msg_length  word_count  sentiment  \\\n",
       "1200           L2895    L2896         163          29   0.130000   \n",
       "1201           L2895    L2895          46          10   0.000000   \n",
       "1202           L2895     None         228          48  -0.216667   \n",
       "1203           L2892    L2892         192          44   0.250000   \n",
       "1204           L2892     None          24           6   0.000000   \n",
       "1205           L2887    L2890          25           7   0.000000   \n",
       "1206           L2887    L2889          78          17   0.000000   \n",
       "1207           L2887    L2888           6           2   0.000000   \n",
       "1208           L2887    L2887           7           2   0.250000   \n",
       "1209           L2887     None          61          15   0.234848   \n",
       "\n",
       "                                                 tokens  \n",
       "1200  [or, czechoslovakia, ., the, slavs, have, been...  \n",
       "1201  [eastern, europe, ., like, what, ?, romania, ?...  \n",
       "1202  [maybe, it, 's, a, ritual, thing, or, someone,...  \n",
       "1203  [look, ,, i, 'm, not, even, sure, she, has, an...  \n",
       "1204                   [what, would, you, call, her, ?]  \n",
       "1205                [who, says, she, 's, a, suspect, ?]  \n",
       "1206  [maybe, you, do, n't, care, about, that, eithe...  \n",
       "1207                                         [hmmmm, .]  \n",
       "1208                                        [pretty, .]  \n",
       "1209  [the, super, said, he, 'd, seen, her, before, ...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations[1200: 1210]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcb8e2e-120c-4573-9238-b5b6ca784862",
   "metadata": {},
   "source": [
    "### Lemmatize tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8de3a380-ee83-4dcf-8bd5-eb62d4bf20d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>msg_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>or czechoslovakia.  the slavs have been fighti...</td>\n",
       "      <td>L2897</td>\n",
       "      <td>L2895</td>\n",
       "      <td>L2896</td>\n",
       "      <td>163</td>\n",
       "      <td>29</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>[or, czechoslovakia, ., the, slav, have, been,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>eastern europe.  like what?  romania? hungary?</td>\n",
       "      <td>L2896</td>\n",
       "      <td>L2895</td>\n",
       "      <td>L2895</td>\n",
       "      <td>46</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[eastern, europe, ., like, what, ?, romania, ?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>maybe it's a ritual thing or someone trying to...</td>\n",
       "      <td>L2895</td>\n",
       "      <td>L2895</td>\n",
       "      <td>None</td>\n",
       "      <td>228</td>\n",
       "      <td>48</td>\n",
       "      <td>-0.216667</td>\n",
       "      <td>[maybe, it, 's, a, ritual, thing, or, someone,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>look, i'm not even sure she has anything to do...</td>\n",
       "      <td>L2893</td>\n",
       "      <td>L2892</td>\n",
       "      <td>L2892</td>\n",
       "      <td>192</td>\n",
       "      <td>44</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>[look, ,, i, 'm, not, even, sure, she, ha, any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>what would you call her?</td>\n",
       "      <td>L2892</td>\n",
       "      <td>L2892</td>\n",
       "      <td>None</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[what, would, you, call, her, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>who says she's a suspect?</td>\n",
       "      <td>L2891</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2890</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[who, say, she, 's, a, suspect, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>maybe you don't care about that either.  prett...</td>\n",
       "      <td>L2890</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2889</td>\n",
       "      <td>78</td>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[maybe, you, do, n't, care, about, that, eithe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>hmmmm.</td>\n",
       "      <td>L2889</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2888</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[hmmmm, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>pretty.</td>\n",
       "      <td>L2888</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2887</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>[pretty, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>the super said he'd seen her before but she di...</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2887</td>\n",
       "      <td>None</td>\n",
       "      <td>61</td>\n",
       "      <td>15</td>\n",
       "      <td>0.234848</td>\n",
       "      <td>[the, super, said, he, 'd, seen, her, before, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     id  \\\n",
       "1200  or czechoslovakia.  the slavs have been fighti...  L2897   \n",
       "1201     eastern europe.  like what?  romania? hungary?  L2896   \n",
       "1202  maybe it's a ritual thing or someone trying to...  L2895   \n",
       "1203  look, i'm not even sure she has anything to do...  L2893   \n",
       "1204                           what would you call her?  L2892   \n",
       "1205                          who says she's a suspect?  L2891   \n",
       "1206  maybe you don't care about that either.  prett...  L2890   \n",
       "1207                                             hmmmm.  L2889   \n",
       "1208                                            pretty.  L2888   \n",
       "1209  the super said he'd seen her before but she di...  L2887   \n",
       "\n",
       "     conversation_id reply_to  msg_length  word_count  sentiment  \\\n",
       "1200           L2895    L2896         163          29   0.130000   \n",
       "1201           L2895    L2895          46          10   0.000000   \n",
       "1202           L2895     None         228          48  -0.216667   \n",
       "1203           L2892    L2892         192          44   0.250000   \n",
       "1204           L2892     None          24           6   0.000000   \n",
       "1205           L2887    L2890          25           7   0.000000   \n",
       "1206           L2887    L2889          78          17   0.000000   \n",
       "1207           L2887    L2888           6           2   0.000000   \n",
       "1208           L2887    L2887           7           2   0.250000   \n",
       "1209           L2887     None          61          15   0.234848   \n",
       "\n",
       "                                                 tokens  \n",
       "1200  [or, czechoslovakia, ., the, slav, have, been,...  \n",
       "1201  [eastern, europe, ., like, what, ?, romania, ?...  \n",
       "1202  [maybe, it, 's, a, ritual, thing, or, someone,...  \n",
       "1203  [look, ,, i, 'm, not, even, sure, she, ha, any...  \n",
       "1204                   [what, would, you, call, her, ?]  \n",
       "1205                 [who, say, she, 's, a, suspect, ?]  \n",
       "1206  [maybe, you, do, n't, care, about, that, eithe...  \n",
       "1207                                         [hmmmm, .]  \n",
       "1208                                        [pretty, .]  \n",
       "1209  [the, super, said, he, 'd, seen, her, before, ...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "conversations.loc[:, 'tokens'] = conversations['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "conversations[1200: 1210]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f562c27d-72f7-4dfd-807e-d200a1dd55bf",
   "metadata": {},
   "source": [
    "## Learning conversation id structure more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee531c94-8903-4532-a413-14a9ea9fb286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>msg_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>you're amazingly self assured. has anyone ever...</td>\n",
       "      <td>L840</td>\n",
       "      <td>L834</td>\n",
       "      <td>L839</td>\n",
       "      <td>61</td>\n",
       "      <td>12</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>[you, 're, amazingly, self, assured, ., ha, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>go to the prom with me</td>\n",
       "      <td>L841</td>\n",
       "      <td>L834</td>\n",
       "      <td>L840</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[go, to, the, prom, with, me]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>is that a request or a command?</td>\n",
       "      <td>L842</td>\n",
       "      <td>L842</td>\n",
       "      <td>None</td>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[is, that, a, request, or, a, command, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>you know what i mean</td>\n",
       "      <td>L843</td>\n",
       "      <td>L842</td>\n",
       "      <td>L842</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.312500</td>\n",
       "      <td>[you, know, what, i, mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>no.</td>\n",
       "      <td>L844</td>\n",
       "      <td>L842</td>\n",
       "      <td>L843</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[no, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>no what?</td>\n",
       "      <td>L845</td>\n",
       "      <td>L842</td>\n",
       "      <td>L844</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[no, what, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>no, i won't go with you</td>\n",
       "      <td>L846</td>\n",
       "      <td>L842</td>\n",
       "      <td>L845</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[no, ,, i, wo, n't, go, with, you]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>why not?</td>\n",
       "      <td>L847</td>\n",
       "      <td>L842</td>\n",
       "      <td>L846</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[why, not, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>because i don't want to. it's a stupid tradition.</td>\n",
       "      <td>L848</td>\n",
       "      <td>L842</td>\n",
       "      <td>L847</td>\n",
       "      <td>49</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>[because, i, do, n't, want, to, ., it, 's, a, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>create a little drama?  start a new rumor?  what?</td>\n",
       "      <td>L852</td>\n",
       "      <td>L852</td>\n",
       "      <td>None</td>\n",
       "      <td>49</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.025568</td>\n",
       "      <td>[create, a, little, drama, ?, start, a, new, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>so i have to have a motive to be with you?</td>\n",
       "      <td>L853</td>\n",
       "      <td>L852</td>\n",
       "      <td>L852</td>\n",
       "      <td>42</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[so, i, have, to, have, a, motive, to, be, wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>you tell me.</td>\n",
       "      <td>L854</td>\n",
       "      <td>L852</td>\n",
       "      <td>L853</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[you, tell, me, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>you need therapy.  has anyone ever told you that?</td>\n",
       "      <td>L855</td>\n",
       "      <td>L852</td>\n",
       "      <td>L854</td>\n",
       "      <td>49</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[you, need, therapy, ., ha, anyone, ever, told...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>answer the question, patrick</td>\n",
       "      <td>L856</td>\n",
       "      <td>L852</td>\n",
       "      <td>L855</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[answer, the, question, ,, patrick]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>nothing!  there's nothing in it for me. just t...</td>\n",
       "      <td>L857</td>\n",
       "      <td>L852</td>\n",
       "      <td>L856</td>\n",
       "      <td>74</td>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[nothing, !, there, 's, nothing, in, it, for, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>then guillermo says,  if you go any lighter, y...</td>\n",
       "      <td>L860</td>\n",
       "      <td>L860</td>\n",
       "      <td>None</td>\n",
       "      <td>87</td>\n",
       "      <td>23</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[then, guillermo, say, ,, if, you, go, any, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>no...</td>\n",
       "      <td>L861</td>\n",
       "      <td>L860</td>\n",
       "      <td>L860</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[no, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>do you listen to this crap?</td>\n",
       "      <td>L862</td>\n",
       "      <td>L862</td>\n",
       "      <td>None</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>[do, you, listen, to, this, crap, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>what crap?</td>\n",
       "      <td>L863</td>\n",
       "      <td>L862</td>\n",
       "      <td>L862</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>[what, crap, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>me.  this endless ...blonde babble. i'm like, ...</td>\n",
       "      <td>L864</td>\n",
       "      <td>L862</td>\n",
       "      <td>L863</td>\n",
       "      <td>60</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>[me, ., this, endless, ..., blonde, babble, .,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>thank god!  if i had to hear one more story ab...</td>\n",
       "      <td>L865</td>\n",
       "      <td>L862</td>\n",
       "      <td>L864</td>\n",
       "      <td>66</td>\n",
       "      <td>15</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[thank, god, !, if, i, had, to, hear, one, mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>i figured you'd get to the good stuff eventually.</td>\n",
       "      <td>L866</td>\n",
       "      <td>L866</td>\n",
       "      <td>None</td>\n",
       "      <td>49</td>\n",
       "      <td>11</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>[i, figured, you, 'd, get, to, the, good, stuf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>what good stuff?</td>\n",
       "      <td>L867</td>\n",
       "      <td>L866</td>\n",
       "      <td>L866</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>[what, good, stuff, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the  real you .</td>\n",
       "      <td>L868</td>\n",
       "      <td>L866</td>\n",
       "      <td>L867</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>[the, real, you, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>like my fear of wearing pastels?</td>\n",
       "      <td>L869</td>\n",
       "      <td>L866</td>\n",
       "      <td>L868</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[like, my, fear, of, wearing, pastel, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i'm kidding.  you know how sometimes you just ...</td>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "      <td>None</td>\n",
       "      <td>101</td>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[i, 'm, kidding, ., you, know, how, sometimes,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text    id conversation_id  \\\n",
       "428  you're amazingly self assured. has anyone ever...  L840            L834   \n",
       "427                             go to the prom with me  L841            L834   \n",
       "426                    is that a request or a command?  L842            L842   \n",
       "425                               you know what i mean  L843            L842   \n",
       "424                                                no.  L844            L842   \n",
       "423                                           no what?  L845            L842   \n",
       "422                            no, i won't go with you  L846            L842   \n",
       "421                                           why not?  L847            L842   \n",
       "420  because i don't want to. it's a stupid tradition.  L848            L842   \n",
       "419  create a little drama?  start a new rumor?  what?  L852            L852   \n",
       "418         so i have to have a motive to be with you?  L853            L852   \n",
       "417                                       you tell me.  L854            L852   \n",
       "416  you need therapy.  has anyone ever told you that?  L855            L852   \n",
       "415                       answer the question, patrick  L856            L852   \n",
       "414  nothing!  there's nothing in it for me. just t...  L857            L852   \n",
       "18   then guillermo says,  if you go any lighter, y...  L860            L860   \n",
       "17                                               no...  L861            L860   \n",
       "16                         do you listen to this crap?  L862            L862   \n",
       "15                                          what crap?  L863            L862   \n",
       "14   me.  this endless ...blonde babble. i'm like, ...  L864            L862   \n",
       "13   thank god!  if i had to hear one more story ab...  L865            L862   \n",
       "12   i figured you'd get to the good stuff eventually.  L866            L866   \n",
       "11                                    what good stuff?  L867            L866   \n",
       "10                                     the  real you .  L868            L866   \n",
       "9                     like my fear of wearing pastels?  L869            L866   \n",
       "8    i'm kidding.  you know how sometimes you just ...  L870            L870   \n",
       "\n",
       "    reply_to  msg_length  word_count  sentiment  \\\n",
       "428     L839          61          12   0.600000   \n",
       "427     L840          22           6   0.000000   \n",
       "426     None          31           8   0.000000   \n",
       "425     L842          20           5  -0.312500   \n",
       "424     L843           3           2   0.000000   \n",
       "423     L844           8           3   0.000000   \n",
       "422     L845          23           8   0.000000   \n",
       "421     L846           8           3   0.000000   \n",
       "420     L847          49          13  -0.800000   \n",
       "419     None          49          12  -0.025568   \n",
       "418     L852          42          12   0.000000   \n",
       "417     L853          12           4   0.000000   \n",
       "416     L854          49          11   0.000000   \n",
       "415     L855          28           5   0.000000   \n",
       "414     L856          74          17   0.000000   \n",
       "18      None          87          23   0.000000   \n",
       "17      L860           5           2   0.000000   \n",
       "16      None          27           7  -0.800000   \n",
       "15      L862          10           3  -0.800000   \n",
       "14      L863          60          15  -0.562500   \n",
       "13      L864          66          15   0.500000   \n",
       "12      None          49          11   0.700000   \n",
       "11      L866          16           4   0.700000   \n",
       "10      L867          15           6   0.200000   \n",
       "9       L868          32           7   0.000000   \n",
       "8       None         101          25   0.000000   \n",
       "\n",
       "                                                tokens  \n",
       "428  [you, 're, amazingly, self, assured, ., ha, an...  \n",
       "427                      [go, to, the, prom, with, me]  \n",
       "426          [is, that, a, request, or, a, command, ?]  \n",
       "425                         [you, know, what, i, mean]  \n",
       "424                                            [no, .]  \n",
       "423                                      [no, what, ?]  \n",
       "422                 [no, ,, i, wo, n't, go, with, you]  \n",
       "421                                      [why, not, ?]  \n",
       "420  [because, i, do, n't, want, to, ., it, 's, a, ...  \n",
       "419  [create, a, little, drama, ?, start, a, new, r...  \n",
       "418  [so, i, have, to, have, a, motive, to, be, wit...  \n",
       "417                                 [you, tell, me, .]  \n",
       "416  [you, need, therapy, ., ha, anyone, ever, told...  \n",
       "415                [answer, the, question, ,, patrick]  \n",
       "414  [nothing, !, there, 's, nothing, in, it, for, ...  \n",
       "18   [then, guillermo, say, ,, if, you, go, any, li...  \n",
       "17                                           [no, ...]  \n",
       "16                [do, you, listen, to, this, crap, ?]  \n",
       "15                                     [what, crap, ?]  \n",
       "14   [me, ., this, endless, ..., blonde, babble, .,...  \n",
       "13   [thank, god, !, if, i, had, to, hear, one, mor...  \n",
       "12   [i, figured, you, 'd, get, to, the, good, stuf...  \n",
       "11                              [what, good, stuff, ?]  \n",
       "10                                 [the, real, you, .]  \n",
       "9             [like, my, fear, of, wearing, pastel, ?]  \n",
       "8    [i, 'm, kidding, ., you, know, how, sometimes,...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sorted_conversations = conversations[conversations['id'].apply(lambda x: 840 <= int(x[1:]) <= 870 if x[1:].isdigit() else False)\n",
    "].sort_values(by='id', key=lambda x: x.str.extract('(\\d+)', expand=False).astype(int))\n",
    "\n",
    "filtered_sorted_conversations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04025ca9-cfba-49e0-9535-8413889bd1dc",
   "metadata": {},
   "source": [
    "### Adding `<start>` and `<end>` tokens to elements of conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "375bdbd4-1760-4a6a-97f3-d692be349adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomui\\AppData\\Local\\Temp\\ipykernel_177364\\581390203.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conversations.loc[:, 'text_with_tokens'] = '<start> ' + conversations.loc[:, 'text'].astype(str) + ' <end>'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>msg_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "      <th>text_with_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>they do not!</td>\n",
       "      <td>L1045</td>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[they, do, not, !]</td>\n",
       "      <td>&lt;start&gt; they do not! &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>they do to!</td>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "      <td>None</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[they, do, to, !]</td>\n",
       "      <td>&lt;start&gt; they do to! &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i hope so.</td>\n",
       "      <td>L985</td>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[i, hope, so, .]</td>\n",
       "      <td>&lt;start&gt; i hope so. &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>she okay?</td>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[she, okay, ?]</td>\n",
       "      <td>&lt;start&gt; she okay? &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>let's go.</td>\n",
       "      <td>L925</td>\n",
       "      <td>L924</td>\n",
       "      <td>L924</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[let, 's, go, .]</td>\n",
       "      <td>&lt;start&gt; let's go. &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wow</td>\n",
       "      <td>L924</td>\n",
       "      <td>L924</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[wow]</td>\n",
       "      <td>&lt;start&gt; wow &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>okay    you're gonna need to learn how to lie.</td>\n",
       "      <td>L872</td>\n",
       "      <td>L870</td>\n",
       "      <td>L871</td>\n",
       "      <td>46</td>\n",
       "      <td>13</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[okay, you, 're, gon, na, need, to, learn, how...</td>\n",
       "      <td>&lt;start&gt; okay    you're gonna need to learn how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>no</td>\n",
       "      <td>L871</td>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[no]</td>\n",
       "      <td>&lt;start&gt; no &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i'm kidding.  you know how sometimes you just ...</td>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "      <td>None</td>\n",
       "      <td>101</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[i, 'm, kidding, ., you, know, how, sometimes,...</td>\n",
       "      <td>&lt;start&gt; i'm kidding.  you know how sometimes y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>like my fear of wearing pastels?</td>\n",
       "      <td>L869</td>\n",
       "      <td>L866</td>\n",
       "      <td>L868</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[like, my, fear, of, wearing, pastel, ?]</td>\n",
       "      <td>&lt;start&gt; like my fear of wearing pastels? &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     id conversation_id  \\\n",
       "0                                       they do not!  L1045           L1044   \n",
       "1                                        they do to!  L1044           L1044   \n",
       "2                                         i hope so.   L985            L984   \n",
       "3                                          she okay?   L984            L984   \n",
       "4                                          let's go.   L925            L924   \n",
       "5                                                wow   L924            L924   \n",
       "6     okay    you're gonna need to learn how to lie.   L872            L870   \n",
       "7                                                 no   L871            L870   \n",
       "8  i'm kidding.  you know how sometimes you just ...   L870            L870   \n",
       "9                   like my fear of wearing pastels?   L869            L866   \n",
       "\n",
       "  reply_to  msg_length  word_count  sentiment  \\\n",
       "0    L1044          12           4        0.0   \n",
       "1     None          11           4        0.0   \n",
       "2     L984          10           4        0.0   \n",
       "3     None           9           3        0.5   \n",
       "4     L924           9           4        0.0   \n",
       "5     None           3           1        0.1   \n",
       "6     L871          46          13        0.5   \n",
       "7     L870           2           1        0.0   \n",
       "8     None         101          25        0.0   \n",
       "9     L868          32           7        0.0   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                                 [they, do, not, !]   \n",
       "1                                  [they, do, to, !]   \n",
       "2                                   [i, hope, so, .]   \n",
       "3                                     [she, okay, ?]   \n",
       "4                                   [let, 's, go, .]   \n",
       "5                                              [wow]   \n",
       "6  [okay, you, 're, gon, na, need, to, learn, how...   \n",
       "7                                               [no]   \n",
       "8  [i, 'm, kidding, ., you, know, how, sometimes,...   \n",
       "9           [like, my, fear, of, wearing, pastel, ?]   \n",
       "\n",
       "                                    text_with_tokens  \n",
       "0                         <start> they do not! <end>  \n",
       "1                          <start> they do to! <end>  \n",
       "2                           <start> i hope so. <end>  \n",
       "3                            <start> she okay? <end>  \n",
       "4                            <start> let's go. <end>  \n",
       "5                                  <start> wow <end>  \n",
       "6  <start> okay    you're gonna need to learn how...  \n",
       "7                                   <start> no <end>  \n",
       "8  <start> i'm kidding.  you know how sometimes y...  \n",
       "9     <start> like my fear of wearing pastels? <end>  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations.loc[:, 'text_with_tokens'] = '<start> ' + conversations.loc[:, 'text'].astype(str) + ' <end>'\n",
    "conversations.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe97c54-62ce-44e6-8dc7-7e69d3ff49c9",
   "metadata": {},
   "source": [
    "id: Unique identifier for each message. <br>\n",
    "conversation_id: Identifier for the conversation to which the message belongs. All messages within the same conversation share this ID. <br>\n",
    "reply_to: ID of the message to which the current message is a response. If this is None, the message is the start of a conversation thread."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6bbc39-fb71-4d10-a010-7a81cce4a869",
   "metadata": {},
   "source": [
    "## Pairing messages - input with responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53453bd2-3a67-434d-85dc-864e58d6bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the DataFrame with itself to form pairs\n",
    "pairs = pd.merge(\n",
    "    conversations, conversations,\n",
    "    left_on='id',\n",
    "    right_on='reply_to',\n",
    "    suffixes=('_input', '_response')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07424e34-23a2-4937-88e2-e6e6be66bdf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_input</th>\n",
       "      <th>id_input</th>\n",
       "      <th>conversation_id_input</th>\n",
       "      <th>reply_to_input</th>\n",
       "      <th>msg_length_input</th>\n",
       "      <th>word_count_input</th>\n",
       "      <th>sentiment_input</th>\n",
       "      <th>tokens_input</th>\n",
       "      <th>text_with_tokens_input</th>\n",
       "      <th>text_response</th>\n",
       "      <th>id_response</th>\n",
       "      <th>conversation_id_response</th>\n",
       "      <th>reply_to_response</th>\n",
       "      <th>msg_length_response</th>\n",
       "      <th>word_count_response</th>\n",
       "      <th>sentiment_response</th>\n",
       "      <th>tokens_response</th>\n",
       "      <th>text_with_tokens_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>they do to!</td>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "      <td>None</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[they, do, to, !]</td>\n",
       "      <td>&lt;start&gt; they do to! &lt;end&gt;</td>\n",
       "      <td>they do not!</td>\n",
       "      <td>L1045</td>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[they, do, not, !]</td>\n",
       "      <td>&lt;start&gt; they do not! &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>she okay?</td>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[she, okay, ?]</td>\n",
       "      <td>&lt;start&gt; she okay? &lt;end&gt;</td>\n",
       "      <td>i hope so.</td>\n",
       "      <td>L985</td>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[i, hope, so, .]</td>\n",
       "      <td>&lt;start&gt; i hope so. &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wow</td>\n",
       "      <td>L924</td>\n",
       "      <td>L924</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[wow]</td>\n",
       "      <td>&lt;start&gt; wow &lt;end&gt;</td>\n",
       "      <td>let's go.</td>\n",
       "      <td>L925</td>\n",
       "      <td>L924</td>\n",
       "      <td>L924</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[let, 's, go, .]</td>\n",
       "      <td>&lt;start&gt; let's go. &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no</td>\n",
       "      <td>L871</td>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[no]</td>\n",
       "      <td>&lt;start&gt; no &lt;end&gt;</td>\n",
       "      <td>okay    you're gonna need to learn how to lie.</td>\n",
       "      <td>L872</td>\n",
       "      <td>L870</td>\n",
       "      <td>L871</td>\n",
       "      <td>46</td>\n",
       "      <td>13</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[okay, you, 're, gon, na, need, to, learn, how...</td>\n",
       "      <td>&lt;start&gt; okay    you're gonna need to learn how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i'm kidding.  you know how sometimes you just ...</td>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "      <td>None</td>\n",
       "      <td>101</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[i, 'm, kidding, ., you, know, how, sometimes,...</td>\n",
       "      <td>&lt;start&gt; i'm kidding.  you know how sometimes y...</td>\n",
       "      <td>no</td>\n",
       "      <td>L871</td>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[no]</td>\n",
       "      <td>&lt;start&gt; no &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_input id_input  \\\n",
       "0                                        they do to!    L1044   \n",
       "1                                          she okay?     L984   \n",
       "2                                                wow     L924   \n",
       "3                                                 no     L871   \n",
       "4  i'm kidding.  you know how sometimes you just ...     L870   \n",
       "\n",
       "  conversation_id_input reply_to_input  msg_length_input  word_count_input  \\\n",
       "0                 L1044           None                11                 4   \n",
       "1                  L984           None                 9                 3   \n",
       "2                  L924           None                 3                 1   \n",
       "3                  L870           L870                 2                 1   \n",
       "4                  L870           None               101                25   \n",
       "\n",
       "   sentiment_input                                       tokens_input  \\\n",
       "0              0.0                                  [they, do, to, !]   \n",
       "1              0.5                                     [she, okay, ?]   \n",
       "2              0.1                                              [wow]   \n",
       "3              0.0                                               [no]   \n",
       "4              0.0  [i, 'm, kidding, ., you, know, how, sometimes,...   \n",
       "\n",
       "                              text_with_tokens_input  \\\n",
       "0                          <start> they do to! <end>   \n",
       "1                            <start> she okay? <end>   \n",
       "2                                  <start> wow <end>   \n",
       "3                                   <start> no <end>   \n",
       "4  <start> i'm kidding.  you know how sometimes y...   \n",
       "\n",
       "                                    text_response id_response  \\\n",
       "0                                    they do not!       L1045   \n",
       "1                                      i hope so.        L985   \n",
       "2                                       let's go.        L925   \n",
       "3  okay    you're gonna need to learn how to lie.        L872   \n",
       "4                                              no        L871   \n",
       "\n",
       "  conversation_id_response reply_to_response  msg_length_response  \\\n",
       "0                    L1044             L1044                   12   \n",
       "1                     L984              L984                   10   \n",
       "2                     L924              L924                    9   \n",
       "3                     L870              L871                   46   \n",
       "4                     L870              L870                    2   \n",
       "\n",
       "   word_count_response  sentiment_response  \\\n",
       "0                    4                 0.0   \n",
       "1                    4                 0.0   \n",
       "2                    4                 0.0   \n",
       "3                   13                 0.5   \n",
       "4                    1                 0.0   \n",
       "\n",
       "                                     tokens_response  \\\n",
       "0                                 [they, do, not, !]   \n",
       "1                                   [i, hope, so, .]   \n",
       "2                                   [let, 's, go, .]   \n",
       "3  [okay, you, 're, gon, na, need, to, learn, how...   \n",
       "4                                               [no]   \n",
       "\n",
       "                           text_with_tokens_response  \n",
       "0                         <start> they do not! <end>  \n",
       "1                           <start> i hope so. <end>  \n",
       "2                            <start> let's go. <end>  \n",
       "3  <start> okay    you're gonna need to learn how...  \n",
       "4                                   <start> no <end>  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b68aa316-af58-4439-9895-098291abe5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the needed columns including IDs\n",
    "training_data = pairs[['id_input', 'text_with_tokens_input', 'tokens_input', 'sentiment_input', 'id_response', 'text_with_tokens_response', 'tokens_response', 'sentiment_response']]\n",
    "\n",
    "# Renaming columns for clarity\n",
    "training_data.columns = ['ID_Input', 'Input', 'Tokens_Input', 'Sentiment_Input', 'ID_Response', 'Response', 'Tokens_Response', 'Sentiment_Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4061571f-9fc5-4bd6-a04a-c7f27f891203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Input</th>\n",
       "      <th>Input</th>\n",
       "      <th>Tokens_Input</th>\n",
       "      <th>Sentiment_Input</th>\n",
       "      <th>ID_Response</th>\n",
       "      <th>Response</th>\n",
       "      <th>Tokens_Response</th>\n",
       "      <th>Sentiment_Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1044</td>\n",
       "      <td>&lt;start&gt; they do to! &lt;end&gt;</td>\n",
       "      <td>[they, do, to, !]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>L1045</td>\n",
       "      <td>&lt;start&gt; they do not! &lt;end&gt;</td>\n",
       "      <td>[they, do, not, !]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L984</td>\n",
       "      <td>&lt;start&gt; she okay? &lt;end&gt;</td>\n",
       "      <td>[she, okay, ?]</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>L985</td>\n",
       "      <td>&lt;start&gt; i hope so. &lt;end&gt;</td>\n",
       "      <td>[i, hope, so, .]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L924</td>\n",
       "      <td>&lt;start&gt; wow &lt;end&gt;</td>\n",
       "      <td>[wow]</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>L925</td>\n",
       "      <td>&lt;start&gt; let's go. &lt;end&gt;</td>\n",
       "      <td>[let, 's, go, .]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L871</td>\n",
       "      <td>&lt;start&gt; no &lt;end&gt;</td>\n",
       "      <td>[no]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>L872</td>\n",
       "      <td>&lt;start&gt; okay    you're gonna need to learn how...</td>\n",
       "      <td>[okay, you, 're, gon, na, need, to, learn, how...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L870</td>\n",
       "      <td>&lt;start&gt; i'm kidding.  you know how sometimes y...</td>\n",
       "      <td>[i, 'm, kidding, ., you, know, how, sometimes,...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>L871</td>\n",
       "      <td>&lt;start&gt; no &lt;end&gt;</td>\n",
       "      <td>[no]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221611</th>\n",
       "      <td>L666520</td>\n",
       "      <td>&lt;start&gt; well i assure you, sir, i have no desi...</td>\n",
       "      <td>[well, i, assure, you, ,, sir, ,, i, have, no,...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>L666521</td>\n",
       "      <td>&lt;start&gt; and i assure you, you do not in fact i...</td>\n",
       "      <td>[and, i, assure, you, ,, you, do, not, in, fac...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221612</th>\n",
       "      <td>L666371</td>\n",
       "      <td>&lt;start&gt; lord chelmsford seems to want me to st...</td>\n",
       "      <td>[lord, chelmsford, seems, to, want, me, to, st...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>L666372</td>\n",
       "      <td>&lt;start&gt; i think chelmsford wants a good man on...</td>\n",
       "      <td>[i, think, chelmsford, want, a, good, man, on,...</td>\n",
       "      <td>0.355556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221613</th>\n",
       "      <td>L666370</td>\n",
       "      <td>&lt;start&gt; i'm to take the sikali with the main c...</td>\n",
       "      <td>[i, 'm, to, take, the, sikali, with, the, main...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>L666371</td>\n",
       "      <td>&lt;start&gt; lord chelmsford seems to want me to st...</td>\n",
       "      <td>[lord, chelmsford, seems, to, want, me, to, st...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221614</th>\n",
       "      <td>L666369</td>\n",
       "      <td>&lt;start&gt; your orders, mr vereker? &lt;end&gt;</td>\n",
       "      <td>[your, order, ,, mr, vereker, ?]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>L666370</td>\n",
       "      <td>&lt;start&gt; i'm to take the sikali with the main c...</td>\n",
       "      <td>[i, 'm, to, take, the, sikali, with, the, main...</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221615</th>\n",
       "      <td>L666256</td>\n",
       "      <td>&lt;start&gt; colonel durnford... william vereker. i...</td>\n",
       "      <td>[colonel, durnford, ..., william, vereker, ., ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>L666257</td>\n",
       "      <td>&lt;start&gt; good ones, yes, mr vereker. gentlemen ...</td>\n",
       "      <td>[good, one, ,, yes, ,, mr, vereker, ., gentlem...</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>221616 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID_Input                                              Input  \\\n",
       "0         L1044                          <start> they do to! <end>   \n",
       "1          L984                            <start> she okay? <end>   \n",
       "2          L924                                  <start> wow <end>   \n",
       "3          L871                                   <start> no <end>   \n",
       "4          L870  <start> i'm kidding.  you know how sometimes y...   \n",
       "...         ...                                                ...   \n",
       "221611  L666520  <start> well i assure you, sir, i have no desi...   \n",
       "221612  L666371  <start> lord chelmsford seems to want me to st...   \n",
       "221613  L666370  <start> i'm to take the sikali with the main c...   \n",
       "221614  L666369             <start> your orders, mr vereker? <end>   \n",
       "221615  L666256  <start> colonel durnford... william vereker. i...   \n",
       "\n",
       "                                             Tokens_Input  Sentiment_Input  \\\n",
       "0                                       [they, do, to, !]         0.000000   \n",
       "1                                          [she, okay, ?]         0.500000   \n",
       "2                                                   [wow]         0.100000   \n",
       "3                                                    [no]         0.000000   \n",
       "4       [i, 'm, kidding, ., you, know, how, sometimes,...         0.000000   \n",
       "...                                                   ...              ...   \n",
       "221611  [well, i, assure, you, ,, sir, ,, i, have, no,...         0.000000   \n",
       "221612  [lord, chelmsford, seems, to, want, me, to, st...         0.000000   \n",
       "221613  [i, 'm, to, take, the, sikali, with, the, main...         0.166667   \n",
       "221614                   [your, order, ,, mr, vereker, ?]         0.000000   \n",
       "221615  [colonel, durnford, ..., william, vereker, ., ...         0.000000   \n",
       "\n",
       "       ID_Response                                           Response  \\\n",
       "0            L1045                         <start> they do not! <end>   \n",
       "1             L985                           <start> i hope so. <end>   \n",
       "2             L925                            <start> let's go. <end>   \n",
       "3             L872  <start> okay    you're gonna need to learn how...   \n",
       "4             L871                                   <start> no <end>   \n",
       "...            ...                                                ...   \n",
       "221611     L666521  <start> and i assure you, you do not in fact i...   \n",
       "221612     L666372  <start> i think chelmsford wants a good man on...   \n",
       "221613     L666371  <start> lord chelmsford seems to want me to st...   \n",
       "221614     L666370  <start> i'm to take the sikali with the main c...   \n",
       "221615     L666257  <start> good ones, yes, mr vereker. gentlemen ...   \n",
       "\n",
       "                                          Tokens_Response  Sentiment_Response  \n",
       "0                                      [they, do, not, !]            0.000000  \n",
       "1                                        [i, hope, so, .]            0.000000  \n",
       "2                                        [let, 's, go, .]            0.000000  \n",
       "3       [okay, you, 're, gon, na, need, to, learn, how...            0.500000  \n",
       "4                                                    [no]            0.000000  \n",
       "...                                                   ...                 ...  \n",
       "221611  [and, i, assure, you, ,, you, do, not, in, fac...            1.000000  \n",
       "221612  [i, think, chelmsford, want, a, good, man, on,...            0.355556  \n",
       "221613  [lord, chelmsford, seems, to, want, me, to, st...            0.000000  \n",
       "221614  [i, 'm, to, take, the, sikali, with, the, main...            0.166667  \n",
       "221615  [good, one, ,, yes, ,, mr, vereker, ., gentlem...            0.700000  \n",
       "\n",
       "[221616 rows x 8 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c01ce521-0bac-4503-88c6-eb710c853130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221616"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking how many pairs I shall get\n",
    "len(conversations) - len(conversations.loc[:, 'conversation_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9e86de81-01d7-462a-8672-5aaeceb37adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 221616 entries, 0 to 221615\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   ID_Input            221616 non-null  object \n",
      " 1   Input               221616 non-null  object \n",
      " 2   Tokens_Input        221616 non-null  object \n",
      " 3   Sentiment_Input     221616 non-null  float64\n",
      " 4   ID_Response         221616 non-null  object \n",
      " 5   Response            221616 non-null  object \n",
      " 6   Tokens_Response     221616 non-null  object \n",
      " 7   Sentiment_Response  221616 non-null  float64\n",
      "dtypes: float64(2), object(6)\n",
      "memory usage: 13.5+ MB\n"
     ]
    }
   ],
   "source": [
    "training_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe78df8c-be4e-4aac-bd99-5c5bfecc74b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment_Input</th>\n",
       "      <th>Sentiment_Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>221616.00000</td>\n",
       "      <td>221616.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.03737</td>\n",
       "      <td>0.042546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.24235</td>\n",
       "      <td>0.245880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.00000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentiment_Input  Sentiment_Response\n",
       "count     221616.00000       221616.000000\n",
       "mean           0.03737            0.042546\n",
       "std            0.24235            0.245880\n",
       "min           -1.00000           -1.000000\n",
       "25%            0.00000            0.000000\n",
       "50%            0.00000            0.000000\n",
       "75%            0.00000            0.025000\n",
       "max            1.00000            1.000000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dafe21-5406-4d92-b940-01b55f93f13b",
   "metadata": {},
   "source": [
    "### Checking token length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "16f9f405-d36f-4dfe-8c68-ce75dde0da0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Token Lengths - Statistics:\n",
      "count    221616.000000\n",
      "mean         13.191800\n",
      "std          13.791245\n",
      "min           0.000000\n",
      "25%           5.000000\n",
      "50%           9.000000\n",
      "75%          16.000000\n",
      "max         369.000000\n",
      "Name: Tokens_Input, dtype: float64\n",
      "\n",
      "Response Token Lengths - Statistics:\n",
      "count    221616.000000\n",
      "mean         13.674523\n",
      "std          14.757182\n",
      "min           0.000000\n",
      "25%           5.000000\n",
      "50%           9.000000\n",
      "75%          17.000000\n",
      "max         673.000000\n",
      "Name: Tokens_Response, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'data' is your DataFrame\n",
    "token_lengths_input = training_data['Tokens_Input'].apply(len)\n",
    "token_lengths_response = training_data['Tokens_Response'].apply(len)\n",
    "\n",
    "print(\"Input Token Lengths - Statistics:\")\n",
    "print(token_lengths_input.describe())\n",
    "\n",
    "print(\"\\nResponse Token Lengths - Statistics:\")\n",
    "print(token_lengths_response.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa1d61c-2d1e-43b2-be8d-2885e3682584",
   "metadata": {},
   "source": [
    "## Saving the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "04767995-fd58-4626-8724-737073710d6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path_parquet = os.path.join(data_dir, 'training_data.parquet')\n",
    "training_data.to_parquet(file_path_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3285d6d-535c-4f34-b96e-d0fa5f5117d6",
   "metadata": {},
   "source": [
    "## Loading the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49f30e3a-7fe0-469b-9ebf-eacf555a4cd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Input</th>\n",
       "      <th>Input</th>\n",
       "      <th>Tokens_Input</th>\n",
       "      <th>Sentiment_Input</th>\n",
       "      <th>ID_Response</th>\n",
       "      <th>Response</th>\n",
       "      <th>Tokens_Response</th>\n",
       "      <th>Sentiment_Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1044</td>\n",
       "      <td>&lt;start&gt; they do to! &lt;end&gt;</td>\n",
       "      <td>[they, do, to, !]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L1045</td>\n",
       "      <td>&lt;start&gt; they do not! &lt;end&gt;</td>\n",
       "      <td>[they, do, not, !]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L984</td>\n",
       "      <td>&lt;start&gt; she okay? &lt;end&gt;</td>\n",
       "      <td>[she, okay, ?]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>L985</td>\n",
       "      <td>&lt;start&gt; i hope so. &lt;end&gt;</td>\n",
       "      <td>[i, hope, so, .]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L924</td>\n",
       "      <td>&lt;start&gt; wow &lt;end&gt;</td>\n",
       "      <td>[wow]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>L925</td>\n",
       "      <td>&lt;start&gt; let's go. &lt;end&gt;</td>\n",
       "      <td>[let, 's, go, .]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L871</td>\n",
       "      <td>&lt;start&gt; no &lt;end&gt;</td>\n",
       "      <td>[no]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L872</td>\n",
       "      <td>&lt;start&gt; okay    you're gonna need to learn how...</td>\n",
       "      <td>[okay, you, 're, gon, na, need, to, learn, how...</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L870</td>\n",
       "      <td>&lt;start&gt; i'm kidding.  you know how sometimes y...</td>\n",
       "      <td>[i, 'm, kidding, ., you, know, how, sometimes,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L871</td>\n",
       "      <td>&lt;start&gt; no &lt;end&gt;</td>\n",
       "      <td>[no]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_Input                                              Input  \\\n",
       "0    L1044                          <start> they do to! <end>   \n",
       "1     L984                            <start> she okay? <end>   \n",
       "2     L924                                  <start> wow <end>   \n",
       "3     L871                                   <start> no <end>   \n",
       "4     L870  <start> i'm kidding.  you know how sometimes y...   \n",
       "\n",
       "                                        Tokens_Input  Sentiment_Input  \\\n",
       "0                                  [they, do, to, !]              0.0   \n",
       "1                                     [she, okay, ?]              0.5   \n",
       "2                                              [wow]              0.1   \n",
       "3                                               [no]              0.0   \n",
       "4  [i, 'm, kidding, ., you, know, how, sometimes,...              0.0   \n",
       "\n",
       "  ID_Response                                           Response  \\\n",
       "0       L1045                         <start> they do not! <end>   \n",
       "1        L985                           <start> i hope so. <end>   \n",
       "2        L925                            <start> let's go. <end>   \n",
       "3        L872  <start> okay    you're gonna need to learn how...   \n",
       "4        L871                                   <start> no <end>   \n",
       "\n",
       "                                     Tokens_Response  Sentiment_Response  \n",
       "0                                 [they, do, not, !]                 0.0  \n",
       "1                                   [i, hope, so, .]                 0.0  \n",
       "2                                   [let, 's, go, .]                 0.0  \n",
       "3  [okay, you, 're, gon, na, need, to, learn, how...                 0.5  \n",
       "4                                               [no]                 0.0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_parquet = os.path.join(data_dir, 'training_data.parquet')\n",
    "data = pd.read_parquet(file_path_parquet)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a529c889-d24a-466b-aba3-a74fabc4142f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Device: NVIDIA GeForce GTX 1660 Ti with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fffc2b-fd10-45c0-b7d7-1e0772b308e2",
   "metadata": {},
   "source": [
    "## Create vocabulary and dataset for chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ff9c9d16-0cc3-4667-9210-2403e960c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2index = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2}\n",
    "        self.index2word = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\"}\n",
    "        self.num_words = 3\n",
    "\n",
    "    def add_sentence(self, sentence: str) -> None:\n",
    "        for word in sentence.split():\n",
    "            if word not in self.word2index:\n",
    "                self.word2index[word] = self.num_words\n",
    "                self.index2word[self.num_words] = word\n",
    "                self.num_words += 1\n",
    "\n",
    "    def sentence_to_indices(self, sentence: str) -> list:\n",
    "        return [self.word2index[word] for word in sentence.split() if word in self.word2index]\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, data, vocab):\n",
    "        self.inputs = [torch.tensor(vocab.sentence_to_indices(sentence)) for sentence in data['Input']]\n",
    "        self.responses = [torch.tensor(vocab.sentence_to_indices(sentence)) for sentence in data['Response']]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.responses[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    # Pad the sequences with 0 (the index for <pad>)\n",
    "    padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    padded_targets = pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    return padded_inputs, padded_targets\n",
    "\n",
    "vocab = Vocabulary()\n",
    "for sentence in data['Input'] + data['Response']:\n",
    "    vocab.add_sentence(sentence)\n",
    "\n",
    "dataset = ChatDataset(data, vocab)\n",
    "indices = list(range(len(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a18de8-2865-4bc8-9fdc-6d8939ada83a",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01e49fb2-89a0-4a75-8189-c9be6fa5f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, test_indices = train_test_split(indices, test_size=0.15, random_state=22)\n",
    "train_indices, val_indices = train_test_split(train_indices, test_size=0.176, random_state=22)  # Adjusting to maintain 70-15-15 split\n",
    "\n",
    "train_loader = DataLoader([dataset[i] for i in train_indices], batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader([dataset[i] for i in val_indices], batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader([dataset[i] for i in test_indices], batch_size=2, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f963c77-d2dc-4e06-b89b-752d460100b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e712a9-0bae-4958-b9b3-b9f0394be4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)  # [batch_size, seq_len, hidden_size]\n",
    "        output, hidden = self.gru(embedded, hidden)  # hidden [1, batch_size, hidden_size]\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)  # [1, batch_size, hidden_size]\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)  # Embed the input, expecting [batch_size, seq_len] where seq_len=1\n",
    "        output, hidden = self.gru(embedded, hidden)  # Process the GRU step\n",
    "        output = self.softmax(self.out(output.squeeze(1)))  # Adjust softmax layer\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "# Define the size of the hidden layer\n",
    "hidden_size = 256\n",
    "encoder = Encoder(vocab.num_words, hidden_size)\n",
    "decoder = Decoder(hidden_size, vocab.num_words)\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    batch_size = input_tensor.size(0)\n",
    "    encoder_hidden = encoder.init_hidden(batch_size)\n",
    "\n",
    "    # Encoder forward pass\n",
    "    encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "\n",
    "    # Initializing decoder input properly as [batch_size, 1]\n",
    "    decoder_input = torch.tensor([[vocab.word2index[\"<start>\"]] for _ in range(batch_size)], dtype=torch.long, device=input_tensor.device)\n",
    "    decoder_hidden = encoder_hidden  # Direct transfer of hidden state from encoder to decoder\n",
    "\n",
    "    loss = 0\n",
    "    for di in range(target_tensor.size(1)):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        # Correctly reshaping decoder_input to maintain [batch_size, 1]\n",
    "        decoder_input = topi.squeeze().detach().unsqueeze(1)  # Ensure this remains [batch_size, 1]\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[:, di])\n",
    "\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_tensor.size(1)\n",
    "\n",
    "encoder_optimizer = Adam(encoder.parameters())\n",
    "decoder_optimizer = Adam(decoder.parameters())\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Example training loop\n",
    "for epoch in range(1, 11):  # Training for 10 epochs for demonstration\n",
    "    total_loss = 0\n",
    "    for input_tensor, target_tensor in train_loader:\n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        total_loss += loss\n",
    "    print(f'Epoch {epoch}, Loss: {total_loss / len(train_loader)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
