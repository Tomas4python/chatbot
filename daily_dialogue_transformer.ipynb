{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90199c4d-e447-46b3-9c7a-e633a3c02f09",
   "metadata": {},
   "source": [
    "# TRANSFORMER SIMPLE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14260f0-4962-42e2-8527-f5189a158c76",
   "metadata": {
    "id": "b0ZiOK0MWE7d"
   },
   "source": [
    "## GPU info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63f70056-2151-4051-8974-714b3e237e43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1715927490489,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "bhvFI7ztVLRy",
    "outputId": "e4fb02b2-7ebf-4f3e-b2a8-f9f189a25c47"
   },
   "outputs": [],
   "source": [
    "# gpu_info = !nvidia-smi\n",
    "# gpu_info = '\\n'.join(gpu_info)\n",
    "# if gpu_info.find('failed') >= 0:\n",
    "#   print('Not connected to a GPU')\n",
    "# else:\n",
    "#   print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a58ec6-07dc-4261-ae09-67ab59ed3fb6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23496,
     "status": "ok",
     "timestamp": 1715927517410,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "RMth1TfbAys6",
    "outputId": "766cb4f5-edfa-4cd7-ad09-825604e4c786"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d5d74b-5e87-4198-b112-9f91b7f65d0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4075,
     "status": "ok",
     "timestamp": 1715927650106,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "db509470-cb51-4c21-84ca-5f928073d68e",
    "outputId": "95b90920-4333-45db-f52c-0f88f1ec6a5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Name: /device:GPU:0\n",
      "Memory Limit: 4158652416 bytes\n",
      "Description: device: 0, name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_gpu_details():\n",
    "    devices = device_lib.list_local_devices()\n",
    "    for device in devices:\n",
    "        if device.device_type == 'GPU':\n",
    "            print(f\"Device Name: {device.name}\")\n",
    "            print(f\"Memory Limit: {device.memory_limit} bytes\")\n",
    "            print(f\"Description: {device.physical_device_desc}\")\n",
    "\n",
    "get_gpu_details()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0510248e-c233-485a-b707-7a7eb9856939",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35a59de0-817a-4089-9a14-0b45925b6298",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 15 # Length of input and target sequences, padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c8162-54ab-45da-b4f0-2d5472decdd8",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e231807-af60-4571-be49-655d80d8eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "# from tensorflow.keras.layers import Input, Embedding, MultiHeadAttention, Dense, LayerNormalization, Dropout, GlobalAveragePooling1D, Layer, Masking\n",
    "# from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Masking, MultiHeadAttention\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b3db57-2ac6-4c1d-b610-3de8e89d3352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "TensorFlow Keras version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"TensorFlow Keras version:\", tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e56f5d37-9a67-4e51-b0b8-404df9d734a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1715930801381,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "y5srV6bjOpFb",
    "outputId": "b74db1e8-1afe-4348-be1a-23c150492567"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')  # Tokenizer\n",
    "nltk.download('wordnet')  # Lemmatizer\n",
    "nltk.download('stopwords')  # Stopwords\n",
    "nltk.download('omw-1.4') # Ensures multilingual contexts\n",
    "\n",
    "# Stopwords list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "initial_preprocessing = True\n",
    "\n",
    "# Load spaCy's English NLP model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b393a656-df75-4deb-af28-d8ba36f526eb",
   "metadata": {
    "id": "f5150d2f-999d-45ad-b2f2-b9b65b883e89"
   },
   "source": [
    "## Create prepocessing functions for initial text and later response generation preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75edfc9e-3560-4ab7-9992-963b8df9f537",
   "metadata": {
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1715930818326,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "f0e8f237-5274-43ee-8ac2-964dd9214ae5"
   },
   "outputs": [],
   "source": [
    "contractions = {\n",
    "    \"’\": \"'\",\n",
    "    \"‘\": \"'\",\n",
    "    \"“\": '\"',\n",
    "    \"”\": '\"',\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"n't\": \" not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"thats's\": \"that is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"that'll\": \"that will\",\n",
    "    \"there'll\": \"there will\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"ain't\": \"is not\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"daren't\": \"dare not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"usedn't\": \"used not\"\n",
    "}\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    # Normalize Unicode string to NFKD form, remove non-ASCII characters, and then decode it back to a UTF-8 string\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove spaces around apostrophes\n",
    "    text = re.sub(r\"\\s*'\\s*\", \"'\", text)\n",
    "    # Add a space before and after any punctuation mark (., !, or ?)\n",
    "    text = re.sub(r\"\\s*([.!?])\\s*\", r\" \\1 \", text)\n",
    "    # Correct contractions\n",
    "    for contraction, replacement in contractions.items():\n",
    "        text = re.sub(re.escape(contraction), replacement, text)\n",
    "    # Replace any sequence of characters that are not letters, basic punctuation\n",
    "    text = re.sub(r\"[^a-z' ]\", ' ', text) # re.sub(r\"[^a-z.,'!? ]\", ' ', text)\n",
    "    # Replace any sequence of whitespace characters with a single space and remove leading and trailing whitespace\n",
    "    text = re.sub(r\"\\s+\", ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_names(text: str) -> str:\n",
    "    # Use spaCy to detect and remove names from the text\n",
    "    doc = nlp(text)\n",
    "    filtered_text = ' '.join([token.text for token in doc if token.ent_type_ != 'PERSON']) # Takes really long time, exlude from chatbot input preprocessing\n",
    "    return filtered_text\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # Normalize text\n",
    "    text = normalize_text(text)\n",
    "    # Remove names using spaCy's NER\n",
    "    if initial_preprocessing:\n",
    "        text = remove_names(text)\n",
    "    # # Remove punctuation\n",
    "    # text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stopwords and tokenize\n",
    "    # words = word_tokenize(text) # More intelligent splitting\n",
    "    # filtered_words = [word for word in words if word not in stop_words]\n",
    "    # # Lemmatize words\n",
    "    # lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    # Add <SOS> and <EOS> tokens, and join the list into a single string\n",
    "    # return ' '.join(['sofs'] + lemmatized_words + ['eofs'])\n",
    "        # Trim the text to the desired length\n",
    "    words = text.split()[:max_length]\n",
    "    trimmed_text = ' '.join(words)  # Consider to remove trimming, if you want pad later on max length\n",
    "    return trimmed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5deb6d2-695b-4025-b6b1-72428d051053",
   "metadata": {},
   "source": [
    "### Load the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5428c87-4ce6-480a-bb32-48f7a6063e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from file\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "tokenizer_path = os.path.join(data_dir, 'tokenizer_dd_tf210.pickle')\n",
    "with open(tokenizer_path, 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e0ec68-2723-436d-a101-ad18df9f0a63",
   "metadata": {},
   "source": [
    "### Loading the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8036e60-f3a3-4beb-9f96-da6496797413",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>response</th>\n",
       "      <th>encoder_input_data</th>\n",
       "      <th>decoder_input_data</th>\n",
       "      <th>decoder_output_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>say how about going for a few beers after dinner</td>\n",
       "      <td>you know that is tempting but is really not go...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 138, 33, 37, 75, 20, 8, 206, 3...</td>\n",
       "      <td>[0, 0, 1, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 4...</td>\n",
       "      <td>[0, 0, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 47, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you know that is tempting but is really not go...</td>\n",
       "      <td>what do you mean it will help us to relax</td>\n",
       "      <td>[0, 0, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 47, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 18, 13, 5, 161, 10, 23, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 18, 13, 5, 161, 10, 23, 101, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what do you mean it will help us to relax</td>\n",
       "      <td>do you really think so i do not it will just m...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 18, 13, 5, 161, 10, 23, 101, 9...</td>\n",
       "      <td>[1, 13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, ...</td>\n",
       "      <td>[13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, 102...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>do you really think so i do not it will just m...</td>\n",
       "      <td>i guess you are right but what shall we do i d...</td>\n",
       "      <td>[13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, 102...</td>\n",
       "      <td>[1, 4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4,...</td>\n",
       "      <td>[4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i guess you are right but what shall we do i d...</td>\n",
       "      <td>i suggest a walk over to the gym where we can ...</td>\n",
       "      <td>[4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4, 13...</td>\n",
       "      <td>[1, 4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 2...</td>\n",
       "      <td>[4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 21, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i suggest a walk over to the gym where we can ...</td>\n",
       "      <td>that 's a good idea i hear mary and sally ofte...</td>\n",
       "      <td>[4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 21, ...</td>\n",
       "      <td>[1, 11, 38, 8, 47, 179, 4, 237, 441, 14, 3323,...</td>\n",
       "      <td>[11, 38, 8, 47, 179, 4, 237, 441, 14, 3323, 30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>that 's a good idea i hear mary and sally ofte...</td>\n",
       "      <td>sounds great to me if they are willing we coul...</td>\n",
       "      <td>[11, 38, 8, 47, 179, 4, 237, 441, 14, 3323, 30...</td>\n",
       "      <td>[1, 154, 99, 7, 26, 57, 54, 17, 1083, 22, 79, ...</td>\n",
       "      <td>[154, 99, 7, 26, 57, 54, 17, 1083, 22, 79, 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sounds great to me if they are willing we coul...</td>\n",
       "      <td>good let us go now</td>\n",
       "      <td>[154, 99, 7, 26, 57, 54, 17, 1083, 22, 79, 200...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 47, 74, 93, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 47, 74, 93, 59,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>good let us go now</td>\n",
       "      <td>all right</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 47, 74, 93, 59,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 50,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>can you do push ups</td>\n",
       "      <td>of course i can it is a piece of cake believe ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 5, 13, 1635...</td>\n",
       "      <td>[1, 16, 125, 4, 21, 10, 9, 8, 773, 16, 899, 25...</td>\n",
       "      <td>[16, 125, 4, 21, 10, 9, 8, 773, 16, 899, 254, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0   say how about going for a few beers after dinner   \n",
       "1  you know that is tempting but is really not go...   \n",
       "2          what do you mean it will help us to relax   \n",
       "3  do you really think so i do not it will just m...   \n",
       "4  i guess you are right but what shall we do i d...   \n",
       "5  i suggest a walk over to the gym where we can ...   \n",
       "6  that 's a good idea i hear mary and sally ofte...   \n",
       "7  sounds great to me if they are willing we coul...   \n",
       "8                                 good let us go now   \n",
       "9                                can you do push ups   \n",
       "\n",
       "                                            response  \\\n",
       "0  you know that is tempting but is really not go...   \n",
       "1          what do you mean it will help us to relax   \n",
       "2  do you really think so i do not it will just m...   \n",
       "3  i guess you are right but what shall we do i d...   \n",
       "4  i suggest a walk over to the gym where we can ...   \n",
       "5  that 's a good idea i hear mary and sally ofte...   \n",
       "6  sounds great to me if they are willing we coul...   \n",
       "7                                 good let us go now   \n",
       "8                                          all right   \n",
       "9  of course i can it is a piece of cake believe ...   \n",
       "\n",
       "                                  encoder_input_data  \\\n",
       "0  [0, 0, 0, 0, 0, 138, 33, 37, 75, 20, 8, 206, 3...   \n",
       "1  [0, 0, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 47, ...   \n",
       "2  [0, 0, 0, 0, 0, 18, 13, 5, 161, 10, 23, 101, 9...   \n",
       "3  [13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, 102...   \n",
       "4  [4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4, 13...   \n",
       "5  [4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 21, ...   \n",
       "6  [11, 38, 8, 47, 179, 4, 237, 441, 14, 3323, 30...   \n",
       "7  [154, 99, 7, 26, 57, 54, 17, 1083, 22, 79, 200...   \n",
       "8  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 47, 74, 93, 59,...   \n",
       "9  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 5, 13, 1635...   \n",
       "\n",
       "                                  decoder_input_data  \\\n",
       "0  [0, 0, 1, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 4...   \n",
       "1  [0, 0, 0, 0, 0, 1, 18, 13, 5, 161, 10, 23, 101...   \n",
       "2  [1, 13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, ...   \n",
       "3  [1, 4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4,...   \n",
       "4  [1, 4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 2...   \n",
       "5  [1, 11, 38, 8, 47, 179, 4, 237, 441, 14, 3323,...   \n",
       "6  [1, 154, 99, 7, 26, 57, 54, 17, 1083, 22, 79, ...   \n",
       "7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 47, 74, 93, ...   \n",
       "8  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 50,...   \n",
       "9  [1, 16, 125, 4, 21, 10, 9, 8, 773, 16, 899, 25...   \n",
       "\n",
       "                                 decoder_output_data  \n",
       "0  [0, 0, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 47, ...  \n",
       "1  [0, 0, 0, 0, 0, 18, 13, 5, 161, 10, 23, 101, 9...  \n",
       "2  [13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, 102...  \n",
       "3  [4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4, 13...  \n",
       "4  [4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 21, ...  \n",
       "5  [11, 38, 8, 47, 179, 4, 237, 441, 14, 3323, 30...  \n",
       "6  [154, 99, 7, 26, 57, 54, 17, 1083, 22, 79, 200...  \n",
       "7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 47, 74, 93, 59,...  \n",
       "8  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 53...  \n",
       "9  [16, 125, 4, 21, 10, 9, 8, 773, 16, 899, 254, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the DataFrame\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "file_path_parquet = os.path.join(data_dir, 'training_df_dd_tf210.parquet')\n",
    "training_data_final = pd.read_parquet(file_path_parquet)\n",
    "\n",
    "training_data_final.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad6aab19-4df8-461d-9dc1-393d1157a83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15384\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.word_index))\n",
    "print(tokenizer.num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3fbffc-0d50-455a-8d1a-2608dad745e3",
   "metadata": {},
   "source": [
    "### Set the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39ff9d2f-e6b8-4c47-aeb5-5af9071ad308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(89861, 15)\n",
      "(89861, 16)\n",
      "(89861, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data = np.array(training_data_final['encoder_input_data'].tolist())\n",
    "decoder_input_data = np.array(training_data_final['decoder_input_data'].tolist())\n",
    "decoder_output_data = np.array(training_data_final['decoder_output_data'].tolist())\n",
    "print(type(encoder_input_data))\n",
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)\n",
    "print(decoder_output_data.shape)\n",
    "encoder_input_data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4757a0c5-f8c6-4aa5-b851-f6f6a86ebe33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data = encoder_input_data.astype('int32')\n",
    "decoder_input_data = decoder_input_data.astype('int32')\n",
    "decoder_output_data = decoder_output_data.astype('int32')\n",
    "encoder_input_data.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076347eb-91fb-4830-a259-27e6241faa67",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01390974-ff00-45ca-ad7b-56b0d60959c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = models.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training, mask=None):\n",
    "        attn_output = self.att(inputs, inputs, attention_mask=mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return self.token_emb.compute_mask(inputs, mask=mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f978610b-f189-4682-a431-47bb450be5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 16)]         0           []                               \n",
      "                                                                                                  \n",
      " token_and_position_embedding_1  (None, 16, 256)     3942656     ['input_2[0][0]']                \n",
      "  (TokenAndPositionEmbedding)                                                                     \n",
      "                                                                                                  \n",
      " tf.math.not_equal_1 (TFOpLambd  (None, 16)          0           ['input_2[0][0]']                \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " transformer_block_1 (Transform  (None, 16, 256)     2367488     ['token_and_position_embedding_1[\n",
      " erBlock)                                                        0][0]',                          \n",
      "                                                                  'tf.math.not_equal_1[0][0]']    \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 15)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 16, 15385)    3953945     ['transformer_block_1[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,264,089\n",
      "Trainable params: 10,264,089\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(vocab_size, max_encoder_seq_length, max_decoder_seq_length, embedding_dim, num_heads, ff_dim):\n",
    "    # Encoder\n",
    "    encoder_inputs = layers.Input(shape=(max_encoder_seq_length,))\n",
    "    encoder_embedding = TokenAndPositionEmbedding(max_encoder_seq_length, vocab_size, embedding_dim)\n",
    "    encoder_mask = encoder_embedding.compute_mask(encoder_inputs)\n",
    "    encoder_transformer_block = TransformerBlock(embedding_dim, num_heads, ff_dim)\n",
    "    encoder_output = encoder_transformer_block(encoder_embedding(encoder_inputs), mask=encoder_mask)\n",
    "\n",
    "    # Decoder\n",
    "    decoder_inputs = layers.Input(shape=(max_decoder_seq_length,))\n",
    "    decoder_embedding = TokenAndPositionEmbedding(max_decoder_seq_length, vocab_size, embedding_dim)\n",
    "    decoder_mask = decoder_embedding.compute_mask(decoder_inputs)\n",
    "    decoder_transformer_block = TransformerBlock(embedding_dim, num_heads, ff_dim)\n",
    "    decoder_output = decoder_transformer_block(decoder_embedding(decoder_inputs), mask=decoder_mask)\n",
    "\n",
    "    # Output\n",
    "    decoder_dense = layers.Dense(vocab_size, activation=\"softmax\")\n",
    "    decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "    # Model\n",
    "    transformer = models.Model([encoder_inputs, decoder_inputs], decoder_output)\n",
    "\n",
    "    return transformer\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 256\n",
    "num_heads = 8\n",
    "ff_dim = 512\n",
    "max_encoder_seq_length = 15\n",
    "max_decoder_seq_length = 16\n",
    "\n",
    "# Create and compile the model\n",
    "model = create_model(vocab_size, max_encoder_seq_length, max_decoder_seq_length, embedding_dim, num_heads, ff_dim)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe71e88b-5724-4c14-ada4-e7f7f2ceb383",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f01f4fdd-9573-4f2f-973e-accfb00f5da2",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'model/transformer_block_1/multi_head_attention_1/and_1' defined at (most recent call last):\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n      super().run_forever()\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\tomui\\AppData\\Local\\Temp\\ipykernel_21136\\908720210.py\", line 2, in <module>\n      history = model.fit(\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tomui\\AppData\\Local\\Temp\\ipykernel_21136\\3638583998.py\", line 14, in call\n      attn_output = self.att(inputs, inputs, attention_mask=mask)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 553, in call\n      attention_mask = self._compute_attention_mask(\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 670, in _compute_attention_mask\n      else tf.cast(attention_mask, bool) & auto_mask\nNode: 'model/transformer_block_1/multi_head_attention_1/and_1'\nrequired broadcastable shapes\n\t [[{{node model/transformer_block_1/multi_head_attention_1/and_1}}]] [Op:__inference_train_function_2726]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mencoder_input_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_output_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Ensure proper shape for loss function\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increase epochs for actual training\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'model/transformer_block_1/multi_head_attention_1/and_1' defined at (most recent call last):\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n      super().run_forever()\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\tomui\\AppData\\Local\\Temp\\ipykernel_21136\\908720210.py\", line 2, in <module>\n      history = model.fit(\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tomui\\AppData\\Local\\Temp\\ipykernel_21136\\3638583998.py\", line 14, in call\n      attn_output = self.att(inputs, inputs, attention_mask=mask)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 553, in call\n      attention_mask = self._compute_attention_mask(\n    File \"C:\\Users\\tomui\\anaconda3\\envs\\tensorflow_cuda_gpu_py310\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 670, in _compute_attention_mask\n      else tf.cast(attention_mask, bool) & auto_mask\nNode: 'model/transformer_block_1/multi_head_attention_1/and_1'\nrequired broadcastable shapes\n\t [[{{node model/transformer_block_1/multi_head_attention_1/and_1}}]] [Op:__inference_train_function_2726]"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    np.expand_dims(decoder_output_data, -1),  # Ensure proper shape for loss function\n",
    "    batch_size=64,\n",
    "    epochs=1,  # Increase epochs for actual training\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda22363-777c-4bac-b55c-b8ba47fd7175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c39826-1839-4bf9-9f32-9fc66393e49c",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084c9088-7775-4318-81f1-00909b13434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.word_index['<START>'], tokenizer.word_index['<END>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba8439-f79d-4b17-b107-f96d61e72dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, test_seq, max_length):\n",
    "    test_seq = np.expand_dims(test_seq, axis=0)\n",
    "    target_seq = np.array([[tokenizer.word_index['<START>']]])\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        padded_target_seq = pad_sequences([target_seq[0]], maxlen=max_length, padding='post')\n",
    "        predictions = model.predict([test_seq, padded_target_seq], verbose=0)\n",
    "        predicted_index = np.argmax(predictions[0, len(target_seq[0]) - 1, :])\n",
    "\n",
    "        if predicted_index == tokenizer.word_index['<END>']:\n",
    "            break\n",
    "\n",
    "        target_seq = np.append(target_seq, [[predicted_index]], axis=-1)\n",
    "\n",
    "    generated_text = tokenizer.sequences_to_texts([target_seq[0]])[0]\n",
    "    generated_text = generated_text.replace('<START>', '').replace('<END>', '').strip()\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b68f62-8d82-4ea3-9061-cb798fcef3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 15\n",
    "\n",
    "# Test examples\n",
    "test_examples = [\n",
    "    \"How are you doing today?\",\n",
    "    \"What is your name?\",\n",
    "    \"Can you help me with my homework?\",\n",
    "    \"What is the weather like?\",\n",
    "    \"Tell me a joke.\",\n",
    "    \"Who is the president of the United States?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Do you like pizza?\",\n",
    "    \"What is your favorite color?\",\n",
    "    \"Goodbye!\"\n",
    "]\n",
    "\n",
    "# Preprocess input text\n",
    "input_text = [preprocess_text(text) for text in test_examples]\n",
    "\n",
    "# Tokenize and pad the test examples\n",
    "test_sequences = tokenizer.texts_to_sequences(input_text)\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=max_encoder_seq_length, padding='pre', truncating='post')\n",
    "\n",
    "# Generate responses\n",
    "for test_seq in padded_test_sequences:\n",
    "    response = generate_response(model, tokenizer, test_seq, max_length=15)\n",
    "    print(f\"Input: {test_examples[padded_test_sequences.tolist().index(test_seq.tolist())]}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d4e44a-16fd-4693-a72a-ce2be1d291c9",
   "metadata": {},
   "source": [
    "### Save the Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915cbad1-d71a-4caa-bbcd-ccd336cc2978",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "weights_path = os.path.join(data_dir, 's2s_model_dd_tf210_weights_transformer.h5')\n",
    "transformer_chatbot.save_weights(weights_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
