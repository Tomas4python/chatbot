{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90199c4d-e447-46b3-9c7a-e633a3c02f09",
   "metadata": {},
   "source": [
    "# TRAINING SIMPLE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14260f0-4962-42e2-8527-f5189a158c76",
   "metadata": {
    "id": "b0ZiOK0MWE7d"
   },
   "source": [
    "## GPU info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63f70056-2151-4051-8974-714b3e237e43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1715927490489,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "bhvFI7ztVLRy",
    "outputId": "e4fb02b2-7ebf-4f3e-b2a8-f9f189a25c47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 18 18:08:22 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 551.61                 Driver Version: 551.61         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1660 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   92C    P0             23W /   60W |    5612MiB /   6144MiB |     59%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1428    C+G   ...8bbwe\\SnippingTool\\SnippingTool.exe      N/A      |\n",
      "|    0   N/A  N/A      5176    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A      9712    C+G   ...ft Office\\root\\Office16\\WINWORD.EXE      N/A      |\n",
      "|    0   N/A  N/A     10484    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     11752      C   ...ensorflow_cuda_gpu_py310\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     11804      C   ...ensorflow_cuda_gpu_py310\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     12444    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     13816    C+G   ...on\\124.0.2478.97\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     15112    C+G   ...l\\Microsoft\\Teams\\current\\Teams.exe      N/A      |\n",
      "|    0   N/A  N/A     16744    C+G   ...al\\Programs\\Messenger\\Messenger.exe      N/A      |\n",
      "|    0   N/A  N/A     16940    C+G   ...__8wekyb3d8bbwe\\Microsoft.Notes.exe      N/A      |\n",
      "|    0   N/A  N/A     16964    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     17404    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     17528    C+G   ...l\\Microsoft\\Teams\\current\\Teams.exe      N/A      |\n",
      "|    0   N/A  N/A     18136    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     20448    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     22284    C+G   ...GeForce Experience\\NVIDIA Share.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a58ec6-07dc-4261-ae09-67ab59ed3fb6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23496,
     "status": "ok",
     "timestamp": 1715927517410,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "RMth1TfbAys6",
    "outputId": "766cb4f5-edfa-4cd7-ad09-825604e4c786"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d5d74b-5e87-4198-b112-9f91b7f65d0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4075,
     "status": "ok",
     "timestamp": 1715927650106,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "db509470-cb51-4c21-84ca-5f928073d68e",
    "outputId": "95b90920-4333-45db-f52c-0f88f1ec6a5e"
   },
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_gpu_details():\n",
    "    devices = device_lib.list_local_devices()\n",
    "    for device in devices:\n",
    "        if device.device_type == 'GPU':\n",
    "            print(f\"Device Name: {device.name}\")\n",
    "            print(f\"Memory Limit: {device.memory_limit} bytes\")\n",
    "            print(f\"Description: {device.physical_device_desc}\")\n",
    "\n",
    "get_gpu_details()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72de3458-7b40-491d-a66f-d8355a455243",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1715927650106,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "6648d492-f4e0-40a8-a3ff-012fa0a0d293"
   },
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe7f8ff-d35e-416c-8252-fc19fc3570e2",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1715927650106,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "8a0af574-6748-467b-8ba6-4da51794c5b9"
   },
   "outputs": [],
   "source": [
    "# !pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d83013-6a91-4f74-bee7-da28b5c4f085",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1715927650106,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "7e4e2a98-f1d4-4f72-b95e-6a75f457e8e8"
   },
   "outputs": [],
   "source": [
    "# !pip install fastparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0510248e-c233-485a-b707-7a7eb9856939",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a59de0-817a-4089-9a14-0b45925b6298",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 15 # Length of input and target sequences, padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c8162-54ab-45da-b4f0-2d5472decdd8",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e231807-af60-4571-be49-655d80d8eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, Concatenate, Layer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56f5d37-9a67-4e51-b0b8-404df9d734a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1715930801381,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "y5srV6bjOpFb",
    "outputId": "b74db1e8-1afe-4348-be1a-23c150492567"
   },
   "outputs": [],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')  # Tokenizer\n",
    "nltk.download('wordnet')  # Lemmatizer\n",
    "nltk.download('stopwords')  # Stopwords\n",
    "nltk.download('omw-1.4') # Ensures multilingual contexts\n",
    "\n",
    "# Stopwords list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "initial_preprocessing = True\n",
    "\n",
    "# # Load spaCy's English NLP model\n",
    "# nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b393a656-df75-4deb-af28-d8ba36f526eb",
   "metadata": {
    "id": "f5150d2f-999d-45ad-b2f2-b9b65b883e89"
   },
   "source": [
    "## Create prepocessing functions for initial text and later response generation preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75edfc9e-3560-4ab7-9992-963b8df9f537",
   "metadata": {
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1715930818326,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "f0e8f237-5274-43ee-8ac2-964dd9214ae5"
   },
   "outputs": [],
   "source": [
    "contractions = {\n",
    "    \"’\": \"'\",\n",
    "    \"‘\": \"'\",\n",
    "    \"“\": '\"',\n",
    "    \"”\": '\"',\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"n't\": \" not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"thats's\": \"that is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"that'll\": \"that will\",\n",
    "    \"there'll\": \"there will\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"ain't\": \"is not\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"daren't\": \"dare not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"usedn't\": \"used not\"\n",
    "}\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    # Normalize Unicode string to NFKD form, remove non-ASCII characters, and then decode it back to a UTF-8 string\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove spaces around apostrophes\n",
    "    text = re.sub(r\"\\s*'\\s*\", \"'\", text)\n",
    "    # Add a space before and after any punctuation mark (., !, or ?)\n",
    "    text = re.sub(r\"\\s*([.!?])\\s*\", r\" \\1 \", text)\n",
    "    # Correct contractions\n",
    "    for contraction, replacement in contractions.items():\n",
    "        text = re.sub(re.escape(contraction), replacement, text)\n",
    "    # Replace any sequence of characters that are not letters, basic punctuation\n",
    "    text = re.sub(r\"[^a-z' ]\", ' ', text) # re.sub(r\"[^a-z.,'!? ]\", ' ', text)\n",
    "    # Replace any sequence of whitespace characters with a single space and remove leading and trailing whitespace\n",
    "    text = re.sub(r\"\\s+\", ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_names(text: str) -> str:\n",
    "    # Use spaCy to detect and remove names from the text\n",
    "    doc = nlp(text)\n",
    "    filtered_text = ' '.join([token.text for token in doc if token.ent_type_ != 'PERSON']) # Takes really long time, exlude from chatbot input preprocessing\n",
    "    return filtered_text\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # Normalize text\n",
    "    text = normalize_text(text)\n",
    "    # Remove names using spaCy's NER\n",
    "    if initial_preprocessing:\n",
    "        text = remove_names(text)\n",
    "    # # Remove punctuation\n",
    "    # text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stopwords and tokenize\n",
    "    # words = word_tokenize(text) # More intelligent splitting\n",
    "    # filtered_words = [word for word in words if word not in stop_words]\n",
    "    # # Lemmatize words\n",
    "    # lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    # Add <SOS> and <EOS> tokens, and join the list into a single string\n",
    "    # return ' '.join(['sofs'] + lemmatized_words + ['eofs'])\n",
    "        # Trim the text to the desired length\n",
    "    words = text.split()[:max_length]\n",
    "    trimmed_text = ' '.join(words)  # Consider to remove trimming, if you want pad later on max length\n",
    "    return trimmed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5deb6d2-695b-4025-b6b1-72428d051053",
   "metadata": {},
   "source": [
    "### Load the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5428c87-4ce6-480a-bb32-48f7a6063e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from file\n",
    "data_dir = os.path.join(os.getcwd(), 'data_dd')\n",
    "tokenizer_path = os.path.join(data_dir, 'tokenizer_dd.pickle')\n",
    "with open(tokenizer_path, 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e0ec68-2723-436d-a101-ad18df9f0a63",
   "metadata": {},
   "source": [
    "### Loading the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8036e60-f3a3-4beb-9f96-da6496797413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the DataFrame\n",
    "data_dir = os.path.join(os.getcwd(), 'data_dd')\n",
    "file_path_parquet = os.path.join(data_dir, 'training_df_dd.parquet')\n",
    "training_data_final = pd.read_parquet(file_path_parquet)\n",
    "\n",
    "training_data_final.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6aab19-4df8-461d-9dc1-393d1157a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tokenizer.word_index))\n",
    "print(tokenizer.num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3fbffc-0d50-455a-8d1a-2608dad745e3",
   "metadata": {},
   "source": [
    "### Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ff9d2f-e6b8-4c47-aeb5-5af9071ad308",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.array(training_data_final['encoder_input_data'].tolist())\n",
    "decoder_input_data = np.array(training_data_final['decoder_input_data'].tolist())\n",
    "decoder_output_data = np.array(training_data_final['decoder_output_data'].tolist())\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "encoder_input_train, encoder_input_val, decoder_input_train, decoder_input_val, decoder_output_train, decoder_output_val = train_test_split(\n",
    "    encoder_input_data, decoder_input_data, decoder_output_data, test_size=0.1, random_state=22)\n",
    "print(encoder_input_train.shape)\n",
    "print(encoder_input_val.shape)\n",
    "print(decoder_input_train.shape)\n",
    "print(decoder_input_val.shape)\n",
    "print(decoder_output_train.shape)\n",
    "print(decoder_output_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076347eb-91fb-4830-a259-27e6241faa67",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5feee9c-140d-4bf3-bd4b-fd7f576ce7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "latent_dim = 100\n",
    "num_encoder_tokens = len(tokenizer.word_index) + 1\n",
    "num_decoder_tokens = len(tokenizer.word_index) + 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define model parameters\n",
    "latent_dim = 100\n",
    "\n",
    "# Define encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding = Embedding(num_encoder_tokens, latent_dim, mask_zero=True)(encoder_inputs)\n",
    "# Make the LSTM layer bidirectional\n",
    "encoder_lstm = Bidirectional(LSTM(latent_dim, return_state=True, dropout=0.3, kernel_regularizer=l2(0.01)))  # , recurrent_dropout=0.2) Removed recurrent_dropout for cuDNN compatibility\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedding)\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Update latent_dim to match the concatenated states\n",
    "latent_dim *= 2\n",
    "\n",
    "# Define decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(num_decoder_tokens, latent_dim, mask_zero=True)\n",
    "decoder_embedded = decoder_embedding(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.3, kernel_regularizer=l2(0.01))  # , recurrent_dropout=0.2) Removed recurrent_dropout for cuDNN compatibility\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedded, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Define checkpoint callback\n",
    "checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/models/seq2seq_dd_model.h5', save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6a4f9-b6f4-4a00-a92b-dcb869e5dda7",
   "metadata": {},
   "source": [
    "### Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9067870-1a68-4a69-bdd4-d8abc6cce21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the directory and file path\n",
    "# data_dir = os.path.join(os.getcwd(), 'data_dd')\n",
    "# file_path_h5 = os.path.join(data_dir, 'seq2seq_dd_model_9 ep_val_loss.h5')\n",
    "\n",
    "# # Load the model\n",
    "# model = load_model(file_path_h5)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe71e88b-5724-4c14-ada4-e7f7f2ceb383",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86cf1fe-85e1-4142-8c54-04d178b499e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    [encoder_input_train, decoder_input_train],\n",
    "    decoder_output_train[:, :, np.newaxis],  # Sparse categorical crossentropy expects a 3D target array\n",
    "    batch_size=64,\n",
    "    epochs=25,\n",
    "    validation_data=([encoder_input_val, decoder_input_val], decoder_output_val[:, :, np.newaxis]),\n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda22363-777c-4bac-b55c-b8ba47fd7175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c39826-1839-4bf9-9f32-9fc66393e49c",
   "metadata": {},
   "source": [
    "### Generating Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba8439-f79d-4b17-b107-f96d61e72dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoder model for inference\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Define decoder model for inference\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_inference_inputs = Input(shape=(None,))\n",
    "decoder_embedding_inference = decoder_embedding(decoder_inference_inputs)\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding_inference, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inference_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# Save token index mappings\n",
    "target_token_index = tokenizer.word_index\n",
    "reverse_target_token_index = {v: k for k, v in target_token_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171a9727-3484-4968-a1e8-b26e3dd695d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate responses\n",
    "def generate_response(input_seq: np.ndarray, max_decoder_seq_length: int) -> str:\n",
    "    # Encode the input sequence to get the internal states\n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "\n",
    "    # Generate empty target sequence of length 1 with only the start token\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = tokenizer.word_index['<START>']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
    "\n",
    "        # Sample a token and add the corresponding character to the decoded sentence\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_token_index[sampled_token_index]\n",
    "        decoded_sentence += ' ' + sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length or find stop token\n",
    "        if (sampled_char == '<END>' or len(decoded_sentence.split()) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence.strip().replace('<START>', '').replace('<END>', '').strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0f56fc-ba3e-40fd-b965-ba93098dda02",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b68f62-8d82-4ea3-9061-cb798fcef3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_preprocessing = False  # Expects spaCy to detect and remove names from the text\n",
    "max_length = 15\n",
    "\n",
    "# Define ten examples to test the model\n",
    "test_examples = [\n",
    "    \"How are you doing today?\",\n",
    "    \"What is your name?\",\n",
    "    \"Can you help me with my homework?\",\n",
    "    \"What is the weather like?\",\n",
    "    \"Tell me a joke.\",\n",
    "    \"Who is the president of the United States?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Do you like pizza?\",\n",
    "    \"What is your favorite color?\",\n",
    "    \"Goodbye!\"\n",
    "]\n",
    "\n",
    "# Preprocess input text (assuming preprocess_text is defined elsewhere)\n",
    "input_text = [preprocess_text(text) for text in test_examples]\n",
    "print(f\"Preprocessed text: {input_text}\")\n",
    "\n",
    "# Tokenize and pad the test examples\n",
    "test_sequences = tokenizer.texts_to_sequences(input_text)\n",
    "print(f\"Tokenizer sequences: {test_sequences}\")\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=max_length, padding='pre', truncating='post')\n",
    "print(f\"Padded sequences: {padded_test_sequences}\")\n",
    "\n",
    "# Generate responses\n",
    "for test_seq in padded_test_sequences:\n",
    "    input_seq = np.array([test_seq])\n",
    "    response = generate_response(input_seq, max_length)\n",
    "    print(f\"Input: {test_examples[padded_test_sequences.tolist().index(test_seq.tolist())]}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf3485-ef0e-4ced-a681-daaa0eca5012",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e2505-888d-4941-b041-3ae9dc061fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'data_dd')\n",
    "file_path_h5 = os.path.join(data_dir, 's2s_model_dd.h5')\n",
    "model.save(file_path_h5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
