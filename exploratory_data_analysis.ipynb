{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f0fb967-b515-4054-9c06-c75938fa7c9f",
   "metadata": {},
   "source": [
    "# CAPSTONE PROJECT - CHATBOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5fa74bc-fba1-44b1-a9a1-c994c4d91e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe3579f-4161-47c3-8237-d609725e0964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16178972-a2e7-4e96-92a8-46f15515490a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Name: /device:GPU:0\n",
      "Memory Limit: 4158652416 bytes\n",
      "Description: device: 0, name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_gpu_details():\n",
    "    devices = device_lib.list_local_devices()\n",
    "    for device in devices:\n",
    "        if device.device_type == 'GPU':\n",
    "            print(f\"Device Name: {device.name}\")\n",
    "            print(f\"Memory Limit: {device.memory_limit} bytes\")\n",
    "            print(f\"Description: {device.physical_device_desc}\")\n",
    "\n",
    "get_gpu_details()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c593d8-dfc1-420a-8f40-7947c049dc69",
   "metadata": {},
   "source": [
    "## DATA\n",
    "### Data sources\n",
    "https://convokit.cornell.edu/documentation/movie.html <br>\n",
    "https://www.cs.cornell.edu/~cristian/Chameleons_in_imagined_conversations.html "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1328c9a6-4d8e-497b-b764-a9ab3eb9f243",
   "metadata": {},
   "source": [
    "### Install ConvoKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d5d481c-79d5-45d7-8883-aa169633c231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install convokit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a957699a-a417-4860-aee7-581bce869888",
   "metadata": {},
   "source": [
    "### Load data from source and save to 'data' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d1157ce-ff4a-440c-bfa4-7d6f0261e902",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from convokit import Corpus, download\n",
    "# import os\n",
    "\n",
    "# # Directory where to save the corpus\n",
    "# data_dir = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "# # Ensure the directory exists\n",
    "# if not os.path.exists(data_dir):\n",
    "#     os.makedirs(data_dir)\n",
    "\n",
    "# # Downloading and saving the corpus\n",
    "# corpus = Corpus(filename=download(\"movie-corpus\", data_dir=data_dir))\n",
    "\n",
    "# # Saving the corpus to the 'data' folder\n",
    "# corpus_path = os.path.join(data_dir, \"movie_corpus\")\n",
    "# corpus.dump(corpus_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9de684-120b-478a-8e60-345a3b392703",
   "metadata": {
    "tags": []
   },
   "source": [
    "Downloading movie-corpus to C:\\Users\\tomui\\Desktop\\capstone_project\\data\\movie-corpus  \n",
    "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0b8ca3-4757-49f0-826a-7a0a3824824a",
   "metadata": {},
   "source": [
    "### Load data from 'data' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8169d5a6-b021-4752-81ee-ed2f81b322e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Speakers: 9035\n",
      "Number of Utterances: 304713\n",
      "Number of Conversations: 83097\n"
     ]
    }
   ],
   "source": [
    "from convokit import Corpus\n",
    "import os\n",
    "\n",
    "# Directory where to load the corpus\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "# Load the corpus from the specified folder\n",
    "loaded_corpus = Corpus(filename=os.path.join(data_dir, \"movie_corpus\"))\n",
    "loaded_corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee27d1da-9ff5-468e-824b-6a02727e1535",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convokit.model.corpus.Corpus"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loaded_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568a5418-4b10-482f-9c59-685028b784df",
   "metadata": {},
   "source": [
    "### Data Structure and Organization\n",
    "```plaintext\n",
    "data/\n",
    "└── movie_corpus/\n",
    "    ├── conversations.json\n",
    "    ├── corpus.json\n",
    "    ├── index.json\n",
    "    ├── speakers.json\n",
    "    └── utterances.jsonl\n",
    "```\n",
    "\n",
    "Description [here](https://convokit.cornell.edu/documentation/movie.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92813c-7f90-4b46-bc21-9e814d5d82bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Choice for exploration\n",
    "The files I need from ConvoKit corpus for my chatbot project depend on the specific functionalities I want to implement in my chatbot. I'll most likely need `utterances.json` because it contains the dialogue data. This is what I'll use to train chatbot to understand and generate human-like responses.\n",
    "\n",
    "Description from source:  \n",
    "> \"Utterance-level information <br>\n",
    "> For each utterance, we provide:\n",
    "> - id: index of the utterance\n",
    "> - speaker: the speaker who authored the utterance\n",
    "> - conversation_id: id of the first utterance in the conversation this utterance belongs to\n",
    "> - reply_to: id of the utterance to which this utterance replies to (None if the utterance is not a reply)\n",
    "> - timestamp: time of the utterance\n",
    "> - text: textual content of the utterance\n",
    "> \n",
    "> Metadata for utterances include:\n",
    "> - movie_idx: index of the movie from which this utterance occurs\n",
    "> - parsed: parsed version of the utterance text, represented as a SpaCy Doc\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55bf77c-945b-4e5e-a56d-f8186c4d023c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Understanding data from `utterances.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d6acc31-6281-4a4c-8f06-40673e42bf61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint as pp\n",
    "\n",
    "# Initialize a list to hold all the utterances\n",
    "utterances = []\n",
    "\n",
    "# Open the file and read line by line\n",
    "with open(os.path.join(data_dir, 'movie_corpus', 'utterances.jsonl'), 'r') as file:\n",
    "    \n",
    "    for line in file:\n",
    "        utterance = json.loads(line)\n",
    "        utterances.append(utterance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c26682ea-4ebb-4705-9933-b91553312d2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(utterances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aac7a307-5edf-4bd6-a360-6def6f7523cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 304713 lines\n",
      "\n",
      "[{'conversation_id': 'L1044',\n",
      "  'id': 'L1045',\n",
      "  'meta': {'movie_id': 'm0',\n",
      "           'parsed': [{'rt': 1,\n",
      "                       'toks': [{'dep': 'nsubj',\n",
      "                                 'dn': [],\n",
      "                                 'tag': 'PRP',\n",
      "                                 'tok': 'They',\n",
      "                                 'up': 1},\n",
      "                                {'dep': 'ROOT',\n",
      "                                 'dn': [0, 2, 3],\n",
      "                                 'tag': 'VBP',\n",
      "                                 'tok': 'do'},\n",
      "                                {'dep': 'neg',\n",
      "                                 'dn': [],\n",
      "                                 'tag': 'RB',\n",
      "                                 'tok': 'not',\n",
      "                                 'up': 1},\n",
      "                                {'dep': 'punct',\n",
      "                                 'dn': [],\n",
      "                                 'tag': '.',\n",
      "                                 'tok': '!',\n",
      "                                 'up': 1}]}]},\n",
      "  'reply-to': 'L1044',\n",
      "  'speaker': 'u0',\n",
      "  'text': 'They do not!',\n",
      "  'timestamp': None,\n",
      "  'vectors': []},\n",
      " {'conversation_id': 'L1044',\n",
      "  'id': 'L1044',\n",
      "  'meta': {'movie_id': 'm0',\n",
      "           'parsed': [{'rt': 1,\n",
      "                       'toks': [{'dep': 'nsubj',\n",
      "                                 'dn': [],\n",
      "                                 'tag': 'PRP',\n",
      "                                 'tok': 'They',\n",
      "                                 'up': 1},\n",
      "                                {'dep': 'ROOT',\n",
      "                                 'dn': [0, 2, 3],\n",
      "                                 'tag': 'VBP',\n",
      "                                 'tok': 'do'},\n",
      "                                {'dep': 'dobj',\n",
      "                                 'dn': [],\n",
      "                                 'tag': 'TO',\n",
      "                                 'tok': 'to',\n",
      "                                 'up': 1},\n",
      "                                {'dep': 'punct',\n",
      "                                 'dn': [],\n",
      "                                 'tag': '.',\n",
      "                                 'tok': '!',\n",
      "                                 'up': 1}]}]},\n",
      "  'reply-to': None,\n",
      "  'speaker': 'u2',\n",
      "  'text': 'They do to!',\n",
      "  'timestamp': None,\n",
      "  'vectors': []},\n",
      " {'conversation_id': 'L984',\n",
      "  'id': 'L985',\n",
      "  'meta': {'movie_id': 'm0',\n",
      "           'parsed': [{'rt': 1,\n",
      "                       'toks': [{'dep': 'nsubj',\n",
      "                                 'dn': [],\n",
      "                                 'tag': 'PRP',\n",
      "                                 'tok': 'I',\n",
      "                                 'up': 1},\n",
      "                                {'dep': 'ROOT',\n",
      "                                 'dn': [0, 2, 3],\n",
      "                                 'tag': 'VBP',\n",
      "                                 'tok': 'hope'},\n",
      "                                {'dep': 'advmod',\n",
      "                                 'dn': [],\n",
      "                                 'tag': 'RB',\n",
      "                                 'tok': 'so',\n",
      "                                 'up': 1},\n",
      "                                {'dep': 'punct',\n",
      "                                 'dn': [],\n",
      "                                 'tag': '.',\n",
      "                                 'tok': '.',\n",
      "                                 'up': 1}]}]},\n",
      "  'reply-to': 'L984',\n",
      "  'speaker': 'u0',\n",
      "  'text': 'I hope so.',\n",
      "  'timestamp': None,\n",
      "  'vectors': []}]\n"
     ]
    }
   ],
   "source": [
    "print(f'There are a total of {len(utterances)} lines\\n')\n",
    "pp(utterances[: 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eac7ca7-8b69-4c5c-abd3-204b45d827bd",
   "metadata": {},
   "source": [
    "### Understanding data from other dataset json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65a6a94c-4467-4488-a2cb-8de7840acc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# with open(os.path.join(data_dir, 'movie_corpus', 'conversations.json'), 'r') as file:\n",
    "#     conversations = json.load(file)\n",
    "# with open(os.path.join(data_dir, 'movie_corpus', 'corpus.json'), 'r') as file:\n",
    "#     conversations = json.load(file)\n",
    "# with open(os.path.join(data_dir, 'movie_corpus', 'index.json'), 'r') as file:\n",
    "#     conversations = json.load(file)\n",
    "# with open(os.path.join(data_dir, 'movie_corpus', 'speakers.json'), 'r') as file:\n",
    "#     conversations = json.load(file)\n",
    "\n",
    "# print(type(conversations))\n",
    "\n",
    "# print(f'There are a total of {len(conversations)} keys in the dictionary\\n')\n",
    "# first_three_items = list(conversations.items())[:3]\n",
    "# pp(first_three_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55568df2-4bb1-4526-907a-455e217fe2d4",
   "metadata": {},
   "source": [
    "## Decision regarding data\n",
    "\n",
    "In developing the chatbot I made the decision to collect only data from the utterances.json file to ensure the chatbot can effectively manage and understand multi-turn conversations. The essential data elements to be gathered include `'text'` for generating responses, `'conversation_id'` for tracking the flow of conversations, and `'reply_to'` for understanding response sequences within the dialogue. While initially, the chatbot will not utilize complex NLP features like parsed linguistic data, the architecture will allow for the integration of these advanced features in the future. While initially I will collect `'parsed'` and `'toks'` information from the utterances.json file, the decision on whether to use this pre-parsed data directly, generate similar data anew, or conduct comparisons between the two will be made later as the project evolves. This approach ensures flexibility in utilizing advanced NLP features as required, maintaining the adaptability of the architecture for future enhancements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a7d75-7c31-4ebc-ae31-904ad911e3a6",
   "metadata": {},
   "source": [
    "## Converting utterances data to DataFrame\n",
    "Pandas provides a powerful and easy-to-use interface for data manipulation, filtering, transformation, and analysis, and integration with Python Ecosystem: seamless integration with other Python libraries for data analysis, machine learning (e.g., scikit-learn, TensorFlow), and visualization (e.g., Matplotlib, Seaborn), as well fast processing for datasets that fit comfortably in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5336f550-8d49-4abe-86e0-4517a993cf71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Flatten the data\n",
    "def flatten_data(data):\n",
    "    flattened_data = []\n",
    "    for entry in data:\n",
    "        flat_entry = {\n",
    "            'id': entry['id'],\n",
    "            'conversation_id': entry['conversation_id'],\n",
    "            'text': entry['text'],\n",
    "            'speaker': entry['speaker'],\n",
    "            'reply_to': entry.get('reply-to'),\n",
    "            'timestamp': entry['timestamp'],\n",
    "            'movie_id': entry['meta']['movie_id'],\n",
    "        }\n",
    "        # Handle nested parsed data\n",
    "        for parsed in entry['meta']['parsed']:\n",
    "            for idx, tok in enumerate(parsed['toks']):\n",
    "                flat_entry[f'tok_{idx}_token'] = tok['tok']\n",
    "                flat_entry[f'tok_{idx}_tag'] = tok['tag']\n",
    "                flat_entry[f'tok_{idx}_dep'] = tok['dep']\n",
    "                # Add other fields from tokens as needed\n",
    "        flattened_data.append(flat_entry)\n",
    "    return flattened_data\n",
    "\n",
    "# Convert to DataFrame\n",
    "flattened_data = flatten_data(utterances)\n",
    "df = pd.DataFrame(flattened_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08e5e09e-d6e0-45d9-b9d8-767c441470be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>tok_0_token</th>\n",
       "      <th>tok_0_tag</th>\n",
       "      <th>tok_0_dep</th>\n",
       "      <th>...</th>\n",
       "      <th>tok_121_dep</th>\n",
       "      <th>tok_122_token</th>\n",
       "      <th>tok_122_tag</th>\n",
       "      <th>tok_122_dep</th>\n",
       "      <th>tok_123_token</th>\n",
       "      <th>tok_123_tag</th>\n",
       "      <th>tok_123_dep</th>\n",
       "      <th>tok_124_token</th>\n",
       "      <th>tok_124_tag</th>\n",
       "      <th>tok_124_dep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1045</td>\n",
       "      <td>L1044</td>\n",
       "      <td>They do not!</td>\n",
       "      <td>u0</td>\n",
       "      <td>L1044</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>They</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "      <td>They do to!</td>\n",
       "      <td>u2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>They</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L985</td>\n",
       "      <td>L984</td>\n",
       "      <td>I hope so.</td>\n",
       "      <td>u0</td>\n",
       "      <td>L984</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "      <td>She okay?</td>\n",
       "      <td>u2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>She</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L925</td>\n",
       "      <td>L924</td>\n",
       "      <td>Let's go.</td>\n",
       "      <td>u0</td>\n",
       "      <td>L924</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>Let</td>\n",
       "      <td>VB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 382 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id conversation_id          text speaker reply_to timestamp movie_id  \\\n",
       "0  L1045           L1044  They do not!      u0    L1044      None       m0   \n",
       "1  L1044           L1044   They do to!      u2     None      None       m0   \n",
       "2   L985            L984    I hope so.      u0     L984      None       m0   \n",
       "3   L984            L984     She okay?      u2     None      None       m0   \n",
       "4   L925            L924     Let's go.      u0     L924      None       m0   \n",
       "\n",
       "  tok_0_token tok_0_tag tok_0_dep  ... tok_121_dep tok_122_token tok_122_tag  \\\n",
       "0        They       PRP     nsubj  ...         NaN           NaN         NaN   \n",
       "1        They       PRP     nsubj  ...         NaN           NaN         NaN   \n",
       "2           I       PRP     nsubj  ...         NaN           NaN         NaN   \n",
       "3         She       PRP     nsubj  ...         NaN           NaN         NaN   \n",
       "4         Let        VB      ROOT  ...         NaN           NaN         NaN   \n",
       "\n",
       "  tok_122_dep tok_123_token tok_123_tag tok_123_dep tok_124_token tok_124_tag  \\\n",
       "0         NaN           NaN         NaN         NaN           NaN         NaN   \n",
       "1         NaN           NaN         NaN         NaN           NaN         NaN   \n",
       "2         NaN           NaN         NaN         NaN           NaN         NaN   \n",
       "3         NaN           NaN         NaN         NaN           NaN         NaN   \n",
       "4         NaN           NaN         NaN         NaN           NaN         NaN   \n",
       "\n",
       "  tok_124_dep  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  \n",
       "\n",
       "[5 rows x 382 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show DataFrame to check structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2845afc3-b7dc-403a-ade7-79a83c710e00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 304713 entries, 0 to 304712\n",
      "Columns: 382 entries, id to tok_124_dep\n",
      "dtypes: object(382)\n",
      "memory usage: 888.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37724f59-ec6d-4493-b7eb-ec280663b221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                      0\n",
      "conversation_id         0\n",
      "text                    0\n",
      "speaker                 0\n",
      "reply_to            83097\n",
      "timestamp          304713\n",
      "movie_id                0\n",
      "tok_0_token           267\n",
      "tok_0_tag             267\n",
      "tok_0_dep             267\n",
      "tok_1_token           625\n",
      "tok_1_tag             625\n",
      "tok_1_dep             625\n",
      "tok_2_token         21729\n",
      "tok_2_tag           21729\n",
      "tok_2_dep           21729\n",
      "tok_3_token         38515\n",
      "tok_3_tag           38515\n",
      "tok_3_dep           38515\n",
      "tok_4_token         63316\n",
      "tok_4_tag           63316\n",
      "tok_4_dep           63316\n",
      "tok_5_token         92404\n",
      "tok_5_tag           92404\n",
      "tok_5_dep           92404\n",
      "tok_6_token        121962\n",
      "tok_6_tag          121962\n",
      "tok_6_dep          121962\n",
      "tok_7_token        149962\n",
      "tok_7_tag          149962\n",
      "tok_7_dep          149962\n",
      "tok_8_token        175281\n",
      "tok_8_tag          175281\n",
      "tok_8_dep          175281\n",
      "tok_9_token        196627\n",
      "tok_9_tag          196627\n",
      "tok_9_dep          196627\n",
      "tok_10_token       214433\n",
      "tok_10_tag         214433\n",
      "tok_10_dep         214433\n",
      "tok_11_token       229294\n",
      "tok_11_tag         229294\n",
      "tok_11_dep         229294\n",
      "tok_12_token       241453\n",
      "tok_12_tag         241453\n",
      "tok_12_dep         241453\n",
      "tok_13_token       251574\n",
      "tok_13_tag         251574\n",
      "tok_13_dep         251574\n",
      "tok_14_token       260081\n",
      "tok_14_tag         260081\n",
      "tok_14_dep         260081\n",
      "tok_15_token       267106\n",
      "tok_15_tag         267106\n",
      "tok_15_dep         267106\n",
      "tok_16_token       273061\n",
      "tok_16_tag         273061\n",
      "tok_16_dep         273061\n",
      "tok_17_token       278121\n",
      "tok_17_tag         278121\n",
      "tok_17_dep         278121\n",
      "tok_18_token       282396\n",
      "tok_18_tag         282396\n",
      "tok_18_dep         282396\n",
      "tok_19_token       285961\n",
      "tok_19_tag         285961\n",
      "tok_19_dep         285961\n",
      "tok_20_token       288804\n",
      "tok_20_tag         288804\n",
      "tok_20_dep         288804\n",
      "tok_21_token       291267\n",
      "tok_21_tag         291267\n",
      "tok_21_dep         291267\n",
      "tok_22_token       293375\n",
      "tok_22_tag         293375\n",
      "tok_22_dep         293375\n",
      "tok_23_token       295125\n",
      "tok_23_tag         295125\n",
      "tok_23_dep         295125\n",
      "tok_24_token       296564\n",
      "tok_24_tag         296564\n",
      "tok_24_dep         296564\n",
      "tok_25_token       297736\n",
      "tok_25_tag         297736\n",
      "tok_25_dep         297736\n",
      "tok_26_token       298756\n",
      "tok_26_tag         298756\n",
      "tok_26_dep         298756\n",
      "tok_27_token       299618\n",
      "tok_27_tag         299618\n",
      "tok_27_dep         299618\n",
      "tok_28_token       300391\n",
      "tok_28_tag         300391\n",
      "tok_28_dep         300391\n",
      "tok_29_token       301006\n",
      "tok_29_tag         301006\n",
      "tok_29_dep         301006\n",
      "tok_30_token       301511\n",
      "tok_30_tag         301511\n",
      "tok_30_dep         301511\n",
      "tok_31_token       301953\n",
      "tok_31_tag         301953\n",
      "tok_31_dep         301953\n",
      "tok_32_token       302347\n",
      "tok_32_tag         302347\n",
      "tok_32_dep         302347\n",
      "tok_33_token       302679\n",
      "tok_33_tag         302679\n",
      "tok_33_dep         302679\n",
      "tok_34_token       302947\n",
      "tok_34_tag         302947\n",
      "tok_34_dep         302947\n",
      "tok_35_token       303177\n",
      "tok_35_tag         303177\n",
      "tok_35_dep         303177\n",
      "tok_36_token       303366\n",
      "tok_36_tag         303366\n",
      "tok_36_dep         303366\n",
      "tok_37_token       303540\n",
      "tok_37_tag         303540\n",
      "tok_37_dep         303540\n",
      "tok_38_token       303700\n",
      "tok_38_tag         303700\n",
      "tok_38_dep         303700\n",
      "tok_39_token       303841\n",
      "tok_39_tag         303841\n",
      "tok_39_dep         303841\n",
      "tok_40_token       303935\n",
      "tok_40_tag         303935\n",
      "tok_40_dep         303935\n",
      "tok_41_token       304030\n",
      "tok_41_tag         304030\n",
      "tok_41_dep         304030\n",
      "tok_42_token       304127\n",
      "tok_42_tag         304127\n",
      "tok_42_dep         304127\n",
      "tok_43_token       304190\n",
      "tok_43_tag         304190\n",
      "tok_43_dep         304190\n",
      "tok_44_token       304252\n",
      "tok_44_tag         304252\n",
      "tok_44_dep         304252\n",
      "tok_45_token       304309\n",
      "tok_45_tag         304309\n",
      "tok_45_dep         304309\n",
      "tok_46_token       304355\n",
      "tok_46_tag         304355\n",
      "tok_46_dep         304355\n",
      "tok_47_token       304400\n",
      "tok_47_tag         304400\n",
      "tok_47_dep         304400\n",
      "tok_48_token       304429\n",
      "tok_48_tag         304429\n",
      "tok_48_dep         304429\n",
      "tok_49_token       304459\n",
      "tok_49_tag         304459\n",
      "tok_49_dep         304459\n",
      "tok_50_token       304484\n",
      "tok_50_tag         304484\n",
      "tok_50_dep         304484\n",
      "tok_51_token       304510\n",
      "tok_51_tag         304510\n",
      "tok_51_dep         304510\n",
      "tok_52_token       304531\n",
      "tok_52_tag         304531\n",
      "tok_52_dep         304531\n",
      "tok_53_token       304550\n",
      "tok_53_tag         304550\n",
      "tok_53_dep         304550\n",
      "tok_54_token       304569\n",
      "tok_54_tag         304569\n",
      "tok_54_dep         304569\n",
      "tok_55_token       304582\n",
      "tok_55_tag         304582\n",
      "tok_55_dep         304582\n",
      "tok_56_token       304594\n",
      "tok_56_tag         304594\n",
      "tok_56_dep         304594\n",
      "tok_57_token       304606\n",
      "tok_57_tag         304606\n",
      "tok_57_dep         304606\n",
      "tok_58_token       304615\n",
      "tok_58_tag         304615\n",
      "tok_58_dep         304615\n",
      "tok_59_token       304623\n",
      "tok_59_tag         304623\n",
      "tok_59_dep         304623\n",
      "tok_60_token       304636\n",
      "tok_60_tag         304636\n",
      "tok_60_dep         304636\n",
      "tok_61_token       304640\n",
      "tok_61_tag         304640\n",
      "tok_61_dep         304640\n",
      "tok_62_token       304646\n",
      "tok_62_tag         304646\n",
      "tok_62_dep         304646\n",
      "tok_63_token       304654\n",
      "tok_63_tag         304654\n",
      "tok_63_dep         304654\n",
      "tok_64_token       304664\n",
      "tok_64_tag         304664\n",
      "tok_64_dep         304664\n",
      "tok_65_token       304669\n",
      "tok_65_tag         304669\n",
      "tok_65_dep         304669\n",
      "tok_66_token       304675\n",
      "tok_66_tag         304675\n",
      "tok_66_dep         304675\n",
      "tok_67_token       304678\n",
      "tok_67_tag         304678\n",
      "tok_67_dep         304678\n",
      "tok_68_token       304680\n",
      "tok_68_tag         304680\n",
      "tok_68_dep         304680\n",
      "tok_69_token       304686\n",
      "tok_69_tag         304686\n",
      "tok_69_dep         304686\n",
      "tok_70_token       304690\n",
      "tok_70_tag         304690\n",
      "tok_70_dep         304690\n",
      "tok_71_token       304692\n",
      "tok_71_tag         304692\n",
      "tok_71_dep         304692\n",
      "tok_72_token       304694\n",
      "tok_72_tag         304694\n",
      "tok_72_dep         304694\n",
      "tok_73_token       304696\n",
      "tok_73_tag         304696\n",
      "tok_73_dep         304696\n",
      "tok_74_token       304697\n",
      "tok_74_tag         304697\n",
      "tok_74_dep         304697\n",
      "tok_75_token       304698\n",
      "tok_75_tag         304698\n",
      "tok_75_dep         304698\n",
      "tok_76_token       304702\n",
      "tok_76_tag         304702\n",
      "tok_76_dep         304702\n",
      "tok_77_token       304702\n",
      "tok_77_tag         304702\n",
      "tok_77_dep         304702\n",
      "tok_78_token       304705\n",
      "tok_78_tag         304705\n",
      "tok_78_dep         304705\n",
      "tok_79_token       304706\n",
      "tok_79_tag         304706\n",
      "tok_79_dep         304706\n",
      "tok_80_token       304707\n",
      "tok_80_tag         304707\n",
      "tok_80_dep         304707\n",
      "tok_81_token       304707\n",
      "tok_81_tag         304707\n",
      "tok_81_dep         304707\n",
      "tok_82_token       304707\n",
      "tok_82_tag         304707\n",
      "tok_82_dep         304707\n",
      "tok_83_token       304707\n",
      "tok_83_tag         304707\n",
      "tok_83_dep         304707\n",
      "tok_84_token       304707\n",
      "tok_84_tag         304707\n",
      "tok_84_dep         304707\n",
      "tok_85_token       304708\n",
      "tok_85_tag         304708\n",
      "tok_85_dep         304708\n",
      "tok_86_token       304708\n",
      "tok_86_tag         304708\n",
      "tok_86_dep         304708\n",
      "tok_87_token       304709\n",
      "tok_87_tag         304709\n",
      "tok_87_dep         304709\n",
      "tok_88_token       304709\n",
      "tok_88_tag         304709\n",
      "tok_88_dep         304709\n",
      "tok_89_token       304710\n",
      "tok_89_tag         304710\n",
      "tok_89_dep         304710\n",
      "tok_90_token       304710\n",
      "tok_90_tag         304710\n",
      "tok_90_dep         304710\n",
      "tok_91_token       304711\n",
      "tok_91_tag         304711\n",
      "tok_91_dep         304711\n",
      "tok_92_token       304711\n",
      "tok_92_tag         304711\n",
      "tok_92_dep         304711\n",
      "tok_93_token       304711\n",
      "tok_93_tag         304711\n",
      "tok_93_dep         304711\n",
      "tok_94_token       304711\n",
      "tok_94_tag         304711\n",
      "tok_94_dep         304711\n",
      "tok_95_token       304711\n",
      "tok_95_tag         304711\n",
      "tok_95_dep         304711\n",
      "tok_96_token       304711\n",
      "tok_96_tag         304711\n",
      "tok_96_dep         304711\n",
      "tok_97_token       304711\n",
      "tok_97_tag         304711\n",
      "tok_97_dep         304711\n",
      "tok_98_token       304711\n",
      "tok_98_tag         304711\n",
      "tok_98_dep         304711\n",
      "tok_99_token       304711\n",
      "tok_99_tag         304711\n",
      "tok_99_dep         304711\n",
      "tok_100_token      304711\n",
      "tok_100_tag        304711\n",
      "tok_100_dep        304711\n",
      "tok_101_token      304712\n",
      "tok_101_tag        304712\n",
      "tok_101_dep        304712\n",
      "tok_102_token      304712\n",
      "tok_102_tag        304712\n",
      "tok_102_dep        304712\n",
      "tok_103_token      304712\n",
      "tok_103_tag        304712\n",
      "tok_103_dep        304712\n",
      "tok_104_token      304712\n",
      "tok_104_tag        304712\n",
      "tok_104_dep        304712\n",
      "tok_105_token      304712\n",
      "tok_105_tag        304712\n",
      "tok_105_dep        304712\n",
      "tok_106_token      304712\n",
      "tok_106_tag        304712\n",
      "tok_106_dep        304712\n",
      "tok_107_token      304712\n",
      "tok_107_tag        304712\n",
      "tok_107_dep        304712\n",
      "tok_108_token      304712\n",
      "tok_108_tag        304712\n",
      "tok_108_dep        304712\n",
      "tok_109_token      304712\n",
      "tok_109_tag        304712\n",
      "tok_109_dep        304712\n",
      "tok_110_token      304712\n",
      "tok_110_tag        304712\n",
      "tok_110_dep        304712\n",
      "tok_111_token      304712\n",
      "tok_111_tag        304712\n",
      "tok_111_dep        304712\n",
      "tok_112_token      304712\n",
      "tok_112_tag        304712\n",
      "tok_112_dep        304712\n",
      "tok_113_token      304712\n",
      "tok_113_tag        304712\n",
      "tok_113_dep        304712\n",
      "tok_114_token      304712\n",
      "tok_114_tag        304712\n",
      "tok_114_dep        304712\n",
      "tok_115_token      304712\n",
      "tok_115_tag        304712\n",
      "tok_115_dep        304712\n",
      "tok_116_token      304712\n",
      "tok_116_tag        304712\n",
      "tok_116_dep        304712\n",
      "tok_117_token      304712\n",
      "tok_117_tag        304712\n",
      "tok_117_dep        304712\n",
      "tok_118_token      304712\n",
      "tok_118_tag        304712\n",
      "tok_118_dep        304712\n",
      "tok_119_token      304712\n",
      "tok_119_tag        304712\n",
      "tok_119_dep        304712\n",
      "tok_120_token      304712\n",
      "tok_120_tag        304712\n",
      "tok_120_dep        304712\n",
      "tok_121_token      304712\n",
      "tok_121_tag        304712\n",
      "tok_121_dep        304712\n",
      "tok_122_token      304712\n",
      "tok_122_tag        304712\n",
      "tok_122_dep        304712\n",
      "tok_123_token      304712\n",
      "tok_123_tag        304712\n",
      "tok_123_dep        304712\n",
      "tok_124_token      304712\n",
      "tok_124_tag        304712\n",
      "tok_124_dep        304712\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Temporarily adjust display settings to show all columns\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(df.isnull().sum())\n",
    "#print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6acbac4-a8a0-4380-9083-b672443d8459",
   "metadata": {},
   "source": [
    "## Saving the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a192d2c-68fb-4062-9c6b-bba33dc0f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fa7802e-00ba-4bf4-b809-4c50902ba93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e9f5895-8d1a-4900-9e50-8c6062ce1348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving the DataFrame\n",
    "file_path_parquet = os.path.join(data_dir, 'utterances.parquet')\n",
    "df.to_parquet(file_path_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df029d37-dc2e-4173-87ce-7c7f4d48ccc2",
   "metadata": {},
   "source": [
    "`utterances.jsonl` - 351 404 KB, `utterances.parquet` - 28 409 KB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30907873-7514-495a-90aa-1e9f247cf987",
   "metadata": {},
   "source": [
    "## Loading the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93a10f80-f191-48a5-99f0-4930f607c96b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>tok_0_token</th>\n",
       "      <th>tok_0_tag</th>\n",
       "      <th>tok_0_dep</th>\n",
       "      <th>...</th>\n",
       "      <th>tok_121_dep</th>\n",
       "      <th>tok_122_token</th>\n",
       "      <th>tok_122_tag</th>\n",
       "      <th>tok_122_dep</th>\n",
       "      <th>tok_123_token</th>\n",
       "      <th>tok_123_tag</th>\n",
       "      <th>tok_123_dep</th>\n",
       "      <th>tok_124_token</th>\n",
       "      <th>tok_124_tag</th>\n",
       "      <th>tok_124_dep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1045</td>\n",
       "      <td>L1044</td>\n",
       "      <td>They do not!</td>\n",
       "      <td>u0</td>\n",
       "      <td>L1044</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>They</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "      <td>They do to!</td>\n",
       "      <td>u2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>They</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L985</td>\n",
       "      <td>L984</td>\n",
       "      <td>I hope so.</td>\n",
       "      <td>u0</td>\n",
       "      <td>L984</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "      <td>She okay?</td>\n",
       "      <td>u2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>She</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L925</td>\n",
       "      <td>L924</td>\n",
       "      <td>Let's go.</td>\n",
       "      <td>u0</td>\n",
       "      <td>L924</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>Let</td>\n",
       "      <td>VB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>L924</td>\n",
       "      <td>L924</td>\n",
       "      <td>Wow</td>\n",
       "      <td>u2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>Wow</td>\n",
       "      <td>UH</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>L872</td>\n",
       "      <td>L870</td>\n",
       "      <td>Okay -- you're gonna need to learn how to lie.</td>\n",
       "      <td>u0</td>\n",
       "      <td>L871</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>Okay</td>\n",
       "      <td>UH</td>\n",
       "      <td>intj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>L871</td>\n",
       "      <td>L870</td>\n",
       "      <td>No</td>\n",
       "      <td>u2</td>\n",
       "      <td>L870</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>No</td>\n",
       "      <td>UH</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "      <td>I'm kidding.  You know how sometimes you just ...</td>\n",
       "      <td>u0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>And</td>\n",
       "      <td>CC</td>\n",
       "      <td>cc</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>L869</td>\n",
       "      <td>L866</td>\n",
       "      <td>Like my fear of wearing pastels?</td>\n",
       "      <td>u0</td>\n",
       "      <td>L868</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>Like</td>\n",
       "      <td>IN</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>L868</td>\n",
       "      <td>L866</td>\n",
       "      <td>The \"real you\".</td>\n",
       "      <td>u2</td>\n",
       "      <td>L867</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>L867</td>\n",
       "      <td>L866</td>\n",
       "      <td>What good stuff?</td>\n",
       "      <td>u0</td>\n",
       "      <td>L866</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>What</td>\n",
       "      <td>WDT</td>\n",
       "      <td>det</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>L866</td>\n",
       "      <td>L866</td>\n",
       "      <td>I figured you'd get to the good stuff eventually.</td>\n",
       "      <td>u2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>L865</td>\n",
       "      <td>L862</td>\n",
       "      <td>Thank God!  If I had to hear one more story ab...</td>\n",
       "      <td>u2</td>\n",
       "      <td>L864</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>If</td>\n",
       "      <td>IN</td>\n",
       "      <td>mark</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>L864</td>\n",
       "      <td>L862</td>\n",
       "      <td>Me.  This endless ...blonde babble. I'm like, ...</td>\n",
       "      <td>u0</td>\n",
       "      <td>L863</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>L863</td>\n",
       "      <td>L862</td>\n",
       "      <td>What crap?</td>\n",
       "      <td>u2</td>\n",
       "      <td>L862</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>What</td>\n",
       "      <td>WDT</td>\n",
       "      <td>det</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>L862</td>\n",
       "      <td>L862</td>\n",
       "      <td>do you listen to this crap?</td>\n",
       "      <td>u0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>do</td>\n",
       "      <td>VBP</td>\n",
       "      <td>aux</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>L861</td>\n",
       "      <td>L860</td>\n",
       "      <td>No...</td>\n",
       "      <td>u2</td>\n",
       "      <td>L860</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>No</td>\n",
       "      <td>UH</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>L860</td>\n",
       "      <td>L860</td>\n",
       "      <td>Then Guillermo says, \"If you go any lighter, y...</td>\n",
       "      <td>u0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>Then</td>\n",
       "      <td>RB</td>\n",
       "      <td>advmod</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>L699</td>\n",
       "      <td>L696</td>\n",
       "      <td>You always been this selfish?</td>\n",
       "      <td>u2</td>\n",
       "      <td>L698</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>You</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 382 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id conversation_id                                               text  \\\n",
       "0   L1045           L1044                                       They do not!   \n",
       "1   L1044           L1044                                        They do to!   \n",
       "2    L985            L984                                         I hope so.   \n",
       "3    L984            L984                                          She okay?   \n",
       "4    L925            L924                                          Let's go.   \n",
       "5    L924            L924                                                Wow   \n",
       "6    L872            L870     Okay -- you're gonna need to learn how to lie.   \n",
       "7    L871            L870                                                 No   \n",
       "8    L870            L870  I'm kidding.  You know how sometimes you just ...   \n",
       "9    L869            L866                   Like my fear of wearing pastels?   \n",
       "10   L868            L866                                    The \"real you\".   \n",
       "11   L867            L866                                   What good stuff?   \n",
       "12   L866            L866  I figured you'd get to the good stuff eventually.   \n",
       "13   L865            L862  Thank God!  If I had to hear one more story ab...   \n",
       "14   L864            L862  Me.  This endless ...blonde babble. I'm like, ...   \n",
       "15   L863            L862                                         What crap?   \n",
       "16   L862            L862                        do you listen to this crap?   \n",
       "17   L861            L860                                              No...   \n",
       "18   L860            L860  Then Guillermo says, \"If you go any lighter, y...   \n",
       "19   L699            L696                      You always been this selfish?   \n",
       "\n",
       "   speaker reply_to timestamp movie_id tok_0_token tok_0_tag tok_0_dep  ...  \\\n",
       "0       u0    L1044      None       m0        They       PRP     nsubj  ...   \n",
       "1       u2     None      None       m0        They       PRP     nsubj  ...   \n",
       "2       u0     L984      None       m0           I       PRP     nsubj  ...   \n",
       "3       u2     None      None       m0         She       PRP     nsubj  ...   \n",
       "4       u0     L924      None       m0         Let        VB      ROOT  ...   \n",
       "5       u2     None      None       m0         Wow        UH      ROOT  ...   \n",
       "6       u0     L871      None       m0        Okay        UH      intj  ...   \n",
       "7       u2     L870      None       m0          No        UH      ROOT  ...   \n",
       "8       u0     None      None       m0         And        CC        cc  ...   \n",
       "9       u0     L868      None       m0        Like        IN      ROOT  ...   \n",
       "10      u2     L867      None       m0         The        DT       det  ...   \n",
       "11      u0     L866      None       m0        What       WDT       det  ...   \n",
       "12      u2     None      None       m0           I       PRP     nsubj  ...   \n",
       "13      u2     L864      None       m0          If        IN      mark  ...   \n",
       "14      u0     L863      None       m0           I       PRP     nsubj  ...   \n",
       "15      u2     L862      None       m0        What       WDT       det  ...   \n",
       "16      u0     None      None       m0          do       VBP       aux  ...   \n",
       "17      u2     L860      None       m0          No        UH      ROOT  ...   \n",
       "18      u0     None      None       m0        Then        RB    advmod  ...   \n",
       "19      u2     L698      None       m0         You       PRP     nsubj  ...   \n",
       "\n",
       "   tok_121_dep tok_122_token tok_122_tag tok_122_dep tok_123_token  \\\n",
       "0         None          None        None        None          None   \n",
       "1         None          None        None        None          None   \n",
       "2         None          None        None        None          None   \n",
       "3         None          None        None        None          None   \n",
       "4         None          None        None        None          None   \n",
       "5         None          None        None        None          None   \n",
       "6         None          None        None        None          None   \n",
       "7         None          None        None        None          None   \n",
       "8         None          None        None        None          None   \n",
       "9         None          None        None        None          None   \n",
       "10        None          None        None        None          None   \n",
       "11        None          None        None        None          None   \n",
       "12        None          None        None        None          None   \n",
       "13        None          None        None        None          None   \n",
       "14        None          None        None        None          None   \n",
       "15        None          None        None        None          None   \n",
       "16        None          None        None        None          None   \n",
       "17        None          None        None        None          None   \n",
       "18        None          None        None        None          None   \n",
       "19        None          None        None        None          None   \n",
       "\n",
       "   tok_123_tag tok_123_dep tok_124_token tok_124_tag tok_124_dep  \n",
       "0         None        None          None        None        None  \n",
       "1         None        None          None        None        None  \n",
       "2         None        None          None        None        None  \n",
       "3         None        None          None        None        None  \n",
       "4         None        None          None        None        None  \n",
       "5         None        None          None        None        None  \n",
       "6         None        None          None        None        None  \n",
       "7         None        None          None        None        None  \n",
       "8         None        None          None        None        None  \n",
       "9         None        None          None        None        None  \n",
       "10        None        None          None        None        None  \n",
       "11        None        None          None        None        None  \n",
       "12        None        None          None        None        None  \n",
       "13        None        None          None        None        None  \n",
       "14        None        None          None        None        None  \n",
       "15        None        None          None        None        None  \n",
       "16        None        None          None        None        None  \n",
       "17        None        None          None        None        None  \n",
       "18        None        None          None        None        None  \n",
       "19        None        None          None        None        None  \n",
       "\n",
       "[20 rows x 382 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the DataFrame\n",
    "file_path_parquet = os.path.join(data_dir, 'utterances.parquet')\n",
    "df_loaded_parquet = pd.read_parquet(file_path_parquet)\n",
    "\n",
    "df_loaded_parquet.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2977c6-4473-451b-a36e-bb2c7a6b65f0",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab66833-1b80-4eaa-913b-ff5c938cca74",
   "metadata": {},
   "source": [
    "### Leaving only necessary data for initial stage of the project\n",
    "Id, conversation_id for tracking the flow of conversations and reply_to for understanding the sequence within the dialogue, and conversation text ofcourse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed419e17-c695-4d32-a6f9-8ad8ac93e68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>reply_to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They do not!</td>\n",
       "      <td>L1045</td>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They do to!</td>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I hope so.</td>\n",
       "      <td>L985</td>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>She okay?</td>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Let's go.</td>\n",
       "      <td>L925</td>\n",
       "      <td>L924</td>\n",
       "      <td>L924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wow</td>\n",
       "      <td>L924</td>\n",
       "      <td>L924</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Okay -- you're gonna need to learn how to lie.</td>\n",
       "      <td>L872</td>\n",
       "      <td>L870</td>\n",
       "      <td>L871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>No</td>\n",
       "      <td>L871</td>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I'm kidding.  You know how sometimes you just ...</td>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Like my fear of wearing pastels?</td>\n",
       "      <td>L869</td>\n",
       "      <td>L866</td>\n",
       "      <td>L868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The \"real you\".</td>\n",
       "      <td>L868</td>\n",
       "      <td>L866</td>\n",
       "      <td>L867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What good stuff?</td>\n",
       "      <td>L867</td>\n",
       "      <td>L866</td>\n",
       "      <td>L866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I figured you'd get to the good stuff eventually.</td>\n",
       "      <td>L866</td>\n",
       "      <td>L866</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Thank God!  If I had to hear one more story ab...</td>\n",
       "      <td>L865</td>\n",
       "      <td>L862</td>\n",
       "      <td>L864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Me.  This endless ...blonde babble. I'm like, ...</td>\n",
       "      <td>L864</td>\n",
       "      <td>L862</td>\n",
       "      <td>L863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What crap?</td>\n",
       "      <td>L863</td>\n",
       "      <td>L862</td>\n",
       "      <td>L862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>do you listen to this crap?</td>\n",
       "      <td>L862</td>\n",
       "      <td>L862</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>No...</td>\n",
       "      <td>L861</td>\n",
       "      <td>L860</td>\n",
       "      <td>L860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Then Guillermo says, \"If you go any lighter, y...</td>\n",
       "      <td>L860</td>\n",
       "      <td>L860</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>You always been this selfish?</td>\n",
       "      <td>L699</td>\n",
       "      <td>L696</td>\n",
       "      <td>L698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>But</td>\n",
       "      <td>L698</td>\n",
       "      <td>L696</td>\n",
       "      <td>L697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Then that's all you had to say.</td>\n",
       "      <td>L697</td>\n",
       "      <td>L696</td>\n",
       "      <td>L696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Well, no...</td>\n",
       "      <td>L696</td>\n",
       "      <td>L696</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>You never wanted to go out with 'me, did you?</td>\n",
       "      <td>L695</td>\n",
       "      <td>L693</td>\n",
       "      <td>L694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I was?</td>\n",
       "      <td>L694</td>\n",
       "      <td>L693</td>\n",
       "      <td>L693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I looked for you back at the party, but you al...</td>\n",
       "      <td>L693</td>\n",
       "      <td>L693</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Tons</td>\n",
       "      <td>L663</td>\n",
       "      <td>L662</td>\n",
       "      <td>L662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Have fun tonight?</td>\n",
       "      <td>L662</td>\n",
       "      <td>L662</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>I believe we share an art instructor</td>\n",
       "      <td>L578</td>\n",
       "      <td>L577</td>\n",
       "      <td>L577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>You know Chastity?</td>\n",
       "      <td>L577</td>\n",
       "      <td>L577</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text     id conversation_id  \\\n",
       "0                                        They do not!  L1045           L1044   \n",
       "1                                         They do to!  L1044           L1044   \n",
       "2                                          I hope so.   L985            L984   \n",
       "3                                           She okay?   L984            L984   \n",
       "4                                           Let's go.   L925            L924   \n",
       "5                                                 Wow   L924            L924   \n",
       "6      Okay -- you're gonna need to learn how to lie.   L872            L870   \n",
       "7                                                  No   L871            L870   \n",
       "8   I'm kidding.  You know how sometimes you just ...   L870            L870   \n",
       "9                    Like my fear of wearing pastels?   L869            L866   \n",
       "10                                    The \"real you\".   L868            L866   \n",
       "11                                   What good stuff?   L867            L866   \n",
       "12  I figured you'd get to the good stuff eventually.   L866            L866   \n",
       "13  Thank God!  If I had to hear one more story ab...   L865            L862   \n",
       "14  Me.  This endless ...blonde babble. I'm like, ...   L864            L862   \n",
       "15                                         What crap?   L863            L862   \n",
       "16                        do you listen to this crap?   L862            L862   \n",
       "17                                              No...   L861            L860   \n",
       "18  Then Guillermo says, \"If you go any lighter, y...   L860            L860   \n",
       "19                      You always been this selfish?   L699            L696   \n",
       "20                                                But   L698            L696   \n",
       "21                    Then that's all you had to say.   L697            L696   \n",
       "22                                        Well, no...   L696            L696   \n",
       "23      You never wanted to go out with 'me, did you?   L695            L693   \n",
       "24                                             I was?   L694            L693   \n",
       "25  I looked for you back at the party, but you al...   L693            L693   \n",
       "26                                               Tons   L663            L662   \n",
       "27                                  Have fun tonight?   L662            L662   \n",
       "28               I believe we share an art instructor   L578            L577   \n",
       "29                                 You know Chastity?   L577            L577   \n",
       "\n",
       "   reply_to  \n",
       "0     L1044  \n",
       "1      None  \n",
       "2      L984  \n",
       "3      None  \n",
       "4      L924  \n",
       "5      None  \n",
       "6      L871  \n",
       "7      L870  \n",
       "8      None  \n",
       "9      L868  \n",
       "10     L867  \n",
       "11     L866  \n",
       "12     None  \n",
       "13     L864  \n",
       "14     L863  \n",
       "15     L862  \n",
       "16     None  \n",
       "17     L860  \n",
       "18     None  \n",
       "19     L698  \n",
       "20     L697  \n",
       "21     L696  \n",
       "22     None  \n",
       "23     L694  \n",
       "24     L693  \n",
       "25     None  \n",
       "26     L662  \n",
       "27     None  \n",
       "28     L577  \n",
       "29     None  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations = df_loaded_parquet[['text', 'id', 'conversation_id', 'reply_to']]\n",
    "conversations.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4690dc51-4ad3-494b-af82-a0cbf300bc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries where 'id' equals 'conversation_id': 83097\n",
      "Total entries where 'reply_to' is None: 83097\n"
     ]
    }
   ],
   "source": [
    "# Cheking if counts of None are the same with 'id' == 'conversation_id'\n",
    "print(\"Total entries where 'id' equals 'conversation_id':\", (df['id'] == df['conversation_id']).sum())\n",
    "print(\"Total entries where 'reply_to' is None:\", df['reply_to'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38235458-3018-42ef-b554-d4c79acd5caa",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Looks like the dataset is well-structured and prepared for further processing: <br>\n",
    "Conversation_id and id:  <br>\n",
    "When conversation_id and id are the same and there's no reply_to, this indicates the start of a new conversation, this allows to understand where each conversation begins.  <br>\n",
    "Counts of None in reply_to:  <br>\n",
    "The count of None in the reply_to field matches the number of conversations (83,097). This confirms that each conversation starts with a message that does not reply to any previous message, this is the first message in the thread.  <br>\n",
    "Data Cleanliness:  <br>\n",
    "The alignment of these counts and the consistency of data formatting suggest that dataset is clean and structured. Each message within the dataset is correctly linked to its conversation, and the flow of conversations is well-defined.  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50da00ca-77b9-47f7-a5cc-e6ad457c318a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 304713 entries, 0 to 304712\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   text             304713 non-null  object\n",
      " 1   id               304713 non-null  object\n",
      " 2   conversation_id  304713 non-null  object\n",
      " 3   reply_to         221616 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 9.3+ MB\n"
     ]
    }
   ],
   "source": [
    "conversations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee973182-677d-4de8-a117-fc7fc7ac35ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     304713\n",
       "unique    265774\n",
       "top        What?\n",
       "freq        1684\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations.text.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048d07c7-d160-4ef1-bc97-ced5d1bc22a0",
   "metadata": {},
   "source": [
    "## Analyze text of conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "352cb5a8-53bf-4255-b52d-4c211fbb8724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1b9c18b-ae4f-46fa-b148-2d04666182c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\tomui\\AppData\\Local\\Temp\\ipykernel_165284\\2271790802.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conversations.loc[:, 'msg_length'] = conversations.loc[:, 'text'].apply(len)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average message length (characters): 55.25953930419772\n",
      "Average message length (words): 13.721094931952361\n",
      "Min message length (characters): 0\n",
      "Max message length (characters): 3046\n",
      "Standard deviation (characters): 64.06661834805733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomui\\AppData\\Local\\Temp\\ipykernel_165284\\2271790802.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conversations.loc[:, 'word_count'] = conversations.loc[:, 'text'].apply(lambda x: len(word_tokenize(x)))\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist, word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "# Ensure that the punkt tokenizer is available\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Basic statistics\n",
    "conversations.loc[:, 'msg_length'] = conversations.loc[:, 'text'].apply(len)\n",
    "conversations.loc[:, 'word_count'] = conversations.loc[:, 'text'].apply(lambda x: len(word_tokenize(x)))\n",
    "\n",
    "\n",
    "print(\"Average message length (characters):\", np.mean(conversations['msg_length']))\n",
    "print(\"Average message length (words):\", np.mean(conversations['word_count']))\n",
    "print(\"Min message length (characters):\", np.min(conversations['msg_length']))\n",
    "print(\"Max message length (characters):\", np.max(conversations['msg_length']))\n",
    "print(\"Standard deviation (characters):\", np.std(conversations['msg_length']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726bf30c-44d5-41f4-b113-0ab3aabe91e8",
   "metadata": {},
   "source": [
    "### Word frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "524fc0f8-89b8-4098-a5eb-cc145fce7dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words: [('.', 332912), (',', 170188), ('you', 148400), ('i', 140952), ('?', 110240), ('the', 99132), ('to', 80649), ('a', 70839), (\"'s\", 66538), ('it', 66076), (\"n't\", 55224), ('...', 50796), ('do', 47049), ('that', 46582), ('and', 45934), ('of', 39338), ('!', 37866), ('what', 37719), ('in', 34129), ('me', 32203), ('is', 31639), ('we', 29291), ('he', 27408), ('--', 26662), ('this', 24616), ('for', 23415), ('have', 22934), (\"'m\", 22578), (\"'re\", 21717), ('know', 21657), ('was', 21407), ('your', 20962), ('my', 20824), ('not', 19883), ('on', 19560), ('no', 19425), ('be', 19414), ('are', 17600), ('but', 17321), ('with', 17249), ('they', 16942), ('just', 15853), ('all', 15392), ('like', 15007), (\"'ll\", 14613), ('did', 14547), ('there', 14446), ('get', 14152), ('about', 14000), ('so', 13447)]\n"
     ]
    }
   ],
   "source": [
    "all_words = ' '.join(conversations['text']).lower()\n",
    "words = word_tokenize(all_words)\n",
    "freq_dist = FreqDist(words)\n",
    "print(\"Most common words:\", freq_dist.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba737cb-fb87-4e16-bd4d-e3d5df9af995",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5cda7ce-2c7e-478f-bf5b-85ad496f1dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentiment (polarity): 0.04174547982992158\n",
      "Sentiment distribution: count    304713.000000\n",
      "mean          0.041745\n",
      "std           0.246197\n",
      "min          -1.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.013889\n",
      "max           1.000000\n",
      "Name: sentiment, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomui\\AppData\\Local\\Temp\\ipykernel_165284\\4254834136.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conversations.loc[:, 'sentiment'] = conversations.loc[:, 'text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n"
     ]
    }
   ],
   "source": [
    "conversations.loc[:, 'sentiment'] = conversations.loc[:, 'text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "print(\"Average sentiment (polarity):\", np.mean(conversations['sentiment']))\n",
    "print(\"Sentiment distribution:\", conversations['sentiment'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b190a4ec-cffb-471f-917f-3aa8a1eb98c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zero-length messages: 267\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>msg_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>154381</th>\n",
       "      <td></td>\n",
       "      <td>L129216</td>\n",
       "      <td>L129216</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153316</th>\n",
       "      <td></td>\n",
       "      <td>L129434</td>\n",
       "      <td>L129427</td>\n",
       "      <td>L129433</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289507</th>\n",
       "      <td></td>\n",
       "      <td>L624042</td>\n",
       "      <td>L624042</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153885</th>\n",
       "      <td></td>\n",
       "      <td>L128055</td>\n",
       "      <td>L128044</td>\n",
       "      <td>L128054</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154115</th>\n",
       "      <td></td>\n",
       "      <td>L128754</td>\n",
       "      <td>L128752</td>\n",
       "      <td>L128753</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154231</th>\n",
       "      <td></td>\n",
       "      <td>L129490</td>\n",
       "      <td>L129485</td>\n",
       "      <td>L129489</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98568</th>\n",
       "      <td></td>\n",
       "      <td>L535288</td>\n",
       "      <td>L535287</td>\n",
       "      <td>L535287</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153454</th>\n",
       "      <td></td>\n",
       "      <td>L128976</td>\n",
       "      <td>L128976</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153595</th>\n",
       "      <td></td>\n",
       "      <td>L128655</td>\n",
       "      <td>L128650</td>\n",
       "      <td>L128654</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153343</th>\n",
       "      <td></td>\n",
       "      <td>L129381</td>\n",
       "      <td>L129381</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text       id conversation_id reply_to  msg_length  word_count  \\\n",
       "154381       L129216         L129216     None           0           0   \n",
       "153316       L129434         L129427  L129433           0           0   \n",
       "289507       L624042         L624042     None           0           0   \n",
       "153885       L128055         L128044  L128054           0           0   \n",
       "154115       L128754         L128752  L128753           0           0   \n",
       "154231       L129490         L129485  L129489           0           0   \n",
       "98568        L535288         L535287  L535287           0           0   \n",
       "153454       L128976         L128976     None           0           0   \n",
       "153595       L128655         L128650  L128654           0           0   \n",
       "153343       L129381         L129381     None           0           0   \n",
       "\n",
       "        sentiment  \n",
       "154381        0.0  \n",
       "153316        0.0  \n",
       "289507        0.0  \n",
       "153885        0.0  \n",
       "154115        0.0  \n",
       "154231        0.0  \n",
       "98568         0.0  \n",
       "153454        0.0  \n",
       "153595        0.0  \n",
       "153343        0.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating zero characters messages\n",
    "zero_length_messages = conversations[conversations['text'].apply(len) == 0]\n",
    "print(\"Number of zero-length messages:\", len(zero_length_messages))\n",
    "zero_length_messages.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde0265a-8eb2-4d16-b041-c898d500ea15",
   "metadata": {},
   "source": [
    "Same id and conversation_id with None in reply_to - these messages likely represent the start of a conversation. Removing them could impact the structure of the conversation as it might remove the entry point for a conversational thread.\n",
    "Different id and conversation_id with a specific reply_to - these are responses within a conversation. Their removal might disrupt the sequence, making it difficult to follow the flow of the conversation.\n",
    "Messages with a specific reply_to - these indicate replies within the conversation sequence. Removing these could create gaps in the conversation history. I've decided to leave zero text conversations for now, besides thera are only 267 of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6dc250b-588e-45ae-be67-0d49c8a41bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of long messages: 3151\n"
     ]
    }
   ],
   "source": [
    "# Calculating long messages\n",
    "long_messages = conversations[conversations['msg_length'] > 300]\n",
    "print(\"Number of long messages:\", len(long_messages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e2e503e-cb43-44d0-9a91-cb0c67b633b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinforced steel core walls.  Buried phone line, completely separate, not connected to the house's main line and never exposed throughout the house's infrastructure or outside the house -- you can call the police; nobody can cut you off. Your own ventilation system, complete with oxygen scrubber, so you've got plenty of fresh air for as long as you like.  And a bank of video monitors --\n"
     ]
    }
   ],
   "source": [
    "# Print sample long messages\n",
    "sampled_text = long_messages.sample(1)['text'].iloc[0]\n",
    "print(sampled_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c4217-6c54-47a0-994e-c9ed8bf57e2c",
   "metadata": {},
   "source": [
    "Decided to leave for now long messages - considering to use advanced NLP models such as BERT or GPT (from the transformer family), which are adept at understanding context over longer stretches of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee967afc-2af9-40b2-91c7-1ededc3643a4",
   "metadata": {},
   "source": [
    "## Conversations text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de1fab-d8ae-4c74-bee6-42862a5dcfdf",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6f74020-cf03-48cc-a39a-1711a31e8998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She starts givin me some bullshit about it aint there Its somewhere else and we can go get it  Im shootin you in the head right then and there Then Im gonna shoot her in the kneecap find out where my godamn money is I go walkin in there and that nigga Winston or anybody else is in there youre the first man shot understand what Im sayin\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "conversations.loc[:, 'text'] = conversations.loc[:, 'text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "long_messages = conversations[conversations['msg_length'] > 300]\n",
    "sampled_text = long_messages.sample(1)['text'].iloc[0]\n",
    "print(sampled_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb176ba-116b-4e03-a983-39066c0244d9",
   "metadata": {},
   "source": [
    "### Normalize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6b4d4f4-2c73-404d-8886-aef6c698056a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no no please it is not a holy relic  you know we have met already in this very room perhaps you wont remember it you were only six years old  he was giving the most brilliant little concert here as he got off the stool he slipped and fell my sister antoinette helped him up herself and do you know what he did jumped straight into her arms and said will you marry me yes or no\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Convert to lowercase\n",
    "conversations.loc[:, 'text'] = conversations.loc[:, 'text'].str.lower().str.strip()\n",
    "# Function to apply the regex and normalization transformations row-wise\n",
    "def normalize_text(text):\n",
    "    # Remove non-alphanumeric characters except for basic punctuation\n",
    "    text = re.sub(r\"[^a-z0-9.',!? ]\", ' ', text)\n",
    "    # Replace numbers with a special token\n",
    "    text = re.sub(r'\\d+', '<num>', text)\n",
    "    # Normalize accented characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    return text\n",
    "\n",
    "# Apply the normalization function to each row in the 'text' column\n",
    "conversations.loc[:, 'text'] = conversations.loc[:, 'text'].apply(normalize_text)\n",
    "long_messages = conversations[conversations['msg_length'] > 300]\n",
    "sampled_text = long_messages.sample(1)['text'].iloc[0]\n",
    "print(sampled_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4354401-9e48-4b03-a10d-16a8628661e1",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7c54d05-d740-40f9-9048-905c7ca9d6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theres nothing brain dead cunt think theres absolutely way world connect us anything want hang phone call im going send friend mine crack open fucking rib spreader\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "conversations.loc[:, 'text'] = conversations['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "long_messages = conversations[conversations['msg_length'] > 300]\n",
    "sampled_text = long_messages.sample(1)['text'].iloc[0]\n",
    "print(sampled_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00370116-00cc-4f88-b20d-b175ab0639de",
   "metadata": {},
   "source": [
    "## Analyze again text of conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9241cf4-d4ba-401b-b31f-9f52a33948af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average message length (characters): 31.98917670069869\n",
      "Average message length (words): 5.393163402939815\n",
      "Min message length (characters): 0\n",
      "Max message length (characters): 1836\n",
      "Standard deviation (characters): 38.791295277648075\n",
      "25th percentile (characters): 10.0\n",
      "75th percentile (characters): 40.0\n",
      "25th percentile (words): 2.0\n",
      "75th percentile (words): 7.0\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics\n",
    "conversations.loc[:, 'msg_length'] = conversations['text'].apply(len)\n",
    "conversations.loc[:, 'word_count'] = conversations['text'].apply(lambda x: len(word_tokenize(x)))\n",
    "\n",
    "# Calculate basic statistics\n",
    "average_msg_length_chars = np.mean(conversations['msg_length'])\n",
    "average_msg_length_words = np.mean(conversations['word_count'])\n",
    "min_msg_length_chars = np.min(conversations['msg_length'])\n",
    "max_msg_length_chars = np.max(conversations['msg_length'])\n",
    "std_dev_msg_length_chars = np.std(conversations['msg_length'])\n",
    "\n",
    "# Calculate 25th and 75th percentiles\n",
    "percentile_25_chars = conversations['msg_length'].quantile(0.25)\n",
    "percentile_75_chars = conversations['msg_length'].quantile(0.75)\n",
    "percentile_25_words = conversations['word_count'].quantile(0.25)\n",
    "percentile_75_words = conversations['word_count'].quantile(0.75)\n",
    "\n",
    "# Print statistics\n",
    "print(\"Average message length (characters):\", average_msg_length_chars)\n",
    "print(\"Average message length (words):\", average_msg_length_words)\n",
    "print(\"Min message length (characters):\", min_msg_length_chars)\n",
    "print(\"Max message length (characters):\", max_msg_length_chars)\n",
    "print(\"Standard deviation (characters):\", std_dev_msg_length_chars)\n",
    "print(\"25th percentile (characters):\", percentile_25_chars)\n",
    "print(\"75th percentile (characters):\", percentile_75_chars)\n",
    "print(\"25th percentile (words):\", percentile_25_words)\n",
    "print(\"75th percentile (words):\", percentile_75_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6d1f17-991c-44cf-ae52-bfd9a4340f07",
   "metadata": {},
   "source": [
    "### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "132d1c00-a51d-4923-a07e-64d19aa5bfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomui\\AppData\\Local\\Temp\\ipykernel_165284\\1695850789.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conversations.loc[:, 'tokens'] = conversations.loc[:, 'text'].apply(word_tokenize)\n"
     ]
    }
   ],
   "source": [
    "conversations.loc[:, 'tokens'] = conversations.loc[:, 'text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c14ed20-528d-4365-a4ea-a45334f731c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>msg_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>czechoslovakia slavs fighting germans russians...</td>\n",
       "      <td>L2897</td>\n",
       "      <td>L2895</td>\n",
       "      <td>L2896</td>\n",
       "      <td>99</td>\n",
       "      <td>12</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>[czechoslovakia, slavs, fighting, germans, rus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>eastern europe like romania hungary</td>\n",
       "      <td>L2896</td>\n",
       "      <td>L2895</td>\n",
       "      <td>L2895</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[eastern, europe, like, romania, hungary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>maybe ritual thing someone trying send message...</td>\n",
       "      <td>L2895</td>\n",
       "      <td>L2895</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.216667</td>\n",
       "      <td>[maybe, ritual, thing, someone, trying, send, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>look im even sure anything saw outside fire th...</td>\n",
       "      <td>L2893</td>\n",
       "      <td>L2892</td>\n",
       "      <td>L2892</td>\n",
       "      <td>106</td>\n",
       "      <td>17</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>[look, im, even, sure, anything, saw, outside,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>would call</td>\n",
       "      <td>L2892</td>\n",
       "      <td>L2892</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[would, call]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>says shes suspect</td>\n",
       "      <td>L2891</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2890</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[says, shes, suspect]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>maybe dont care either prettiest suspect ive a...</td>\n",
       "      <td>L2890</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2889</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[maybe, dont, care, either, prettiest, suspect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>hmmmm</td>\n",
       "      <td>L2889</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2888</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[hmmmm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>pretty</td>\n",
       "      <td>L2888</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2887</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>[pretty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>super said hed seen didnt live</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2887</td>\n",
       "      <td>None</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>0.234848</td>\n",
       "      <td>[super, said, hed, seen, didnt, live]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     id  \\\n",
       "1200  czechoslovakia slavs fighting germans russians...  L2897   \n",
       "1201                eastern europe like romania hungary  L2896   \n",
       "1202  maybe ritual thing someone trying send message...  L2895   \n",
       "1203  look im even sure anything saw outside fire th...  L2893   \n",
       "1204                                         would call  L2892   \n",
       "1205                                  says shes suspect  L2891   \n",
       "1206  maybe dont care either prettiest suspect ive a...  L2890   \n",
       "1207                                              hmmmm  L2889   \n",
       "1208                                             pretty  L2888   \n",
       "1209                     super said hed seen didnt live  L2887   \n",
       "\n",
       "     conversation_id reply_to  msg_length  word_count  sentiment  \\\n",
       "1200           L2895    L2896          99          12   0.130000   \n",
       "1201           L2895    L2895          35           5   0.000000   \n",
       "1202           L2895     None         150          21  -0.216667   \n",
       "1203           L2892    L2892         106          17   0.250000   \n",
       "1204           L2892     None          10           2   0.000000   \n",
       "1205           L2887    L2890          17           3   0.000000   \n",
       "1206           L2887    L2889          51           8   0.000000   \n",
       "1207           L2887    L2888           5           1   0.000000   \n",
       "1208           L2887    L2887           6           1   0.250000   \n",
       "1209           L2887     None          30           6   0.234848   \n",
       "\n",
       "                                                 tokens  \n",
       "1200  [czechoslovakia, slavs, fighting, germans, rus...  \n",
       "1201          [eastern, europe, like, romania, hungary]  \n",
       "1202  [maybe, ritual, thing, someone, trying, send, ...  \n",
       "1203  [look, im, even, sure, anything, saw, outside,...  \n",
       "1204                                      [would, call]  \n",
       "1205                              [says, shes, suspect]  \n",
       "1206  [maybe, dont, care, either, prettiest, suspect...  \n",
       "1207                                            [hmmmm]  \n",
       "1208                                           [pretty]  \n",
       "1209              [super, said, hed, seen, didnt, live]  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations[1200: 1210]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcb8e2e-120c-4573-9238-b5b6ca784862",
   "metadata": {},
   "source": [
    "### Lemmatize tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8de3a380-ee83-4dcf-8bd5-eb62d4bf20d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>msg_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>czechoslovakia slavs fighting germans russians...</td>\n",
       "      <td>L2897</td>\n",
       "      <td>L2895</td>\n",
       "      <td>L2896</td>\n",
       "      <td>99</td>\n",
       "      <td>12</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>[czechoslovakia, slav, fighting, german, russi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>eastern europe like romania hungary</td>\n",
       "      <td>L2896</td>\n",
       "      <td>L2895</td>\n",
       "      <td>L2895</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[eastern, europe, like, romania, hungary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>maybe ritual thing someone trying send message...</td>\n",
       "      <td>L2895</td>\n",
       "      <td>L2895</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.216667</td>\n",
       "      <td>[maybe, ritual, thing, someone, trying, send, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>look im even sure anything saw outside fire th...</td>\n",
       "      <td>L2893</td>\n",
       "      <td>L2892</td>\n",
       "      <td>L2892</td>\n",
       "      <td>106</td>\n",
       "      <td>17</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>[look, im, even, sure, anything, saw, outside,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>would call</td>\n",
       "      <td>L2892</td>\n",
       "      <td>L2892</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[would, call]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>says shes suspect</td>\n",
       "      <td>L2891</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2890</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[say, shes, suspect]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>maybe dont care either prettiest suspect ive a...</td>\n",
       "      <td>L2890</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2889</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[maybe, dont, care, either, prettiest, suspect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>hmmmm</td>\n",
       "      <td>L2889</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2888</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[hmmmm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>pretty</td>\n",
       "      <td>L2888</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2887</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>[pretty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>super said hed seen didnt live</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2887</td>\n",
       "      <td>None</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>0.234848</td>\n",
       "      <td>[super, said, hed, seen, didnt, live]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     id  \\\n",
       "1200  czechoslovakia slavs fighting germans russians...  L2897   \n",
       "1201                eastern europe like romania hungary  L2896   \n",
       "1202  maybe ritual thing someone trying send message...  L2895   \n",
       "1203  look im even sure anything saw outside fire th...  L2893   \n",
       "1204                                         would call  L2892   \n",
       "1205                                  says shes suspect  L2891   \n",
       "1206  maybe dont care either prettiest suspect ive a...  L2890   \n",
       "1207                                              hmmmm  L2889   \n",
       "1208                                             pretty  L2888   \n",
       "1209                     super said hed seen didnt live  L2887   \n",
       "\n",
       "     conversation_id reply_to  msg_length  word_count  sentiment  \\\n",
       "1200           L2895    L2896          99          12   0.130000   \n",
       "1201           L2895    L2895          35           5   0.000000   \n",
       "1202           L2895     None         150          21  -0.216667   \n",
       "1203           L2892    L2892         106          17   0.250000   \n",
       "1204           L2892     None          10           2   0.000000   \n",
       "1205           L2887    L2890          17           3   0.000000   \n",
       "1206           L2887    L2889          51           8   0.000000   \n",
       "1207           L2887    L2888           5           1   0.000000   \n",
       "1208           L2887    L2887           6           1   0.250000   \n",
       "1209           L2887     None          30           6   0.234848   \n",
       "\n",
       "                                                 tokens  \n",
       "1200  [czechoslovakia, slav, fighting, german, russi...  \n",
       "1201          [eastern, europe, like, romania, hungary]  \n",
       "1202  [maybe, ritual, thing, someone, trying, send, ...  \n",
       "1203  [look, im, even, sure, anything, saw, outside,...  \n",
       "1204                                      [would, call]  \n",
       "1205                               [say, shes, suspect]  \n",
       "1206  [maybe, dont, care, either, prettiest, suspect...  \n",
       "1207                                            [hmmmm]  \n",
       "1208                                           [pretty]  \n",
       "1209              [super, said, hed, seen, didnt, live]  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "conversations.loc[:, 'tokens'] = conversations['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "conversations[1200: 1210]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fad3497-9ac8-47fd-b0bb-a4c3771b4cc6",
   "metadata": {},
   "source": [
    "### Adding `<start>` and `<end>` tokens to lists of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "375bdbd4-1760-4a6a-97f3-d692be349adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomui\\AppData\\Local\\Temp\\ipykernel_165284\\3005176868.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conversations['tokens'] = conversations['tokens'].apply(lambda x: ['<start>'] + x + ['<end>'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>msg_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>czechoslovakia slavs fighting germans russians...</td>\n",
       "      <td>L2897</td>\n",
       "      <td>L2895</td>\n",
       "      <td>L2896</td>\n",
       "      <td>99</td>\n",
       "      <td>12</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>[&lt;start&gt;, czechoslovakia, slav, fighting, germ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>eastern europe like romania hungary</td>\n",
       "      <td>L2896</td>\n",
       "      <td>L2895</td>\n",
       "      <td>L2895</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[&lt;start&gt;, eastern, europe, like, romania, hung...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>maybe ritual thing someone trying send message...</td>\n",
       "      <td>L2895</td>\n",
       "      <td>L2895</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.216667</td>\n",
       "      <td>[&lt;start&gt;, maybe, ritual, thing, someone, tryin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>look im even sure anything saw outside fire th...</td>\n",
       "      <td>L2893</td>\n",
       "      <td>L2892</td>\n",
       "      <td>L2892</td>\n",
       "      <td>106</td>\n",
       "      <td>17</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>[&lt;start&gt;, look, im, even, sure, anything, saw,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>would call</td>\n",
       "      <td>L2892</td>\n",
       "      <td>L2892</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[&lt;start&gt;, would, call, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>says shes suspect</td>\n",
       "      <td>L2891</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2890</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[&lt;start&gt;, say, shes, suspect, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>maybe dont care either prettiest suspect ive a...</td>\n",
       "      <td>L2890</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2889</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[&lt;start&gt;, maybe, dont, care, either, prettiest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>hmmmm</td>\n",
       "      <td>L2889</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2888</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[&lt;start&gt;, hmmmm, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>pretty</td>\n",
       "      <td>L2888</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2887</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>[&lt;start&gt;, pretty, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>super said hed seen didnt live</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2887</td>\n",
       "      <td>None</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>0.234848</td>\n",
       "      <td>[&lt;start&gt;, super, said, hed, seen, didnt, live,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     id  \\\n",
       "1200  czechoslovakia slavs fighting germans russians...  L2897   \n",
       "1201                eastern europe like romania hungary  L2896   \n",
       "1202  maybe ritual thing someone trying send message...  L2895   \n",
       "1203  look im even sure anything saw outside fire th...  L2893   \n",
       "1204                                         would call  L2892   \n",
       "1205                                  says shes suspect  L2891   \n",
       "1206  maybe dont care either prettiest suspect ive a...  L2890   \n",
       "1207                                              hmmmm  L2889   \n",
       "1208                                             pretty  L2888   \n",
       "1209                     super said hed seen didnt live  L2887   \n",
       "\n",
       "     conversation_id reply_to  msg_length  word_count  sentiment  \\\n",
       "1200           L2895    L2896          99          12   0.130000   \n",
       "1201           L2895    L2895          35           5   0.000000   \n",
       "1202           L2895     None         150          21  -0.216667   \n",
       "1203           L2892    L2892         106          17   0.250000   \n",
       "1204           L2892     None          10           2   0.000000   \n",
       "1205           L2887    L2890          17           3   0.000000   \n",
       "1206           L2887    L2889          51           8   0.000000   \n",
       "1207           L2887    L2888           5           1   0.000000   \n",
       "1208           L2887    L2887           6           1   0.250000   \n",
       "1209           L2887     None          30           6   0.234848   \n",
       "\n",
       "                                                 tokens  \n",
       "1200  [<start>, czechoslovakia, slav, fighting, germ...  \n",
       "1201  [<start>, eastern, europe, like, romania, hung...  \n",
       "1202  [<start>, maybe, ritual, thing, someone, tryin...  \n",
       "1203  [<start>, look, im, even, sure, anything, saw,...  \n",
       "1204                      [<start>, would, call, <end>]  \n",
       "1205               [<start>, say, shes, suspect, <end>]  \n",
       "1206  [<start>, maybe, dont, care, either, prettiest...  \n",
       "1207                            [<start>, hmmmm, <end>]  \n",
       "1208                           [<start>, pretty, <end>]  \n",
       "1209  [<start>, super, said, hed, seen, didnt, live,...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding <start> and <end> tokens to each list in the 'tokens' column\n",
    "conversations['tokens'] = conversations['tokens'].apply(lambda x: ['<start>'] + x + ['<end>'])\n",
    "conversations[1200: 1210]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f562c27d-72f7-4dfd-807e-d200a1dd55bf",
   "metadata": {},
   "source": [
    "## Learning conversation id structure more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee531c94-8903-4532-a413-14a9ea9fb286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>msg_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>youre amazingly selfassured anyone ever told</td>\n",
       "      <td>L840</td>\n",
       "      <td>L834</td>\n",
       "      <td>L839</td>\n",
       "      <td>44</td>\n",
       "      <td>6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>[&lt;start&gt;, youre, amazingly, selfassured, anyon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>go prom</td>\n",
       "      <td>L841</td>\n",
       "      <td>L834</td>\n",
       "      <td>L840</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[&lt;start&gt;, go, prom, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>request command</td>\n",
       "      <td>L842</td>\n",
       "      <td>L842</td>\n",
       "      <td>None</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[&lt;start&gt;, request, command, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>know mean</td>\n",
       "      <td>L843</td>\n",
       "      <td>L842</td>\n",
       "      <td>L842</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.312500</td>\n",
       "      <td>[&lt;start&gt;, know, mean, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td></td>\n",
       "      <td>L844</td>\n",
       "      <td>L842</td>\n",
       "      <td>L843</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td></td>\n",
       "      <td>L845</td>\n",
       "      <td>L842</td>\n",
       "      <td>L844</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>wont go</td>\n",
       "      <td>L846</td>\n",
       "      <td>L842</td>\n",
       "      <td>L845</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[&lt;start&gt;, wont, go, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td></td>\n",
       "      <td>L847</td>\n",
       "      <td>L842</td>\n",
       "      <td>L846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>dont want stupid tradition</td>\n",
       "      <td>L848</td>\n",
       "      <td>L842</td>\n",
       "      <td>L847</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>[&lt;start&gt;, dont, want, stupid, tradition, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>create little drama start new rumor</td>\n",
       "      <td>L852</td>\n",
       "      <td>L852</td>\n",
       "      <td>None</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.025568</td>\n",
       "      <td>[&lt;start&gt;, create, little, drama, start, new, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>motive</td>\n",
       "      <td>L853</td>\n",
       "      <td>L852</td>\n",
       "      <td>L852</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[&lt;start&gt;, motive, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>tell</td>\n",
       "      <td>L854</td>\n",
       "      <td>L852</td>\n",
       "      <td>L853</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[&lt;start&gt;, tell, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>need therapy anyone ever told</td>\n",
       "      <td>L855</td>\n",
       "      <td>L852</td>\n",
       "      <td>L854</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[&lt;start&gt;, need, therapy, anyone, ever, told, &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>answer question patrick</td>\n",
       "      <td>L856</td>\n",
       "      <td>L852</td>\n",
       "      <td>L855</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[&lt;start&gt;, answer, question, patrick, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>nothing theres nothing pleasure company</td>\n",
       "      <td>L857</td>\n",
       "      <td>L852</td>\n",
       "      <td>L856</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[&lt;start&gt;, nothing, there, nothing, pleasure, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>guillermo says go lighter youre gonna look lik...</td>\n",
       "      <td>L860</td>\n",
       "      <td>L860</td>\n",
       "      <td>None</td>\n",
       "      <td>59</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[&lt;start&gt;, guillermo, say, go, lighter, youre, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td></td>\n",
       "      <td>L861</td>\n",
       "      <td>L860</td>\n",
       "      <td>L860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>listen crap</td>\n",
       "      <td>L862</td>\n",
       "      <td>L862</td>\n",
       "      <td>None</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>[&lt;start&gt;, listen, crap, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>crap</td>\n",
       "      <td>L863</td>\n",
       "      <td>L862</td>\n",
       "      <td>L862</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>[&lt;start&gt;, crap, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>endless blonde babble im like boring</td>\n",
       "      <td>L864</td>\n",
       "      <td>L862</td>\n",
       "      <td>L863</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>[&lt;start&gt;, endless, blonde, babble, im, like, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>thank god hear one story coiffure</td>\n",
       "      <td>L865</td>\n",
       "      <td>L862</td>\n",
       "      <td>L864</td>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[&lt;start&gt;, thank, god, hear, one, story, coiffu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>figured youd get good stuff eventually</td>\n",
       "      <td>L866</td>\n",
       "      <td>L866</td>\n",
       "      <td>None</td>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>[&lt;start&gt;, figured, youd, get, good, stuff, eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>good stuff</td>\n",
       "      <td>L867</td>\n",
       "      <td>L866</td>\n",
       "      <td>L866</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>[&lt;start&gt;, good, stuff, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>real</td>\n",
       "      <td>L868</td>\n",
       "      <td>L866</td>\n",
       "      <td>L867</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>[&lt;start&gt;, real, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>like fear wearing pastels</td>\n",
       "      <td>L869</td>\n",
       "      <td>L866</td>\n",
       "      <td>L868</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[&lt;start&gt;, like, fear, wearing, pastel, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>im kidding know sometimes become persona dont ...</td>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "      <td>None</td>\n",
       "      <td>55</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[&lt;start&gt;, im, kidding, know, sometimes, become...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text    id conversation_id  \\\n",
       "428       youre amazingly selfassured anyone ever told  L840            L834   \n",
       "427                                            go prom  L841            L834   \n",
       "426                                    request command  L842            L842   \n",
       "425                                          know mean  L843            L842   \n",
       "424                                                     L844            L842   \n",
       "423                                                     L845            L842   \n",
       "422                                            wont go  L846            L842   \n",
       "421                                                     L847            L842   \n",
       "420                         dont want stupid tradition  L848            L842   \n",
       "419                create little drama start new rumor  L852            L852   \n",
       "418                                             motive  L853            L852   \n",
       "417                                               tell  L854            L852   \n",
       "416                      need therapy anyone ever told  L855            L852   \n",
       "415                            answer question patrick  L856            L852   \n",
       "414            nothing theres nothing pleasure company  L857            L852   \n",
       "18   guillermo says go lighter youre gonna look lik...  L860            L860   \n",
       "17                                                      L861            L860   \n",
       "16                                         listen crap  L862            L862   \n",
       "15                                                crap  L863            L862   \n",
       "14                endless blonde babble im like boring  L864            L862   \n",
       "13                   thank god hear one story coiffure  L865            L862   \n",
       "12              figured youd get good stuff eventually  L866            L866   \n",
       "11                                          good stuff  L867            L866   \n",
       "10                                                real  L868            L866   \n",
       "9                            like fear wearing pastels  L869            L866   \n",
       "8    im kidding know sometimes become persona dont ...  L870            L870   \n",
       "\n",
       "    reply_to  msg_length  word_count  sentiment  \\\n",
       "428     L839          44           6   0.600000   \n",
       "427     L840           7           2   0.000000   \n",
       "426     None          15           2   0.000000   \n",
       "425     L842           9           2  -0.312500   \n",
       "424     L843           0           0   0.000000   \n",
       "423     L844           0           0   0.000000   \n",
       "422     L845           7           2   0.000000   \n",
       "421     L846           0           0   0.000000   \n",
       "420     L847          26           4  -0.800000   \n",
       "419     None          35           6  -0.025568   \n",
       "418     L852           6           1   0.000000   \n",
       "417     L853           4           1   0.000000   \n",
       "416     L854          29           5   0.000000   \n",
       "415     L855          23           3   0.000000   \n",
       "414     L856          39           5   0.000000   \n",
       "18      None          59          13   0.000000   \n",
       "17      L860           0           0   0.000000   \n",
       "16      None          11           2  -0.800000   \n",
       "15      L862           4           1  -0.800000   \n",
       "14      L863          36           6  -0.562500   \n",
       "13      L864          33           6   0.500000   \n",
       "12      None          38           6   0.700000   \n",
       "11      L866          10           2   0.700000   \n",
       "10      L867           4           1   0.200000   \n",
       "9       L868          25           4   0.000000   \n",
       "8       None          55           9   0.000000   \n",
       "\n",
       "                                                tokens  \n",
       "428  [<start>, youre, amazingly, selfassured, anyon...  \n",
       "427                         [<start>, go, prom, <end>]  \n",
       "426                 [<start>, request, command, <end>]  \n",
       "425                       [<start>, know, mean, <end>]  \n",
       "424                                   [<start>, <end>]  \n",
       "423                                   [<start>, <end>]  \n",
       "422                         [<start>, wont, go, <end>]  \n",
       "421                                   [<start>, <end>]  \n",
       "420    [<start>, dont, want, stupid, tradition, <end>]  \n",
       "419  [<start>, create, little, drama, start, new, r...  \n",
       "418                           [<start>, motive, <end>]  \n",
       "417                             [<start>, tell, <end>]  \n",
       "416  [<start>, need, therapy, anyone, ever, told, <...  \n",
       "415        [<start>, answer, question, patrick, <end>]  \n",
       "414  [<start>, nothing, there, nothing, pleasure, c...  \n",
       "18   [<start>, guillermo, say, go, lighter, youre, ...  \n",
       "17                                    [<start>, <end>]  \n",
       "16                      [<start>, listen, crap, <end>]  \n",
       "15                              [<start>, crap, <end>]  \n",
       "14   [<start>, endless, blonde, babble, im, like, b...  \n",
       "13   [<start>, thank, god, hear, one, story, coiffu...  \n",
       "12   [<start>, figured, youd, get, good, stuff, eve...  \n",
       "11                       [<start>, good, stuff, <end>]  \n",
       "10                              [<start>, real, <end>]  \n",
       "9        [<start>, like, fear, wearing, pastel, <end>]  \n",
       "8    [<start>, im, kidding, know, sometimes, become...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sorted_conversations = conversations[conversations['id'].apply(lambda x: 840 <= int(x[1:]) <= 870 if x[1:].isdigit() else False)\n",
    "].sort_values(by='id', key=lambda x: x.str.extract('(\\d+)', expand=False).astype(int))\n",
    "\n",
    "filtered_sorted_conversations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe97c54-62ce-44e6-8dc7-7e69d3ff49c9",
   "metadata": {},
   "source": [
    "id: Unique identifier for each message. <br>\n",
    "conversation_id: Identifier for the conversation to which the message belongs. All messages within the same conversation share this ID. <br>\n",
    "reply_to: ID of the message to which the current message is a response. If this is None, the message is the start of a conversation thread."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6bbc39-fb71-4d10-a010-7a81cce4a869",
   "metadata": {},
   "source": [
    "## Pairing messages - input with responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53453bd2-3a67-434d-85dc-864e58d6bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the DataFrame with itself to form pairs\n",
    "pairs = pd.merge(\n",
    "    conversations, conversations,\n",
    "    left_on='id',\n",
    "    right_on='reply_to',\n",
    "    suffixes=('_input', '_response')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07424e34-23a2-4937-88e2-e6e6be66bdf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_input</th>\n",
       "      <th>id_input</th>\n",
       "      <th>conversation_id_input</th>\n",
       "      <th>reply_to_input</th>\n",
       "      <th>msg_length_input</th>\n",
       "      <th>word_count_input</th>\n",
       "      <th>sentiment_input</th>\n",
       "      <th>tokens_input</th>\n",
       "      <th>text_response</th>\n",
       "      <th>id_response</th>\n",
       "      <th>conversation_id_response</th>\n",
       "      <th>reply_to_response</th>\n",
       "      <th>msg_length_response</th>\n",
       "      <th>word_count_response</th>\n",
       "      <th>sentiment_response</th>\n",
       "      <th>tokens_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "      <td></td>\n",
       "      <td>L1045</td>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>okay</td>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[&lt;start&gt;, okay, &lt;end&gt;]</td>\n",
       "      <td>hope</td>\n",
       "      <td>L985</td>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[&lt;start&gt;, hope, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wow</td>\n",
       "      <td>L924</td>\n",
       "      <td>L924</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[&lt;start&gt;, wow, &lt;end&gt;]</td>\n",
       "      <td>lets go</td>\n",
       "      <td>L925</td>\n",
       "      <td>L924</td>\n",
       "      <td>L924</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[&lt;start&gt;, let, go, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>L871</td>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "      <td>okay youre gonna need learn lie</td>\n",
       "      <td>L872</td>\n",
       "      <td>L870</td>\n",
       "      <td>L871</td>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[&lt;start&gt;, okay, youre, gon, na, need, learn, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im kidding know sometimes become persona dont ...</td>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "      <td>None</td>\n",
       "      <td>55</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[&lt;start&gt;, im, kidding, know, sometimes, become...</td>\n",
       "      <td></td>\n",
       "      <td>L871</td>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_input id_input  \\\n",
       "0                                                       L1044   \n",
       "1                                               okay     L984   \n",
       "2                                                wow     L924   \n",
       "3                                                        L871   \n",
       "4  im kidding know sometimes become persona dont ...     L870   \n",
       "\n",
       "  conversation_id_input reply_to_input  msg_length_input  word_count_input  \\\n",
       "0                 L1044           None                 0                 0   \n",
       "1                  L984           None                 4                 1   \n",
       "2                  L924           None                 3                 1   \n",
       "3                  L870           L870                 0                 0   \n",
       "4                  L870           None                55                 9   \n",
       "\n",
       "   sentiment_input                                       tokens_input  \\\n",
       "0              0.0                                   [<start>, <end>]   \n",
       "1              0.5                             [<start>, okay, <end>]   \n",
       "2              0.1                              [<start>, wow, <end>]   \n",
       "3              0.0                                   [<start>, <end>]   \n",
       "4              0.0  [<start>, im, kidding, know, sometimes, become...   \n",
       "\n",
       "                     text_response id_response conversation_id_response  \\\n",
       "0                                        L1045                    L1044   \n",
       "1                             hope        L985                     L984   \n",
       "2                          lets go        L925                     L924   \n",
       "3  okay youre gonna need learn lie        L872                     L870   \n",
       "4                                         L871                     L870   \n",
       "\n",
       "  reply_to_response  msg_length_response  word_count_response  \\\n",
       "0             L1044                    0                    0   \n",
       "1              L984                    4                    1   \n",
       "2              L924                    7                    2   \n",
       "3              L871                   31                    7   \n",
       "4              L870                    0                    0   \n",
       "\n",
       "   sentiment_response                                    tokens_response  \n",
       "0                 0.0                                   [<start>, <end>]  \n",
       "1                 0.0                             [<start>, hope, <end>]  \n",
       "2                 0.0                          [<start>, let, go, <end>]  \n",
       "3                 0.5  [<start>, okay, youre, gon, na, need, learn, l...  \n",
       "4                 0.0                                   [<start>, <end>]  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b68aa316-af58-4439-9895-098291abe5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Selecting the needed columns including IDs\n",
    "# training_data = pairs[['id_input', 'text_with_tokens_input', 'tokens_input', 'sentiment_input', 'id_response', 'text_with_tokens_response', 'tokens_response', 'sentiment_response']]\n",
    "\n",
    "# # Renaming columns for clarity\n",
    "# training_data.columns = ['ID_Input', 'Input', 'Tokens_Input', 'Sentiment_Input', 'ID_Response', 'Response', 'Tokens_Response', 'Sentiment_Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9ab0bfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the needed columns including IDs\n",
    "training_data = pairs[['id_input', 'text_input', 'tokens_input', 'id_response', 'text_response', 'tokens_response']]\n",
    "\n",
    "# Renaming columns for clarity\n",
    "training_data.columns = ['ID_Input', 'Input', 'Tokens_Input', 'ID_Response', 'Response', 'Tokens_Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4061571f-9fc5-4bd6-a04a-c7f27f891203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Input</th>\n",
       "      <th>Input</th>\n",
       "      <th>Tokens_Input</th>\n",
       "      <th>ID_Response</th>\n",
       "      <th>Response</th>\n",
       "      <th>Tokens_Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1044</td>\n",
       "      <td></td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "      <td>L1045</td>\n",
       "      <td></td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L984</td>\n",
       "      <td>okay</td>\n",
       "      <td>[&lt;start&gt;, okay, &lt;end&gt;]</td>\n",
       "      <td>L985</td>\n",
       "      <td>hope</td>\n",
       "      <td>[&lt;start&gt;, hope, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L924</td>\n",
       "      <td>wow</td>\n",
       "      <td>[&lt;start&gt;, wow, &lt;end&gt;]</td>\n",
       "      <td>L925</td>\n",
       "      <td>lets go</td>\n",
       "      <td>[&lt;start&gt;, let, go, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L871</td>\n",
       "      <td></td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "      <td>L872</td>\n",
       "      <td>okay youre gonna need learn lie</td>\n",
       "      <td>[&lt;start&gt;, okay, youre, gon, na, need, learn, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L870</td>\n",
       "      <td>im kidding know sometimes become persona dont ...</td>\n",
       "      <td>[&lt;start&gt;, im, kidding, know, sometimes, become...</td>\n",
       "      <td>L871</td>\n",
       "      <td></td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221611</th>\n",
       "      <td>L666520</td>\n",
       "      <td>well assure sir desire create difficulties &lt;num&gt;</td>\n",
       "      <td>[&lt;start&gt;, well, assure, sir, desire, create, d...</td>\n",
       "      <td>L666521</td>\n",
       "      <td>assure fact id obliged best advice scouts seen</td>\n",
       "      <td>[&lt;start&gt;, assure, fact, id, obliged, best, adv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221612</th>\n",
       "      <td>L666371</td>\n",
       "      <td>lord chelmsford seems want stay back basutos</td>\n",
       "      <td>[&lt;start&gt;, lord, chelmsford, seems, want, stay,...</td>\n",
       "      <td>L666372</td>\n",
       "      <td>think chelmsford wants good man border fears f...</td>\n",
       "      <td>[&lt;start&gt;, think, chelmsford, want, good, man, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221613</th>\n",
       "      <td>L666370</td>\n",
       "      <td>im take sikali main column river</td>\n",
       "      <td>[&lt;start&gt;, im, take, sikali, main, column, rive...</td>\n",
       "      <td>L666371</td>\n",
       "      <td>lord chelmsford seems want stay back basutos</td>\n",
       "      <td>[&lt;start&gt;, lord, chelmsford, seems, want, stay,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221614</th>\n",
       "      <td>L666369</td>\n",
       "      <td>orders mr vereker</td>\n",
       "      <td>[&lt;start&gt;, order, mr, vereker, &lt;end&gt;]</td>\n",
       "      <td>L666370</td>\n",
       "      <td>im take sikali main column river</td>\n",
       "      <td>[&lt;start&gt;, im, take, sikali, main, column, rive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221615</th>\n",
       "      <td>L666256</td>\n",
       "      <td>colonel durnford william vereker hear seeking ...</td>\n",
       "      <td>[&lt;start&gt;, colonel, durnford, william, vereker,...</td>\n",
       "      <td>L666257</td>\n",
       "      <td>good ones yes mr vereker gentlemen ride shoot</td>\n",
       "      <td>[&lt;start&gt;, good, one, yes, mr, vereker, gentlem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>221616 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID_Input                                              Input  \\\n",
       "0         L1044                                                      \n",
       "1          L984                                               okay   \n",
       "2          L924                                                wow   \n",
       "3          L871                                                      \n",
       "4          L870  im kidding know sometimes become persona dont ...   \n",
       "...         ...                                                ...   \n",
       "221611  L666520   well assure sir desire create difficulties <num>   \n",
       "221612  L666371       lord chelmsford seems want stay back basutos   \n",
       "221613  L666370                   im take sikali main column river   \n",
       "221614  L666369                                  orders mr vereker   \n",
       "221615  L666256  colonel durnford william vereker hear seeking ...   \n",
       "\n",
       "                                             Tokens_Input ID_Response  \\\n",
       "0                                        [<start>, <end>]       L1045   \n",
       "1                                  [<start>, okay, <end>]        L985   \n",
       "2                                   [<start>, wow, <end>]        L925   \n",
       "3                                        [<start>, <end>]        L872   \n",
       "4       [<start>, im, kidding, know, sometimes, become...        L871   \n",
       "...                                                   ...         ...   \n",
       "221611  [<start>, well, assure, sir, desire, create, d...     L666521   \n",
       "221612  [<start>, lord, chelmsford, seems, want, stay,...     L666372   \n",
       "221613  [<start>, im, take, sikali, main, column, rive...     L666371   \n",
       "221614               [<start>, order, mr, vereker, <end>]     L666370   \n",
       "221615  [<start>, colonel, durnford, william, vereker,...     L666257   \n",
       "\n",
       "                                                 Response  \\\n",
       "0                                                           \n",
       "1                                                    hope   \n",
       "2                                                 lets go   \n",
       "3                         okay youre gonna need learn lie   \n",
       "4                                                           \n",
       "...                                                   ...   \n",
       "221611     assure fact id obliged best advice scouts seen   \n",
       "221612  think chelmsford wants good man border fears f...   \n",
       "221613       lord chelmsford seems want stay back basutos   \n",
       "221614                   im take sikali main column river   \n",
       "221615      good ones yes mr vereker gentlemen ride shoot   \n",
       "\n",
       "                                          Tokens_Response  \n",
       "0                                        [<start>, <end>]  \n",
       "1                                  [<start>, hope, <end>]  \n",
       "2                               [<start>, let, go, <end>]  \n",
       "3       [<start>, okay, youre, gon, na, need, learn, l...  \n",
       "4                                        [<start>, <end>]  \n",
       "...                                                   ...  \n",
       "221611  [<start>, assure, fact, id, obliged, best, adv...  \n",
       "221612  [<start>, think, chelmsford, want, good, man, ...  \n",
       "221613  [<start>, lord, chelmsford, seems, want, stay,...  \n",
       "221614  [<start>, im, take, sikali, main, column, rive...  \n",
       "221615  [<start>, good, one, yes, mr, vereker, gentlem...  \n",
       "\n",
       "[221616 rows x 6 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c01ce521-0bac-4503-88c6-eb710c853130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221616"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking how many pairs I shall get\n",
    "len(conversations) - len(conversations.loc[:, 'conversation_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e86de81-01d7-462a-8672-5aaeceb37adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 221616 entries, 0 to 221615\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   ID_Input         221616 non-null  object\n",
      " 1   Input            221616 non-null  object\n",
      " 2   Tokens_Input     221616 non-null  object\n",
      " 3   ID_Response      221616 non-null  object\n",
      " 4   Response         221616 non-null  object\n",
      " 5   Tokens_Response  221616 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 10.1+ MB\n"
     ]
    }
   ],
   "source": [
    "training_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dafe21-5406-4d92-b940-01b55f93f13b",
   "metadata": {},
   "source": [
    "### Checking token length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16f9f405-d36f-4dfe-8c68-ce75dde0da0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Token Lengths - Statistics:\n",
      "count    221616.000000\n",
      "mean          7.212381\n",
      "std           5.866951\n",
      "min           2.000000\n",
      "25%           4.000000\n",
      "50%           5.000000\n",
      "75%           9.000000\n",
      "max         155.000000\n",
      "Name: Tokens_Input, dtype: float64\n",
      "\n",
      "Response Token Lengths - Statistics:\n",
      "count    221616.000000\n",
      "mean          7.408829\n",
      "std           6.223942\n",
      "min           2.000000\n",
      "25%           4.000000\n",
      "50%           6.000000\n",
      "75%           9.000000\n",
      "max         274.000000\n",
      "Name: Tokens_Response, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "token_lengths_input = training_data['Tokens_Input'].apply(len)\n",
    "token_lengths_response = training_data['Tokens_Response'].apply(len)\n",
    "\n",
    "print(\"Input Token Lengths - Statistics:\")\n",
    "print(token_lengths_input.describe())\n",
    "\n",
    "print(\"\\nResponse Token Lengths - Statistics:\")\n",
    "print(token_lengths_response.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa1d61c-2d1e-43b2-be8d-2885e3682584",
   "metadata": {},
   "source": [
    "## Saving the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "04767995-fd58-4626-8724-737073710d6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path_parquet = os.path.join(data_dir, 'training_data.parquet')\n",
    "training_data.to_parquet(file_path_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "384bf4a7-b962-48b1-9bb4-568b4a0ec985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Input</th>\n",
       "      <th>Input</th>\n",
       "      <th>Tokens_Input</th>\n",
       "      <th>ID_Response</th>\n",
       "      <th>Response</th>\n",
       "      <th>Tokens_Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1044</td>\n",
       "      <td></td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "      <td>L1045</td>\n",
       "      <td></td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L984</td>\n",
       "      <td>okay</td>\n",
       "      <td>[&lt;start&gt;, okay, &lt;end&gt;]</td>\n",
       "      <td>L985</td>\n",
       "      <td>hope</td>\n",
       "      <td>[&lt;start&gt;, hope, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L924</td>\n",
       "      <td>wow</td>\n",
       "      <td>[&lt;start&gt;, wow, &lt;end&gt;]</td>\n",
       "      <td>L925</td>\n",
       "      <td>lets go</td>\n",
       "      <td>[&lt;start&gt;, let, go, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L871</td>\n",
       "      <td></td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "      <td>L872</td>\n",
       "      <td>okay youre gonna need learn lie</td>\n",
       "      <td>[&lt;start&gt;, okay, youre, gon, na, need, learn, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L870</td>\n",
       "      <td>im kidding know sometimes become persona dont ...</td>\n",
       "      <td>[&lt;start&gt;, im, kidding, know, sometimes, become...</td>\n",
       "      <td>L871</td>\n",
       "      <td></td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221611</th>\n",
       "      <td>L666520</td>\n",
       "      <td>well assure sir desire create difficulties &lt;num&gt;</td>\n",
       "      <td>[&lt;start&gt;, well, assure, sir, desire, create, d...</td>\n",
       "      <td>L666521</td>\n",
       "      <td>assure fact id obliged best advice scouts seen</td>\n",
       "      <td>[&lt;start&gt;, assure, fact, id, obliged, best, adv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221612</th>\n",
       "      <td>L666371</td>\n",
       "      <td>lord chelmsford seems want stay back basutos</td>\n",
       "      <td>[&lt;start&gt;, lord, chelmsford, seems, want, stay,...</td>\n",
       "      <td>L666372</td>\n",
       "      <td>think chelmsford wants good man border fears f...</td>\n",
       "      <td>[&lt;start&gt;, think, chelmsford, want, good, man, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221613</th>\n",
       "      <td>L666370</td>\n",
       "      <td>im take sikali main column river</td>\n",
       "      <td>[&lt;start&gt;, im, take, sikali, main, column, rive...</td>\n",
       "      <td>L666371</td>\n",
       "      <td>lord chelmsford seems want stay back basutos</td>\n",
       "      <td>[&lt;start&gt;, lord, chelmsford, seems, want, stay,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221614</th>\n",
       "      <td>L666369</td>\n",
       "      <td>orders mr vereker</td>\n",
       "      <td>[&lt;start&gt;, order, mr, vereker, &lt;end&gt;]</td>\n",
       "      <td>L666370</td>\n",
       "      <td>im take sikali main column river</td>\n",
       "      <td>[&lt;start&gt;, im, take, sikali, main, column, rive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221615</th>\n",
       "      <td>L666256</td>\n",
       "      <td>colonel durnford william vereker hear seeking ...</td>\n",
       "      <td>[&lt;start&gt;, colonel, durnford, william, vereker,...</td>\n",
       "      <td>L666257</td>\n",
       "      <td>good ones yes mr vereker gentlemen ride shoot</td>\n",
       "      <td>[&lt;start&gt;, good, one, yes, mr, vereker, gentlem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>221616 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID_Input                                              Input  \\\n",
       "0         L1044                                                      \n",
       "1          L984                                               okay   \n",
       "2          L924                                                wow   \n",
       "3          L871                                                      \n",
       "4          L870  im kidding know sometimes become persona dont ...   \n",
       "...         ...                                                ...   \n",
       "221611  L666520   well assure sir desire create difficulties <num>   \n",
       "221612  L666371       lord chelmsford seems want stay back basutos   \n",
       "221613  L666370                   im take sikali main column river   \n",
       "221614  L666369                                  orders mr vereker   \n",
       "221615  L666256  colonel durnford william vereker hear seeking ...   \n",
       "\n",
       "                                             Tokens_Input ID_Response  \\\n",
       "0                                        [<start>, <end>]       L1045   \n",
       "1                                  [<start>, okay, <end>]        L985   \n",
       "2                                   [<start>, wow, <end>]        L925   \n",
       "3                                        [<start>, <end>]        L872   \n",
       "4       [<start>, im, kidding, know, sometimes, become...        L871   \n",
       "...                                                   ...         ...   \n",
       "221611  [<start>, well, assure, sir, desire, create, d...     L666521   \n",
       "221612  [<start>, lord, chelmsford, seems, want, stay,...     L666372   \n",
       "221613  [<start>, im, take, sikali, main, column, rive...     L666371   \n",
       "221614               [<start>, order, mr, vereker, <end>]     L666370   \n",
       "221615  [<start>, colonel, durnford, william, vereker,...     L666257   \n",
       "\n",
       "                                                 Response  \\\n",
       "0                                                           \n",
       "1                                                    hope   \n",
       "2                                                 lets go   \n",
       "3                         okay youre gonna need learn lie   \n",
       "4                                                           \n",
       "...                                                   ...   \n",
       "221611     assure fact id obliged best advice scouts seen   \n",
       "221612  think chelmsford wants good man border fears f...   \n",
       "221613       lord chelmsford seems want stay back basutos   \n",
       "221614                   im take sikali main column river   \n",
       "221615      good ones yes mr vereker gentlemen ride shoot   \n",
       "\n",
       "                                          Tokens_Response  \n",
       "0                                        [<start>, <end>]  \n",
       "1                                  [<start>, hope, <end>]  \n",
       "2                               [<start>, let, go, <end>]  \n",
       "3       [<start>, okay, youre, gon, na, need, learn, l...  \n",
       "4                                        [<start>, <end>]  \n",
       "...                                                   ...  \n",
       "221611  [<start>, assure, fact, id, obliged, best, adv...  \n",
       "221612  [<start>, think, chelmsford, want, good, man, ...  \n",
       "221613  [<start>, lord, chelmsford, seems, want, stay,...  \n",
       "221614  [<start>, im, take, sikali, main, column, rive...  \n",
       "221615  [<start>, good, one, yes, mr, vereker, gentlem...  \n",
       "\n",
       "[221616 rows x 6 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
