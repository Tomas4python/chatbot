{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce3a8e4-4a95-41ab-a269-c4c219626237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69f2edba-be1f-46f6-854e-c5e566fdda65",
   "metadata": {},
   "source": [
    "## Loading the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6976721-d931-48af-a675-a13840b84494",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Input</th>\n",
       "      <th>Input</th>\n",
       "      <th>Tokens_Input</th>\n",
       "      <th>ID_Response</th>\n",
       "      <th>Response</th>\n",
       "      <th>Tokens_Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1044</td>\n",
       "      <td></td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "      <td>L1045</td>\n",
       "      <td></td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L984</td>\n",
       "      <td>okay</td>\n",
       "      <td>[&lt;start&gt;, okay, &lt;end&gt;]</td>\n",
       "      <td>L985</td>\n",
       "      <td>hope</td>\n",
       "      <td>[&lt;start&gt;, hope, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L924</td>\n",
       "      <td>wow</td>\n",
       "      <td>[&lt;start&gt;, wow, &lt;end&gt;]</td>\n",
       "      <td>L925</td>\n",
       "      <td>lets go</td>\n",
       "      <td>[&lt;start&gt;, let, go, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L871</td>\n",
       "      <td></td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "      <td>L872</td>\n",
       "      <td>okay youre gonna need learn lie</td>\n",
       "      <td>[&lt;start&gt;, okay, youre, gon, na, need, learn, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L870</td>\n",
       "      <td>im kidding know sometimes become persona dont ...</td>\n",
       "      <td>[&lt;start&gt;, im, kidding, know, sometimes, become...</td>\n",
       "      <td>L871</td>\n",
       "      <td></td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>L868</td>\n",
       "      <td>real</td>\n",
       "      <td>[&lt;start&gt;, real, &lt;end&gt;]</td>\n",
       "      <td>L869</td>\n",
       "      <td>like fear wearing pastels</td>\n",
       "      <td>[&lt;start&gt;, like, fear, wearing, pastel, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>L867</td>\n",
       "      <td>good stuff</td>\n",
       "      <td>[&lt;start&gt;, good, stuff, &lt;end&gt;]</td>\n",
       "      <td>L868</td>\n",
       "      <td>real</td>\n",
       "      <td>[&lt;start&gt;, real, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>L866</td>\n",
       "      <td>figured youd get good stuff eventually</td>\n",
       "      <td>[&lt;start&gt;, figured, youd, get, good, stuff, eve...</td>\n",
       "      <td>L867</td>\n",
       "      <td>good stuff</td>\n",
       "      <td>[&lt;start&gt;, good, stuff, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>L864</td>\n",
       "      <td>endless blonde babble im like boring</td>\n",
       "      <td>[&lt;start&gt;, endless, blonde, babble, im, like, b...</td>\n",
       "      <td>L865</td>\n",
       "      <td>thank god hear one story coiffure</td>\n",
       "      <td>[&lt;start&gt;, thank, god, hear, one, story, coiffu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>L863</td>\n",
       "      <td>crap</td>\n",
       "      <td>[&lt;start&gt;, crap, &lt;end&gt;]</td>\n",
       "      <td>L864</td>\n",
       "      <td>endless blonde babble im like boring</td>\n",
       "      <td>[&lt;start&gt;, endless, blonde, babble, im, like, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>L862</td>\n",
       "      <td>listen crap</td>\n",
       "      <td>[&lt;start&gt;, listen, crap, &lt;end&gt;]</td>\n",
       "      <td>L863</td>\n",
       "      <td>crap</td>\n",
       "      <td>[&lt;start&gt;, crap, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>L860</td>\n",
       "      <td>guillermo says go lighter youre gonna look lik...</td>\n",
       "      <td>[&lt;start&gt;, guillermo, say, go, lighter, youre, ...</td>\n",
       "      <td>L861</td>\n",
       "      <td></td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>L698</td>\n",
       "      <td></td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "      <td>L699</td>\n",
       "      <td>always selfish</td>\n",
       "      <td>[&lt;start&gt;, always, selfish, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>L697</td>\n",
       "      <td>thats say</td>\n",
       "      <td>[&lt;start&gt;, thats, say, &lt;end&gt;]</td>\n",
       "      <td>L698</td>\n",
       "      <td></td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>L696</td>\n",
       "      <td>well</td>\n",
       "      <td>[&lt;start&gt;, well, &lt;end&gt;]</td>\n",
       "      <td>L697</td>\n",
       "      <td>thats say</td>\n",
       "      <td>[&lt;start&gt;, thats, say, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>L694</td>\n",
       "      <td></td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "      <td>L695</td>\n",
       "      <td>never wanted go</td>\n",
       "      <td>[&lt;start&gt;, never, wanted, go, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>L693</td>\n",
       "      <td>looked back party always seemed occupied</td>\n",
       "      <td>[&lt;start&gt;, looked, back, party, always, seemed,...</td>\n",
       "      <td>L694</td>\n",
       "      <td></td>\n",
       "      <td>[&lt;start&gt;, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>L662</td>\n",
       "      <td>fun tonight</td>\n",
       "      <td>[&lt;start&gt;, fun, tonight, &lt;end&gt;]</td>\n",
       "      <td>L663</td>\n",
       "      <td>tons</td>\n",
       "      <td>[&lt;start&gt;, ton, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>L577</td>\n",
       "      <td>know chastity</td>\n",
       "      <td>[&lt;start&gt;, know, chastity, &lt;end&gt;]</td>\n",
       "      <td>L578</td>\n",
       "      <td>believe share art instructor</td>\n",
       "      <td>[&lt;start&gt;, believe, share, art, instructor, &lt;end&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>L575</td>\n",
       "      <td>hi</td>\n",
       "      <td>[&lt;start&gt;, hi, &lt;end&gt;]</td>\n",
       "      <td>L576</td>\n",
       "      <td>looks like things worked tonight huh</td>\n",
       "      <td>[&lt;start&gt;, look, like, thing, worked, tonight, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_Input                                              Input  \\\n",
       "0     L1044                                                      \n",
       "1      L984                                               okay   \n",
       "2      L924                                                wow   \n",
       "3      L871                                                      \n",
       "4      L870  im kidding know sometimes become persona dont ...   \n",
       "5      L868                                               real   \n",
       "6      L867                                         good stuff   \n",
       "7      L866             figured youd get good stuff eventually   \n",
       "8      L864               endless blonde babble im like boring   \n",
       "9      L863                                               crap   \n",
       "10     L862                                        listen crap   \n",
       "11     L860  guillermo says go lighter youre gonna look lik...   \n",
       "12     L698                                                      \n",
       "13     L697                                          thats say   \n",
       "14     L696                                               well   \n",
       "15     L694                                                      \n",
       "16     L693           looked back party always seemed occupied   \n",
       "17     L662                                        fun tonight   \n",
       "18     L577                                      know chastity   \n",
       "19     L575                                                 hi   \n",
       "\n",
       "                                         Tokens_Input ID_Response  \\\n",
       "0                                    [<start>, <end>]       L1045   \n",
       "1                              [<start>, okay, <end>]        L985   \n",
       "2                               [<start>, wow, <end>]        L925   \n",
       "3                                    [<start>, <end>]        L872   \n",
       "4   [<start>, im, kidding, know, sometimes, become...        L871   \n",
       "5                              [<start>, real, <end>]        L869   \n",
       "6                       [<start>, good, stuff, <end>]        L868   \n",
       "7   [<start>, figured, youd, get, good, stuff, eve...        L867   \n",
       "8   [<start>, endless, blonde, babble, im, like, b...        L865   \n",
       "9                              [<start>, crap, <end>]        L864   \n",
       "10                     [<start>, listen, crap, <end>]        L863   \n",
       "11  [<start>, guillermo, say, go, lighter, youre, ...        L861   \n",
       "12                                   [<start>, <end>]        L699   \n",
       "13                       [<start>, thats, say, <end>]        L698   \n",
       "14                             [<start>, well, <end>]        L697   \n",
       "15                                   [<start>, <end>]        L695   \n",
       "16  [<start>, looked, back, party, always, seemed,...        L694   \n",
       "17                     [<start>, fun, tonight, <end>]        L663   \n",
       "18                   [<start>, know, chastity, <end>]        L578   \n",
       "19                               [<start>, hi, <end>]        L576   \n",
       "\n",
       "                                Response  \\\n",
       "0                                          \n",
       "1                                   hope   \n",
       "2                                lets go   \n",
       "3        okay youre gonna need learn lie   \n",
       "4                                          \n",
       "5              like fear wearing pastels   \n",
       "6                                   real   \n",
       "7                             good stuff   \n",
       "8      thank god hear one story coiffure   \n",
       "9   endless blonde babble im like boring   \n",
       "10                                  crap   \n",
       "11                                         \n",
       "12                        always selfish   \n",
       "13                                         \n",
       "14                             thats say   \n",
       "15                       never wanted go   \n",
       "16                                         \n",
       "17                                  tons   \n",
       "18          believe share art instructor   \n",
       "19  looks like things worked tonight huh   \n",
       "\n",
       "                                      Tokens_Response  \n",
       "0                                    [<start>, <end>]  \n",
       "1                              [<start>, hope, <end>]  \n",
       "2                           [<start>, let, go, <end>]  \n",
       "3   [<start>, okay, youre, gon, na, need, learn, l...  \n",
       "4                                    [<start>, <end>]  \n",
       "5       [<start>, like, fear, wearing, pastel, <end>]  \n",
       "6                              [<start>, real, <end>]  \n",
       "7                       [<start>, good, stuff, <end>]  \n",
       "8   [<start>, thank, god, hear, one, story, coiffu...  \n",
       "9   [<start>, endless, blonde, babble, im, like, b...  \n",
       "10                             [<start>, crap, <end>]  \n",
       "11                                   [<start>, <end>]  \n",
       "12                  [<start>, always, selfish, <end>]  \n",
       "13                                   [<start>, <end>]  \n",
       "14                       [<start>, thats, say, <end>]  \n",
       "15                [<start>, never, wanted, go, <end>]  \n",
       "16                                   [<start>, <end>]  \n",
       "17                              [<start>, ton, <end>]  \n",
       "18  [<start>, believe, share, art, instructor, <end>]  \n",
       "19  [<start>, look, like, thing, worked, tonight, ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "file_path_parquet = os.path.join(data_dir, 'training_data.parquet')\n",
    "data = pd.read_parquet(file_path_parquet)\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f242e4fc-badb-4d3d-9bb5-26312dc42246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Name: /device:GPU:0\n",
      "Memory Limit: 4158652416 bytes\n",
      "Description: device: 0, name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "import tensorflow\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_gpu_details():\n",
    "    devices = device_lib.list_local_devices()\n",
    "    for device in devices:\n",
    "        if device.device_type == 'GPU':\n",
    "            print(f\"Device Name: {device.name}\")\n",
    "            print(f\"Memory Limit: {device.memory_limit} bytes\")\n",
    "            print(f\"Description: {device.physical_device_desc}\")\n",
    "\n",
    "get_gpu_details()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dab305d-0f5f-457e-b1f2-138e10a497d9",
   "metadata": {},
   "source": [
    "## Encoder-decoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "549e3ae5-8f52-4a5d-9778-8ab1961cc83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d7fc7f-9fdc-4221-82d9-3576d7cf99c1",
   "metadata": {},
   "source": [
    "### Work on sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5a1883-8b33-45e9-adb6-d38af7e0e557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize tokenizer and fit on texts\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(['<start> <end>'] + data['Input'] + data['Response'])\n",
    "\n",
    "# # Adding start and end tokens to each input and response\n",
    "# data['Input'] = ['<start> ' + text + ' <end>' for text in data['Input']]\n",
    "# data['Response'] = ['<start> ' + text + ' <end>' for text in data['Response']]\n",
    "\n",
    "# # Convert text to sequences\n",
    "# input_sequences = tokenizer.texts_to_sequences(data['Input'])\n",
    "# target_sequences = tokenizer.texts_to_sequences(data['Response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43c29a9-9a2a-4694-b00d-2f2ddc0d5f2c",
   "metadata": {},
   "source": [
    "### Work with lemmatized tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5322e7ba-b4fb-46f9-ab43-9b9d3913c789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is already loaded with 'Input' and 'Response' columns\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['Tokens_Input'].tolist() + data['Tokens_Response'].tolist())\n",
    "\n",
    "# Convert text to sequences\n",
    "input_sequences = data['Tokens_Input']\n",
    "target_sequences = data['Tokens_Response']\n",
    "\n",
    "max_length = 30\n",
    "\n",
    "# Pad both input and target sequences to max_length\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_length, padding='post')\n",
    "target_sequences = pad_sequences(target_sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7031845c-975e-46bd-ab6e-3d5ffbcd539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_sequences[88])\n",
    "print(target_sequences[88])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ffd72-e051-413b-bea5-b38b7575f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and validation sets\n",
    "input_train, input_val, target_train, target_val = train_test_split(input_sequences, target_sequences, test_size=0.1, random_state=22)\n",
    "\n",
    "# Building the model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding = Embedding(vocab_size, 128, mask_zero=True)(encoder_inputs)\n",
    "encoder_outputs, state_h, state_c = LSTM(256, return_state=True)(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(vocab_size, 128, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=False)\n",
    "decoder_outputs = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "output = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], output)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68b53dc-7a5a-4964-bf1d-20f91dd438ff",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6d6709-2165-4e72-a232-9b48fae889db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare decoder input data that just contains the start token\n",
    "decoder_input_train = np.hstack([np.zeros((target_train.shape[0], 1)), target_train[:, :-1]])  # shift target sequences\n",
    "decoder_input_val = np.hstack([np.zeros((target_val.shape[0], 1)), target_val[:, :-1]])\n",
    "\n",
    "# Fit model\n",
    "model.fit([input_train, decoder_input_train], np.expand_dims(target_train, -1),\n",
    "          validation_data=([input_val, decoder_input_val], np.expand_dims(target_val, -1)),\n",
    "          epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e424c-a43e-4bbe-b300-7a6ef09766e1",
   "metadata": {},
   "source": [
    "## Generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040370f8-814f-4dcd-af13-fe2496f7738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')  # Tokenizer\n",
    "nltk.download('wordnet')  # Lemmatizer\n",
    "nltk.download('stopwords')  # Stopwords\n",
    "\n",
    "# Stopwords list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"[^a-z0-9.',!? ]\", ' ', text)\n",
    "    text = re.sub(r'\\d+', '<num>', text)\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # Normalize text\n",
    "    text = normalize_text(text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stopwords\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Function to generate responses\n",
    "def generate_response(input_text: str) -> str:\n",
    "    # Preprocess the input text\n",
    "    processed_text = preprocess_text(input_text)\n",
    "    processed_text = '<start> ' + processed_text + ' <end>'\n",
    "    \n",
    "    # Tokenize and Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(processed_text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Convert to sequence\n",
    "    input_seq = tokenizer.texts_to_sequences([words])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "    # Initialize the state of the decoder using the encoder's output\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Start the sequence with the `<start>` token\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = tokenizer.word_index['<start>']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = tokenizer.index_word.get(sampled_token_index, '')  # Fallback to empty if unknown\n",
    "        \n",
    "        # Append sampled token to the decoded sentence\n",
    "        if sampled_char != '<end>':\n",
    "            decoded_sentence += ' ' + sampled_char\n",
    "        \n",
    "        # Exit condition: either hit max length or find stop token.\n",
    "        if sampled_char == '<end>' or len(decoded_sentence) > max_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence to the newly predicted token\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update the states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ace37-59f9-4cfe-a040-30ef022e5168",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14813b8f-8f01-49dc-ae12-5e7bd9be9460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "print(\"User: Is she okay?\")\n",
    "print(\"Bot:\", generate_response('she okay?'))\n",
    "\n",
    "print(\"User: How are you feeling today?\")\n",
    "print(\"Bot:\", generate_response('How are you feeling today?'))\n",
    "\n",
    "print(\"User: Hi there!\")\n",
    "print(\"Bot:\", generate_response('Hi there!'))\n",
    "\n",
    "print(\"User: Can you tell me the weather forecast for today?\")\n",
    "print(\"Bot:\", generate_response('Can you tell me the weather forecast for today?'))\n",
    "\n",
    "print(\"User: I think artificial intelligence is changing the world.\")\n",
    "print(\"Bot:\", generate_response('I think artificial intelligence is changing the world.'))\n",
    "\n",
    "print(\"User: Any good movie recommendations?\")\n",
    "print(\"Bot:\", generate_response('Any good movie recommendations?'))\n",
    "\n",
    "print(\"User: What do you mean by that?\")\n",
    "print(\"Bot:\", generate_response('What do you mean by that?'))\n",
    "\n",
    "print(\"User: I'm feeling really sad today.\")\n",
    "print(\"Bot:\", generate_response(\"I'm feeling really sad today.\"))\n",
    "\n",
    "print(\"User: What are the implications of quantum computing on cybersecurity?\")\n",
    "print(\"Bot:\", generate_response('What are the implications of quantum computing on cybersecurity?'))\n",
    "\n",
    "print(\"User: Why did the chicken cross the road?\")\n",
    "print(\"Bot:\", generate_response('Why did the chicken cross the road?'))\n",
    "\n",
    "print(\"User: Can you explain the plot of The Matrix?\")\n",
    "print(\"Bot:\", generate_response('Can you explain the plot of The Matrix?'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
