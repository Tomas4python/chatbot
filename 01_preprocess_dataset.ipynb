{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f0fb967-b515-4054-9c06-c75938fa7c9f",
   "metadata": {},
   "source": [
    "# CAPSTONE PROJECT - CHATBOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c593d8-dfc1-420a-8f40-7947c049dc69",
   "metadata": {},
   "source": [
    "## DATA\n",
    "### Data sources\n",
    "https://convokit.cornell.edu/documentation/movie.html <br>\n",
    "https://www.cs.cornell.edu/~cristian/Chameleons_in_imagined_conversations.html "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1328c9a6-4d8e-497b-b764-a9ab3eb9f243",
   "metadata": {},
   "source": [
    "### Install ConvoKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d5d481c-79d5-45d7-8883-aa169633c231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install convokit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a957699a-a417-4860-aee7-581bce869888",
   "metadata": {},
   "source": [
    "### Load data from source and save to 'data' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d1157ce-ff4a-440c-bfa4-7d6f0261e902",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading movie-corpus to C:\\Users\\tomui\\Desktop\\capstone_project\\data\\movie-corpus\n",
      "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done\n"
     ]
    }
   ],
   "source": [
    "# from convokit import Corpus, download\n",
    "# import os\n",
    "\n",
    "# # Directory where to save the corpus\n",
    "# data_dir = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "# # Ensure the directory exists\n",
    "# if not os.path.exists(data_dir):\n",
    "#     os.makedirs(data_dir)\n",
    "\n",
    "# # Downloading and saving the corpus\n",
    "# corpus = Corpus(filename=download(\"movie-corpus\", data_dir=data_dir))\n",
    "\n",
    "# # Saving the corpus to the 'data' folder\n",
    "# corpus_path = os.path.join(data_dir, \"movie_corpus\")\n",
    "# corpus.dump(corpus_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9de684-120b-478a-8e60-345a3b392703",
   "metadata": {
    "tags": []
   },
   "source": [
    "Downloading movie-corpus to C:\\Users\\tomui\\Desktop\\capstone_project\\data\\movie-corpus  \r\n",
    "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0b8ca3-4757-49f0-826a-7a0a3824824a",
   "metadata": {},
   "source": [
    "### Load data from 'data' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8169d5a6-b021-4752-81ee-ed2f81b322e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Speakers: 9035\n",
      "Number of Utterances: 304713\n",
      "Number of Conversations: 83097\n"
     ]
    }
   ],
   "source": [
    "from convokit import Corpus\n",
    "import os\n",
    "\n",
    "# Directory where to load the corpus\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "# Load the corpus from the specified folder\n",
    "loaded_corpus = Corpus(filename=os.path.join(data_dir, \"movie_corpus\"))\n",
    "loaded_corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee27d1da-9ff5-468e-824b-6a02727e1535",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convokit.model.corpus.Corpus"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loaded_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568a5418-4b10-482f-9c59-685028b784df",
   "metadata": {},
   "source": [
    "### Data Structure and Organization\n",
    "```plaintext\n",
    "data/\n",
    "└── movie_corpus/\n",
    "    ├── conversations.json\n",
    "    ├── corpus.json\n",
    "    ├── index.json\n",
    "    ├── speakers.json\n",
    "    └── utterances.jsonl\n",
    "```\n",
    "\n",
    "Description [here](https://convokit.cornell.edu/documentation/movie.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92813c-7f90-4b46-bc21-9e814d5d82bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Choice for exploration\n",
    "The files I need from ConvoKit corpus for my chatbot project depend on the specific functionalities I want to implement in my chatbot. I'll most likely need `utterances.json` because it contains the dialogue data. This is what I'll use to train chatbot to understand and generate human-like responses.\n",
    "\n",
    "Description from source:  \n",
    "> \"Utterance-level information <br>\n",
    "> For each utterance, we provide:\n",
    "> - id: index of the utterance\n",
    "> - speaker: the speaker who authored the utterance\n",
    "> - conversation_id: id of the first utterance in the conversation this utterance belongs to\n",
    "> - reply_to: id of the utterance to which this utterance replies to (None if the utterance is not a reply)\n",
    "> - timestamp: time of the utterance\n",
    "> - text: textual content of the utterance\n",
    "> \n",
    "> Metadata for utterances include:\n",
    "> - movie_idx: index of the movie from which this utterance occurs\n",
    "> - parsed: parsed version of the utterance text, represented as a SpaCy Doc\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55bf77c-945b-4e5e-a56d-f8186c4d023c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Understanding data from `utterances.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d6acc31-6281-4a4c-8f06-40673e42bf61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint as pp\n",
    "\n",
    "# Initialize a list to hold all the utterances\n",
    "utterances = []\n",
    "\n",
    "# Open the file and read line by line\n",
    "with open(os.path.join(data_dir, 'movie_corpus', 'utterances.jsonl'), 'r') as file:\n",
    "    \n",
    "    for line in file:\n",
    "        utterance = json.loads(line)\n",
    "        utterances.append(utterance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c26682ea-4ebb-4705-9933-b91553312d2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(utterances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aac7a307-5edf-4bd6-a360-6def6f7523cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 304713 lines\n",
      "\n",
      "[{'conversation_id': 'L1044',\n",
      "  'id': 'L1045',\n",
      "  'meta': {'movie_id': 'm0',\n",
      "           'parsed': [{'rt': 1,\n",
      "                       'toks': [{'dep': 'nsubj',\n",
      "                                 'dn': [],\n",
      "                                 'tag': 'PRP',\n",
      "                                 'tok': 'They',\n",
      "                                 'up': 1},\n",
      "                                {'dep': 'ROOT',\n",
      "                                 'dn': [0, 2, 3],\n",
      "                                 'tag': 'VBP',\n",
      "                                 'tok': 'do'},\n",
      "                                {'dep': 'neg',\n",
      "                                 'dn': [],\n",
      "                                 'tag': 'RB',\n",
      "                                 'tok': 'not',\n",
      "                                 'up': 1},\n",
      "                                {'dep': 'punct',\n",
      "                                 'dn': [],\n",
      "                                 'tag': '.',\n",
      "                                 'tok': '!',\n",
      "                                 'up': 1}]}]},\n",
      "  'reply-to': 'L1044',\n",
      "  'speaker': 'u0',\n",
      "  'text': 'They do not!',\n",
      "  'timestamp': None,\n",
      "  'vectors': []},\n",
      " {'conversation_id': 'L1044',\n",
      "  'id': 'L1044',\n",
      "  'meta': {'movie_id': 'm0',\n",
      "           'parsed': [{'rt': 1,\n",
      "                       'toks': [{'dep': 'nsubj',\n",
      "                                 'dn': [],\n",
      "                                 'tag': 'PRP',\n",
      "                                 'tok': 'They',\n",
      "                                 'up': 1},\n",
      "                                {'dep': 'ROOT',\n",
      "                                 'dn': [0, 2, 3],\n",
      "                                 'tag': 'VBP',\n",
      "                                 'tok': 'do'},\n",
      "                                {'dep': 'dobj',\n",
      "                                 'dn': [],\n",
      "                                 'tag': 'TO',\n",
      "                                 'tok': 'to',\n",
      "                                 'up': 1},\n",
      "                                {'dep': 'punct',\n",
      "                                 'dn': [],\n",
      "                                 'tag': '.',\n",
      "                                 'tok': '!',\n",
      "                                 'up': 1}]}]},\n",
      "  'reply-to': None,\n",
      "  'speaker': 'u2',\n",
      "  'text': 'They do to!',\n",
      "  'timestamp': None,\n",
      "  'vectors': []},\n",
      " {'conversation_id': 'L984',\n",
      "  'id': 'L985',\n",
      "  'meta': {'movie_id': 'm0',\n",
      "           'parsed': [{'rt': 1,\n",
      "                       'toks': [{'dep': 'nsubj',\n",
      "                                 'dn': [],\n",
      "                                 'tag': 'PRP',\n",
      "                                 'tok': 'I',\n",
      "                                 'up': 1},\n",
      "                                {'dep': 'ROOT',\n",
      "                                 'dn': [0, 2, 3],\n",
      "                                 'tag': 'VBP',\n",
      "                                 'tok': 'hope'},\n",
      "                                {'dep': 'advmod',\n",
      "                                 'dn': [],\n",
      "                                 'tag': 'RB',\n",
      "                                 'tok': 'so',\n",
      "                                 'up': 1},\n",
      "                                {'dep': 'punct',\n",
      "                                 'dn': [],\n",
      "                                 'tag': '.',\n",
      "                                 'tok': '.',\n",
      "                                 'up': 1}]}]},\n",
      "  'reply-to': 'L984',\n",
      "  'speaker': 'u0',\n",
      "  'text': 'I hope so.',\n",
      "  'timestamp': None,\n",
      "  'vectors': []}]\n"
     ]
    }
   ],
   "source": [
    "print(f'There are a total of {len(utterances)} lines\\n')\n",
    "pp(utterances[: 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eac7ca7-8b69-4c5c-abd3-204b45d827bd",
   "metadata": {},
   "source": [
    "### Understanding data from other dataset json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65a6a94c-4467-4488-a2cb-8de7840acc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# with open(os.path.join(data_dir, 'movie_corpus', 'conversations.json'), 'r') as file:\n",
    "#     conversations = json.load(file)\n",
    "# with open(os.path.join(data_dir, 'movie_corpus', 'corpus.json'), 'r') as file:\n",
    "#     conversations = json.load(file)\n",
    "# with open(os.path.join(data_dir, 'movie_corpus', 'index.json'), 'r') as file:\n",
    "#     conversations = json.load(file)\n",
    "# with open(os.path.join(data_dir, 'movie_corpus', 'speakers.json'), 'r') as file:\n",
    "#     conversations = json.load(file)\n",
    "\n",
    "# print(type(conversations))\n",
    "\n",
    "# print(f'There are a total of {len(conversations)} keys in the dictionary\\n')\n",
    "# first_three_items = list(conversations.items())[:3]\n",
    "# pp(first_three_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55568df2-4bb1-4526-907a-455e217fe2d4",
   "metadata": {},
   "source": [
    "## Decision regarding data\n",
    "\n",
    "In developing the chatbot I made the decision to collect only data from the utterances.json file to ensure the chatbot can effectively manage and understand multi-turn conversations. The essential data elements to be gathered include `'text'` for generating responses, `'conversation_id'` for tracking the flow of conversations, and `'reply_to'` for understanding response sequences within the dialogue. While initially, the chatbot will not utilize complex NLP features like parsed linguistic data, the architecture will allow for the integration of these advanced features in the future. While initially I will collect `'parsed'` and `'toks'` information from the utterances.json file, the decision on whether to use this pre-parsed data directly, generate similar data anew, or conduct comparisons between the two will be made later as the project evolves. This approach ensures flexibility in utilizing advanced NLP features as required, maintaining the adaptability of the architecture for future enhancements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a7d75-7c31-4ebc-ae31-904ad911e3a6",
   "metadata": {},
   "source": [
    "## Converting utterances data to DataFrame\n",
    "Pandas provides a powerful and easy-to-use interface for data manipulation, filtering, transformation, and analysis, and integration with Python Ecosystem: seamless integration with other Python libraries for data analysis, machine learning (e.g., scikit-learn, TensorFlow), and visualization (e.g., Matplotlib, Seaborn), as well fast processing for datasets that fit comfortably in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5336f550-8d49-4abe-86e0-4517a993cf71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Flatten the data\n",
    "def flatten_data(data):\n",
    "    flattened_data = []\n",
    "    for entry in data:\n",
    "        flat_entry = {\n",
    "            'id': entry['id'],\n",
    "            'conversation_id': entry['conversation_id'],\n",
    "            'text': entry['text'],\n",
    "            'speaker': entry['speaker'],\n",
    "            'reply_to': entry.get('reply-to'),\n",
    "            'timestamp': entry['timestamp'],\n",
    "            'movie_id': entry['meta']['movie_id'],\n",
    "        }\n",
    "        # Handle nested parsed data\n",
    "        for parsed in entry['meta']['parsed']:\n",
    "            for idx, tok in enumerate(parsed['toks']):\n",
    "                flat_entry[f'tok_{idx}_token'] = tok['tok']\n",
    "                flat_entry[f'tok_{idx}_tag'] = tok['tag']\n",
    "                flat_entry[f'tok_{idx}_dep'] = tok['dep']\n",
    "                # Add other fields from tokens as needed\n",
    "        flattened_data.append(flat_entry)\n",
    "    return flattened_data\n",
    "\n",
    "# Convert to DataFrame\n",
    "flattened_data = flatten_data(utterances)\n",
    "df = pd.DataFrame(flattened_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08e5e09e-d6e0-45d9-b9d8-767c441470be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>tok_0_token</th>\n",
       "      <th>tok_0_tag</th>\n",
       "      <th>tok_0_dep</th>\n",
       "      <th>...</th>\n",
       "      <th>tok_121_dep</th>\n",
       "      <th>tok_122_token</th>\n",
       "      <th>tok_122_tag</th>\n",
       "      <th>tok_122_dep</th>\n",
       "      <th>tok_123_token</th>\n",
       "      <th>tok_123_tag</th>\n",
       "      <th>tok_123_dep</th>\n",
       "      <th>tok_124_token</th>\n",
       "      <th>tok_124_tag</th>\n",
       "      <th>tok_124_dep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1045</td>\n",
       "      <td>L1044</td>\n",
       "      <td>They do not!</td>\n",
       "      <td>u0</td>\n",
       "      <td>L1044</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>They</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "      <td>They do to!</td>\n",
       "      <td>u2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>They</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L985</td>\n",
       "      <td>L984</td>\n",
       "      <td>I hope so.</td>\n",
       "      <td>u0</td>\n",
       "      <td>L984</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "      <td>She okay?</td>\n",
       "      <td>u2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>She</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L925</td>\n",
       "      <td>L924</td>\n",
       "      <td>Let's go.</td>\n",
       "      <td>u0</td>\n",
       "      <td>L924</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>Let</td>\n",
       "      <td>VB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 382 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id conversation_id          text speaker reply_to timestamp movie_id  \\\n",
       "0  L1045           L1044  They do not!      u0    L1044      None       m0   \n",
       "1  L1044           L1044   They do to!      u2     None      None       m0   \n",
       "2   L985            L984    I hope so.      u0     L984      None       m0   \n",
       "3   L984            L984     She okay?      u2     None      None       m0   \n",
       "4   L925            L924     Let's go.      u0     L924      None       m0   \n",
       "\n",
       "  tok_0_token tok_0_tag tok_0_dep  ... tok_121_dep tok_122_token tok_122_tag  \\\n",
       "0        They       PRP     nsubj  ...         NaN           NaN         NaN   \n",
       "1        They       PRP     nsubj  ...         NaN           NaN         NaN   \n",
       "2           I       PRP     nsubj  ...         NaN           NaN         NaN   \n",
       "3         She       PRP     nsubj  ...         NaN           NaN         NaN   \n",
       "4         Let        VB      ROOT  ...         NaN           NaN         NaN   \n",
       "\n",
       "  tok_122_dep tok_123_token tok_123_tag tok_123_dep tok_124_token tok_124_tag  \\\n",
       "0         NaN           NaN         NaN         NaN           NaN         NaN   \n",
       "1         NaN           NaN         NaN         NaN           NaN         NaN   \n",
       "2         NaN           NaN         NaN         NaN           NaN         NaN   \n",
       "3         NaN           NaN         NaN         NaN           NaN         NaN   \n",
       "4         NaN           NaN         NaN         NaN           NaN         NaN   \n",
       "\n",
       "  tok_124_dep  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  \n",
       "\n",
       "[5 rows x 382 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show DataFrame to check structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2845afc3-b7dc-403a-ade7-79a83c710e00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 304713 entries, 0 to 304712\n",
      "Columns: 382 entries, id to tok_124_dep\n",
      "dtypes: object(382)\n",
      "memory usage: 888.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37724f59-ec6d-4493-b7eb-ec280663b221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                      0\n",
      "conversation_id         0\n",
      "text                    0\n",
      "speaker                 0\n",
      "reply_to            83097\n",
      "timestamp          304713\n",
      "movie_id                0\n",
      "tok_0_token           267\n",
      "tok_0_tag             267\n",
      "tok_0_dep             267\n",
      "tok_1_token           625\n",
      "tok_1_tag             625\n",
      "tok_1_dep             625\n",
      "tok_2_token         21729\n",
      "tok_2_tag           21729\n",
      "tok_2_dep           21729\n",
      "tok_3_token         38515\n",
      "tok_3_tag           38515\n",
      "tok_3_dep           38515\n",
      "tok_4_token         63316\n",
      "tok_4_tag           63316\n",
      "tok_4_dep           63316\n",
      "tok_5_token         92404\n",
      "tok_5_tag           92404\n",
      "tok_5_dep           92404\n",
      "tok_6_token        121962\n",
      "tok_6_tag          121962\n",
      "tok_6_dep          121962\n",
      "tok_7_token        149962\n",
      "tok_7_tag          149962\n",
      "tok_7_dep          149962\n",
      "tok_8_token        175281\n",
      "tok_8_tag          175281\n",
      "tok_8_dep          175281\n",
      "tok_9_token        196627\n",
      "tok_9_tag          196627\n",
      "tok_9_dep          196627\n",
      "tok_10_token       214433\n",
      "tok_10_tag         214433\n",
      "tok_10_dep         214433\n",
      "tok_11_token       229294\n",
      "tok_11_tag         229294\n",
      "tok_11_dep         229294\n",
      "tok_12_token       241453\n",
      "tok_12_tag         241453\n",
      "tok_12_dep         241453\n",
      "tok_13_token       251574\n",
      "tok_13_tag         251574\n",
      "tok_13_dep         251574\n",
      "tok_14_token       260081\n",
      "tok_14_tag         260081\n",
      "tok_14_dep         260081\n",
      "tok_15_token       267106\n",
      "tok_15_tag         267106\n",
      "tok_15_dep         267106\n",
      "tok_16_token       273061\n",
      "tok_16_tag         273061\n",
      "tok_16_dep         273061\n",
      "tok_17_token       278121\n",
      "tok_17_tag         278121\n",
      "tok_17_dep         278121\n",
      "tok_18_token       282396\n",
      "tok_18_tag         282396\n",
      "tok_18_dep         282396\n",
      "tok_19_token       285961\n",
      "tok_19_tag         285961\n",
      "tok_19_dep         285961\n",
      "tok_20_token       288804\n",
      "tok_20_tag         288804\n",
      "tok_20_dep         288804\n",
      "tok_21_token       291267\n",
      "tok_21_tag         291267\n",
      "tok_21_dep         291267\n",
      "tok_22_token       293375\n",
      "tok_22_tag         293375\n",
      "tok_22_dep         293375\n",
      "tok_23_token       295125\n",
      "tok_23_tag         295125\n",
      "tok_23_dep         295125\n",
      "tok_24_token       296564\n",
      "tok_24_tag         296564\n",
      "tok_24_dep         296564\n",
      "tok_25_token       297736\n",
      "tok_25_tag         297736\n",
      "tok_25_dep         297736\n",
      "tok_26_token       298756\n",
      "tok_26_tag         298756\n",
      "tok_26_dep         298756\n",
      "tok_27_token       299618\n",
      "tok_27_tag         299618\n",
      "tok_27_dep         299618\n",
      "tok_28_token       300391\n",
      "tok_28_tag         300391\n",
      "tok_28_dep         300391\n",
      "tok_29_token       301006\n",
      "tok_29_tag         301006\n",
      "tok_29_dep         301006\n",
      "tok_30_token       301511\n",
      "tok_30_tag         301511\n",
      "tok_30_dep         301511\n",
      "tok_31_token       301953\n",
      "tok_31_tag         301953\n",
      "tok_31_dep         301953\n",
      "tok_32_token       302347\n",
      "tok_32_tag         302347\n",
      "tok_32_dep         302347\n",
      "tok_33_token       302679\n",
      "tok_33_tag         302679\n",
      "tok_33_dep         302679\n",
      "tok_34_token       302947\n",
      "tok_34_tag         302947\n",
      "tok_34_dep         302947\n",
      "tok_35_token       303177\n",
      "tok_35_tag         303177\n",
      "tok_35_dep         303177\n",
      "tok_36_token       303366\n",
      "tok_36_tag         303366\n",
      "tok_36_dep         303366\n",
      "tok_37_token       303540\n",
      "tok_37_tag         303540\n",
      "tok_37_dep         303540\n",
      "tok_38_token       303700\n",
      "tok_38_tag         303700\n",
      "tok_38_dep         303700\n",
      "tok_39_token       303841\n",
      "tok_39_tag         303841\n",
      "tok_39_dep         303841\n",
      "tok_40_token       303935\n",
      "tok_40_tag         303935\n",
      "tok_40_dep         303935\n",
      "tok_41_token       304030\n",
      "tok_41_tag         304030\n",
      "tok_41_dep         304030\n",
      "tok_42_token       304127\n",
      "tok_42_tag         304127\n",
      "tok_42_dep         304127\n",
      "tok_43_token       304190\n",
      "tok_43_tag         304190\n",
      "tok_43_dep         304190\n",
      "tok_44_token       304252\n",
      "tok_44_tag         304252\n",
      "tok_44_dep         304252\n",
      "tok_45_token       304309\n",
      "tok_45_tag         304309\n",
      "tok_45_dep         304309\n",
      "tok_46_token       304355\n",
      "tok_46_tag         304355\n",
      "tok_46_dep         304355\n",
      "tok_47_token       304400\n",
      "tok_47_tag         304400\n",
      "tok_47_dep         304400\n",
      "tok_48_token       304429\n",
      "tok_48_tag         304429\n",
      "tok_48_dep         304429\n",
      "tok_49_token       304459\n",
      "tok_49_tag         304459\n",
      "tok_49_dep         304459\n",
      "tok_50_token       304484\n",
      "tok_50_tag         304484\n",
      "tok_50_dep         304484\n",
      "tok_51_token       304510\n",
      "tok_51_tag         304510\n",
      "tok_51_dep         304510\n",
      "tok_52_token       304531\n",
      "tok_52_tag         304531\n",
      "tok_52_dep         304531\n",
      "tok_53_token       304550\n",
      "tok_53_tag         304550\n",
      "tok_53_dep         304550\n",
      "tok_54_token       304569\n",
      "tok_54_tag         304569\n",
      "tok_54_dep         304569\n",
      "tok_55_token       304582\n",
      "tok_55_tag         304582\n",
      "tok_55_dep         304582\n",
      "tok_56_token       304594\n",
      "tok_56_tag         304594\n",
      "tok_56_dep         304594\n",
      "tok_57_token       304606\n",
      "tok_57_tag         304606\n",
      "tok_57_dep         304606\n",
      "tok_58_token       304615\n",
      "tok_58_tag         304615\n",
      "tok_58_dep         304615\n",
      "tok_59_token       304623\n",
      "tok_59_tag         304623\n",
      "tok_59_dep         304623\n",
      "tok_60_token       304636\n",
      "tok_60_tag         304636\n",
      "tok_60_dep         304636\n",
      "tok_61_token       304640\n",
      "tok_61_tag         304640\n",
      "tok_61_dep         304640\n",
      "tok_62_token       304646\n",
      "tok_62_tag         304646\n",
      "tok_62_dep         304646\n",
      "tok_63_token       304654\n",
      "tok_63_tag         304654\n",
      "tok_63_dep         304654\n",
      "tok_64_token       304664\n",
      "tok_64_tag         304664\n",
      "tok_64_dep         304664\n",
      "tok_65_token       304669\n",
      "tok_65_tag         304669\n",
      "tok_65_dep         304669\n",
      "tok_66_token       304675\n",
      "tok_66_tag         304675\n",
      "tok_66_dep         304675\n",
      "tok_67_token       304678\n",
      "tok_67_tag         304678\n",
      "tok_67_dep         304678\n",
      "tok_68_token       304680\n",
      "tok_68_tag         304680\n",
      "tok_68_dep         304680\n",
      "tok_69_token       304686\n",
      "tok_69_tag         304686\n",
      "tok_69_dep         304686\n",
      "tok_70_token       304690\n",
      "tok_70_tag         304690\n",
      "tok_70_dep         304690\n",
      "tok_71_token       304692\n",
      "tok_71_tag         304692\n",
      "tok_71_dep         304692\n",
      "tok_72_token       304694\n",
      "tok_72_tag         304694\n",
      "tok_72_dep         304694\n",
      "tok_73_token       304696\n",
      "tok_73_tag         304696\n",
      "tok_73_dep         304696\n",
      "tok_74_token       304697\n",
      "tok_74_tag         304697\n",
      "tok_74_dep         304697\n",
      "tok_75_token       304698\n",
      "tok_75_tag         304698\n",
      "tok_75_dep         304698\n",
      "tok_76_token       304702\n",
      "tok_76_tag         304702\n",
      "tok_76_dep         304702\n",
      "tok_77_token       304702\n",
      "tok_77_tag         304702\n",
      "tok_77_dep         304702\n",
      "tok_78_token       304705\n",
      "tok_78_tag         304705\n",
      "tok_78_dep         304705\n",
      "tok_79_token       304706\n",
      "tok_79_tag         304706\n",
      "tok_79_dep         304706\n",
      "tok_80_token       304707\n",
      "tok_80_tag         304707\n",
      "tok_80_dep         304707\n",
      "tok_81_token       304707\n",
      "tok_81_tag         304707\n",
      "tok_81_dep         304707\n",
      "tok_82_token       304707\n",
      "tok_82_tag         304707\n",
      "tok_82_dep         304707\n",
      "tok_83_token       304707\n",
      "tok_83_tag         304707\n",
      "tok_83_dep         304707\n",
      "tok_84_token       304707\n",
      "tok_84_tag         304707\n",
      "tok_84_dep         304707\n",
      "tok_85_token       304708\n",
      "tok_85_tag         304708\n",
      "tok_85_dep         304708\n",
      "tok_86_token       304708\n",
      "tok_86_tag         304708\n",
      "tok_86_dep         304708\n",
      "tok_87_token       304709\n",
      "tok_87_tag         304709\n",
      "tok_87_dep         304709\n",
      "tok_88_token       304709\n",
      "tok_88_tag         304709\n",
      "tok_88_dep         304709\n",
      "tok_89_token       304710\n",
      "tok_89_tag         304710\n",
      "tok_89_dep         304710\n",
      "tok_90_token       304710\n",
      "tok_90_tag         304710\n",
      "tok_90_dep         304710\n",
      "tok_91_token       304711\n",
      "tok_91_tag         304711\n",
      "tok_91_dep         304711\n",
      "tok_92_token       304711\n",
      "tok_92_tag         304711\n",
      "tok_92_dep         304711\n",
      "tok_93_token       304711\n",
      "tok_93_tag         304711\n",
      "tok_93_dep         304711\n",
      "tok_94_token       304711\n",
      "tok_94_tag         304711\n",
      "tok_94_dep         304711\n",
      "tok_95_token       304711\n",
      "tok_95_tag         304711\n",
      "tok_95_dep         304711\n",
      "tok_96_token       304711\n",
      "tok_96_tag         304711\n",
      "tok_96_dep         304711\n",
      "tok_97_token       304711\n",
      "tok_97_tag         304711\n",
      "tok_97_dep         304711\n",
      "tok_98_token       304711\n",
      "tok_98_tag         304711\n",
      "tok_98_dep         304711\n",
      "tok_99_token       304711\n",
      "tok_99_tag         304711\n",
      "tok_99_dep         304711\n",
      "tok_100_token      304711\n",
      "tok_100_tag        304711\n",
      "tok_100_dep        304711\n",
      "tok_101_token      304712\n",
      "tok_101_tag        304712\n",
      "tok_101_dep        304712\n",
      "tok_102_token      304712\n",
      "tok_102_tag        304712\n",
      "tok_102_dep        304712\n",
      "tok_103_token      304712\n",
      "tok_103_tag        304712\n",
      "tok_103_dep        304712\n",
      "tok_104_token      304712\n",
      "tok_104_tag        304712\n",
      "tok_104_dep        304712\n",
      "tok_105_token      304712\n",
      "tok_105_tag        304712\n",
      "tok_105_dep        304712\n",
      "tok_106_token      304712\n",
      "tok_106_tag        304712\n",
      "tok_106_dep        304712\n",
      "tok_107_token      304712\n",
      "tok_107_tag        304712\n",
      "tok_107_dep        304712\n",
      "tok_108_token      304712\n",
      "tok_108_tag        304712\n",
      "tok_108_dep        304712\n",
      "tok_109_token      304712\n",
      "tok_109_tag        304712\n",
      "tok_109_dep        304712\n",
      "tok_110_token      304712\n",
      "tok_110_tag        304712\n",
      "tok_110_dep        304712\n",
      "tok_111_token      304712\n",
      "tok_111_tag        304712\n",
      "tok_111_dep        304712\n",
      "tok_112_token      304712\n",
      "tok_112_tag        304712\n",
      "tok_112_dep        304712\n",
      "tok_113_token      304712\n",
      "tok_113_tag        304712\n",
      "tok_113_dep        304712\n",
      "tok_114_token      304712\n",
      "tok_114_tag        304712\n",
      "tok_114_dep        304712\n",
      "tok_115_token      304712\n",
      "tok_115_tag        304712\n",
      "tok_115_dep        304712\n",
      "tok_116_token      304712\n",
      "tok_116_tag        304712\n",
      "tok_116_dep        304712\n",
      "tok_117_token      304712\n",
      "tok_117_tag        304712\n",
      "tok_117_dep        304712\n",
      "tok_118_token      304712\n",
      "tok_118_tag        304712\n",
      "tok_118_dep        304712\n",
      "tok_119_token      304712\n",
      "tok_119_tag        304712\n",
      "tok_119_dep        304712\n",
      "tok_120_token      304712\n",
      "tok_120_tag        304712\n",
      "tok_120_dep        304712\n",
      "tok_121_token      304712\n",
      "tok_121_tag        304712\n",
      "tok_121_dep        304712\n",
      "tok_122_token      304712\n",
      "tok_122_tag        304712\n",
      "tok_122_dep        304712\n",
      "tok_123_token      304712\n",
      "tok_123_tag        304712\n",
      "tok_123_dep        304712\n",
      "tok_124_token      304712\n",
      "tok_124_tag        304712\n",
      "tok_124_dep        304712\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Temporarily adjust display settings to show all columns\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(df.isnull().sum())\n",
    "#print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6acbac4-a8a0-4380-9083-b672443d8459",
   "metadata": {},
   "source": [
    "## Saving the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e9f5895-8d1a-4900-9e50-8c6062ce1348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving the DataFrame\n",
    "file_path_parquet = os.path.join(data_dir, 'utterances.parquet')\n",
    "df.to_parquet(file_path_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df029d37-dc2e-4173-87ce-7c7f4d48ccc2",
   "metadata": {},
   "source": [
    "`utterances.jsonl` - 351 404 KB, `utterances.parquet` - 28 409 KB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30907873-7514-495a-90aa-1e9f247cf987",
   "metadata": {},
   "source": [
    "## Loading the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93a10f80-f191-48a5-99f0-4930f607c96b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>tok_0_token</th>\n",
       "      <th>tok_0_tag</th>\n",
       "      <th>tok_0_dep</th>\n",
       "      <th>...</th>\n",
       "      <th>tok_121_dep</th>\n",
       "      <th>tok_122_token</th>\n",
       "      <th>tok_122_tag</th>\n",
       "      <th>tok_122_dep</th>\n",
       "      <th>tok_123_token</th>\n",
       "      <th>tok_123_tag</th>\n",
       "      <th>tok_123_dep</th>\n",
       "      <th>tok_124_token</th>\n",
       "      <th>tok_124_tag</th>\n",
       "      <th>tok_124_dep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1045</td>\n",
       "      <td>L1044</td>\n",
       "      <td>They do not!</td>\n",
       "      <td>u0</td>\n",
       "      <td>L1044</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>They</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "      <td>They do to!</td>\n",
       "      <td>u2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>They</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L985</td>\n",
       "      <td>L984</td>\n",
       "      <td>I hope so.</td>\n",
       "      <td>u0</td>\n",
       "      <td>L984</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "      <td>She okay?</td>\n",
       "      <td>u2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>She</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L925</td>\n",
       "      <td>L924</td>\n",
       "      <td>Let's go.</td>\n",
       "      <td>u0</td>\n",
       "      <td>L924</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>Let</td>\n",
       "      <td>VB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>L924</td>\n",
       "      <td>L924</td>\n",
       "      <td>Wow</td>\n",
       "      <td>u2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>Wow</td>\n",
       "      <td>UH</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>L872</td>\n",
       "      <td>L870</td>\n",
       "      <td>Okay -- you're gonna need to learn how to lie.</td>\n",
       "      <td>u0</td>\n",
       "      <td>L871</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>Okay</td>\n",
       "      <td>UH</td>\n",
       "      <td>intj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>L871</td>\n",
       "      <td>L870</td>\n",
       "      <td>No</td>\n",
       "      <td>u2</td>\n",
       "      <td>L870</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>No</td>\n",
       "      <td>UH</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "      <td>I'm kidding.  You know how sometimes you just ...</td>\n",
       "      <td>u0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>And</td>\n",
       "      <td>CC</td>\n",
       "      <td>cc</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>L869</td>\n",
       "      <td>L866</td>\n",
       "      <td>Like my fear of wearing pastels?</td>\n",
       "      <td>u0</td>\n",
       "      <td>L868</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>Like</td>\n",
       "      <td>IN</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>L868</td>\n",
       "      <td>L866</td>\n",
       "      <td>The \"real you\".</td>\n",
       "      <td>u2</td>\n",
       "      <td>L867</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>L867</td>\n",
       "      <td>L866</td>\n",
       "      <td>What good stuff?</td>\n",
       "      <td>u0</td>\n",
       "      <td>L866</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>What</td>\n",
       "      <td>WDT</td>\n",
       "      <td>det</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>L866</td>\n",
       "      <td>L866</td>\n",
       "      <td>I figured you'd get to the good stuff eventually.</td>\n",
       "      <td>u2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>L865</td>\n",
       "      <td>L862</td>\n",
       "      <td>Thank God!  If I had to hear one more story ab...</td>\n",
       "      <td>u2</td>\n",
       "      <td>L864</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>If</td>\n",
       "      <td>IN</td>\n",
       "      <td>mark</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>L864</td>\n",
       "      <td>L862</td>\n",
       "      <td>Me.  This endless ...blonde babble. I'm like, ...</td>\n",
       "      <td>u0</td>\n",
       "      <td>L863</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>L863</td>\n",
       "      <td>L862</td>\n",
       "      <td>What crap?</td>\n",
       "      <td>u2</td>\n",
       "      <td>L862</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>What</td>\n",
       "      <td>WDT</td>\n",
       "      <td>det</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>L862</td>\n",
       "      <td>L862</td>\n",
       "      <td>do you listen to this crap?</td>\n",
       "      <td>u0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>do</td>\n",
       "      <td>VBP</td>\n",
       "      <td>aux</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>L861</td>\n",
       "      <td>L860</td>\n",
       "      <td>No...</td>\n",
       "      <td>u2</td>\n",
       "      <td>L860</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>No</td>\n",
       "      <td>UH</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>L860</td>\n",
       "      <td>L860</td>\n",
       "      <td>Then Guillermo says, \"If you go any lighter, y...</td>\n",
       "      <td>u0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>Then</td>\n",
       "      <td>RB</td>\n",
       "      <td>advmod</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>L699</td>\n",
       "      <td>L696</td>\n",
       "      <td>You always been this selfish?</td>\n",
       "      <td>u2</td>\n",
       "      <td>L698</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>You</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 382 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id conversation_id                                               text  \\\n",
       "0   L1045           L1044                                       They do not!   \n",
       "1   L1044           L1044                                        They do to!   \n",
       "2    L985            L984                                         I hope so.   \n",
       "3    L984            L984                                          She okay?   \n",
       "4    L925            L924                                          Let's go.   \n",
       "5    L924            L924                                                Wow   \n",
       "6    L872            L870     Okay -- you're gonna need to learn how to lie.   \n",
       "7    L871            L870                                                 No   \n",
       "8    L870            L870  I'm kidding.  You know how sometimes you just ...   \n",
       "9    L869            L866                   Like my fear of wearing pastels?   \n",
       "10   L868            L866                                    The \"real you\".   \n",
       "11   L867            L866                                   What good stuff?   \n",
       "12   L866            L866  I figured you'd get to the good stuff eventually.   \n",
       "13   L865            L862  Thank God!  If I had to hear one more story ab...   \n",
       "14   L864            L862  Me.  This endless ...blonde babble. I'm like, ...   \n",
       "15   L863            L862                                         What crap?   \n",
       "16   L862            L862                        do you listen to this crap?   \n",
       "17   L861            L860                                              No...   \n",
       "18   L860            L860  Then Guillermo says, \"If you go any lighter, y...   \n",
       "19   L699            L696                      You always been this selfish?   \n",
       "\n",
       "   speaker reply_to timestamp movie_id tok_0_token tok_0_tag tok_0_dep  ...  \\\n",
       "0       u0    L1044      None       m0        They       PRP     nsubj  ...   \n",
       "1       u2     None      None       m0        They       PRP     nsubj  ...   \n",
       "2       u0     L984      None       m0           I       PRP     nsubj  ...   \n",
       "3       u2     None      None       m0         She       PRP     nsubj  ...   \n",
       "4       u0     L924      None       m0         Let        VB      ROOT  ...   \n",
       "5       u2     None      None       m0         Wow        UH      ROOT  ...   \n",
       "6       u0     L871      None       m0        Okay        UH      intj  ...   \n",
       "7       u2     L870      None       m0          No        UH      ROOT  ...   \n",
       "8       u0     None      None       m0         And        CC        cc  ...   \n",
       "9       u0     L868      None       m0        Like        IN      ROOT  ...   \n",
       "10      u2     L867      None       m0         The        DT       det  ...   \n",
       "11      u0     L866      None       m0        What       WDT       det  ...   \n",
       "12      u2     None      None       m0           I       PRP     nsubj  ...   \n",
       "13      u2     L864      None       m0          If        IN      mark  ...   \n",
       "14      u0     L863      None       m0           I       PRP     nsubj  ...   \n",
       "15      u2     L862      None       m0        What       WDT       det  ...   \n",
       "16      u0     None      None       m0          do       VBP       aux  ...   \n",
       "17      u2     L860      None       m0          No        UH      ROOT  ...   \n",
       "18      u0     None      None       m0        Then        RB    advmod  ...   \n",
       "19      u2     L698      None       m0         You       PRP     nsubj  ...   \n",
       "\n",
       "   tok_121_dep tok_122_token tok_122_tag tok_122_dep tok_123_token  \\\n",
       "0         None          None        None        None          None   \n",
       "1         None          None        None        None          None   \n",
       "2         None          None        None        None          None   \n",
       "3         None          None        None        None          None   \n",
       "4         None          None        None        None          None   \n",
       "5         None          None        None        None          None   \n",
       "6         None          None        None        None          None   \n",
       "7         None          None        None        None          None   \n",
       "8         None          None        None        None          None   \n",
       "9         None          None        None        None          None   \n",
       "10        None          None        None        None          None   \n",
       "11        None          None        None        None          None   \n",
       "12        None          None        None        None          None   \n",
       "13        None          None        None        None          None   \n",
       "14        None          None        None        None          None   \n",
       "15        None          None        None        None          None   \n",
       "16        None          None        None        None          None   \n",
       "17        None          None        None        None          None   \n",
       "18        None          None        None        None          None   \n",
       "19        None          None        None        None          None   \n",
       "\n",
       "   tok_123_tag tok_123_dep tok_124_token tok_124_tag tok_124_dep  \n",
       "0         None        None          None        None        None  \n",
       "1         None        None          None        None        None  \n",
       "2         None        None          None        None        None  \n",
       "3         None        None          None        None        None  \n",
       "4         None        None          None        None        None  \n",
       "5         None        None          None        None        None  \n",
       "6         None        None          None        None        None  \n",
       "7         None        None          None        None        None  \n",
       "8         None        None          None        None        None  \n",
       "9         None        None          None        None        None  \n",
       "10        None        None          None        None        None  \n",
       "11        None        None          None        None        None  \n",
       "12        None        None          None        None        None  \n",
       "13        None        None          None        None        None  \n",
       "14        None        None          None        None        None  \n",
       "15        None        None          None        None        None  \n",
       "16        None        None          None        None        None  \n",
       "17        None        None          None        None        None  \n",
       "18        None        None          None        None        None  \n",
       "19        None        None          None        None        None  \n",
       "\n",
       "[20 rows x 382 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the DataFrame\n",
    "file_path_parquet = os.path.join(data_dir, 'utterances.parquet')\n",
    "df_loaded_parquet = pd.read_parquet(file_path_parquet)\n",
    "\n",
    "df_loaded_parquet.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2977c6-4473-451b-a36e-bb2c7a6b65f0",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab66833-1b80-4eaa-913b-ff5c938cca74",
   "metadata": {},
   "source": [
    "### Leaving only necessary data for initial stage of the project\n",
    "Id, conversation_id for tracking the flow of conversations and reply_to for understanding the sequence within the dialogue, and conversation text ofcourse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed419e17-c695-4d32-a6f9-8ad8ac93e68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>reply_to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They do not!</td>\n",
       "      <td>L1045</td>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They do to!</td>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I hope so.</td>\n",
       "      <td>L985</td>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>She okay?</td>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Let's go.</td>\n",
       "      <td>L925</td>\n",
       "      <td>L924</td>\n",
       "      <td>L924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wow</td>\n",
       "      <td>L924</td>\n",
       "      <td>L924</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Okay -- you're gonna need to learn how to lie.</td>\n",
       "      <td>L872</td>\n",
       "      <td>L870</td>\n",
       "      <td>L871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>No</td>\n",
       "      <td>L871</td>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I'm kidding.  You know how sometimes you just ...</td>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Like my fear of wearing pastels?</td>\n",
       "      <td>L869</td>\n",
       "      <td>L866</td>\n",
       "      <td>L868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The \"real you\".</td>\n",
       "      <td>L868</td>\n",
       "      <td>L866</td>\n",
       "      <td>L867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What good stuff?</td>\n",
       "      <td>L867</td>\n",
       "      <td>L866</td>\n",
       "      <td>L866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I figured you'd get to the good stuff eventually.</td>\n",
       "      <td>L866</td>\n",
       "      <td>L866</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Thank God!  If I had to hear one more story ab...</td>\n",
       "      <td>L865</td>\n",
       "      <td>L862</td>\n",
       "      <td>L864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Me.  This endless ...blonde babble. I'm like, ...</td>\n",
       "      <td>L864</td>\n",
       "      <td>L862</td>\n",
       "      <td>L863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What crap?</td>\n",
       "      <td>L863</td>\n",
       "      <td>L862</td>\n",
       "      <td>L862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>do you listen to this crap?</td>\n",
       "      <td>L862</td>\n",
       "      <td>L862</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>No...</td>\n",
       "      <td>L861</td>\n",
       "      <td>L860</td>\n",
       "      <td>L860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Then Guillermo says, \"If you go any lighter, y...</td>\n",
       "      <td>L860</td>\n",
       "      <td>L860</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>You always been this selfish?</td>\n",
       "      <td>L699</td>\n",
       "      <td>L696</td>\n",
       "      <td>L698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>But</td>\n",
       "      <td>L698</td>\n",
       "      <td>L696</td>\n",
       "      <td>L697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Then that's all you had to say.</td>\n",
       "      <td>L697</td>\n",
       "      <td>L696</td>\n",
       "      <td>L696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Well, no...</td>\n",
       "      <td>L696</td>\n",
       "      <td>L696</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>You never wanted to go out with 'me, did you?</td>\n",
       "      <td>L695</td>\n",
       "      <td>L693</td>\n",
       "      <td>L694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I was?</td>\n",
       "      <td>L694</td>\n",
       "      <td>L693</td>\n",
       "      <td>L693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I looked for you back at the party, but you al...</td>\n",
       "      <td>L693</td>\n",
       "      <td>L693</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Tons</td>\n",
       "      <td>L663</td>\n",
       "      <td>L662</td>\n",
       "      <td>L662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Have fun tonight?</td>\n",
       "      <td>L662</td>\n",
       "      <td>L662</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>I believe we share an art instructor</td>\n",
       "      <td>L578</td>\n",
       "      <td>L577</td>\n",
       "      <td>L577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>You know Chastity?</td>\n",
       "      <td>L577</td>\n",
       "      <td>L577</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text     id conversation_id  \\\n",
       "0                                        They do not!  L1045           L1044   \n",
       "1                                         They do to!  L1044           L1044   \n",
       "2                                          I hope so.   L985            L984   \n",
       "3                                           She okay?   L984            L984   \n",
       "4                                           Let's go.   L925            L924   \n",
       "5                                                 Wow   L924            L924   \n",
       "6      Okay -- you're gonna need to learn how to lie.   L872            L870   \n",
       "7                                                  No   L871            L870   \n",
       "8   I'm kidding.  You know how sometimes you just ...   L870            L870   \n",
       "9                    Like my fear of wearing pastels?   L869            L866   \n",
       "10                                    The \"real you\".   L868            L866   \n",
       "11                                   What good stuff?   L867            L866   \n",
       "12  I figured you'd get to the good stuff eventually.   L866            L866   \n",
       "13  Thank God!  If I had to hear one more story ab...   L865            L862   \n",
       "14  Me.  This endless ...blonde babble. I'm like, ...   L864            L862   \n",
       "15                                         What crap?   L863            L862   \n",
       "16                        do you listen to this crap?   L862            L862   \n",
       "17                                              No...   L861            L860   \n",
       "18  Then Guillermo says, \"If you go any lighter, y...   L860            L860   \n",
       "19                      You always been this selfish?   L699            L696   \n",
       "20                                                But   L698            L696   \n",
       "21                    Then that's all you had to say.   L697            L696   \n",
       "22                                        Well, no...   L696            L696   \n",
       "23      You never wanted to go out with 'me, did you?   L695            L693   \n",
       "24                                             I was?   L694            L693   \n",
       "25  I looked for you back at the party, but you al...   L693            L693   \n",
       "26                                               Tons   L663            L662   \n",
       "27                                  Have fun tonight?   L662            L662   \n",
       "28               I believe we share an art instructor   L578            L577   \n",
       "29                                 You know Chastity?   L577            L577   \n",
       "\n",
       "   reply_to  \n",
       "0     L1044  \n",
       "1      None  \n",
       "2      L984  \n",
       "3      None  \n",
       "4      L924  \n",
       "5      None  \n",
       "6      L871  \n",
       "7      L870  \n",
       "8      None  \n",
       "9      L868  \n",
       "10     L867  \n",
       "11     L866  \n",
       "12     None  \n",
       "13     L864  \n",
       "14     L863  \n",
       "15     L862  \n",
       "16     None  \n",
       "17     L860  \n",
       "18     None  \n",
       "19     L698  \n",
       "20     L697  \n",
       "21     L696  \n",
       "22     None  \n",
       "23     L694  \n",
       "24     L693  \n",
       "25     None  \n",
       "26     L662  \n",
       "27     None  \n",
       "28     L577  \n",
       "29     None  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations = df_loaded_parquet[['text', 'id', 'conversation_id', 'reply_to']]\n",
    "conversations.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4690dc51-4ad3-494b-af82-a0cbf300bc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries where 'id' equals 'conversation_id': 83097\n",
      "Total entries where 'reply_to' is None: 83097\n"
     ]
    }
   ],
   "source": [
    "# Cheking if counts of None are the same with 'id' == 'conversation_id'\n",
    "print(\"Total entries where 'id' equals 'conversation_id':\", (df['id'] == df['conversation_id']).sum())\n",
    "print(\"Total entries where 'reply_to' is None:\", df['reply_to'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38235458-3018-42ef-b554-d4c79acd5caa",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Looks like the dataset is well-structured and prepared for further processing: <br>\n",
    "Conversation_id and id:  <br>\n",
    "When conversation_id and id are the same and there's no reply_to, this indicates the start of a new conversation, this allows to understand where each conversation begins.  <br>\n",
    "Counts of None in reply_to:  <br>\n",
    "The count of None in the reply_to field matches the number of conversations (83,097). This confirms that each conversation starts with a message that does not reply to any previous message, this is the first message in the thread.  <br>\n",
    "Data Cleanliness:  <br>\n",
    "The alignment of these counts and the consistency of data formatting suggest that dataset is clean and structured. Each message within the dataset is correctly linked to its conversation, and the flow of conversations is well-defined.  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50da00ca-77b9-47f7-a5cc-e6ad457c318a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 304713 entries, 0 to 304712\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   text             304713 non-null  object\n",
      " 1   id               304713 non-null  object\n",
      " 2   conversation_id  304713 non-null  object\n",
      " 3   reply_to         221616 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 9.3+ MB\n"
     ]
    }
   ],
   "source": [
    "conversations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee973182-677d-4de8-a117-fc7fc7ac35ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     304713\n",
       "unique    265774\n",
       "top        What?\n",
       "freq        1684\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations.text.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048d07c7-d160-4ef1-bc97-ced5d1bc22a0",
   "metadata": {},
   "source": [
    "## Analyze text of conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352cb5a8-53bf-4255-b52d-4c211fbb8724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae4c3812-49d3-4b6e-8f83-c66bae36b2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average message length (characters): 55.25953930419772\n",
      "Average message length (words): 13.721094931952361\n",
      "Min message length (characters): 0\n",
      "Max message length (characters): 3046\n",
      "Standard deviation (characters): 64.06661834805733\n",
      "Most common words: [('.', 332912), (',', 170188), ('you', 148400), ('i', 140952), ('?', 110240), ('the', 99132), ('to', 80649), ('a', 70839), (\"'s\", 66538), ('it', 66076), (\"n't\", 55224), ('...', 50796), ('do', 47049), ('that', 46582), ('and', 45934), ('of', 39338), ('!', 37866), ('what', 37719), ('in', 34129), ('me', 32203), ('is', 31639), ('we', 29291), ('he', 27408), ('--', 26662), ('this', 24616), ('for', 23415), ('have', 22934), (\"'m\", 22578), (\"'re\", 21717), ('know', 21657), ('was', 21407), ('your', 20962), ('my', 20824), ('not', 19883), ('on', 19560), ('no', 19425), ('be', 19414), ('are', 17600), ('but', 17321), ('with', 17249), ('they', 16942), ('just', 15853), ('all', 15392), ('like', 15007), (\"'ll\", 14613), ('did', 14547), ('there', 14446), ('get', 14152), ('about', 14000), ('so', 13447)]\n",
      "Average sentiment (polarity): 0.04174547982992158\n",
      "Sentiment distribution: count    304713.000000\n",
      "mean          0.041745\n",
      "std           0.246197\n",
      "min          -1.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.013889\n",
      "max           1.000000\n",
      "Name: sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist, word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "# Ensure that the punkt tokenizer is available\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Basic statistics\n",
    "conversations.loc[:, 'msg_length'] = conversations['text'].apply(len)\n",
    "conversations.loc[:, 'word_count'] = conversations['text'].apply(lambda x: len(word_tokenize(x)))\n",
    "\n",
    "\n",
    "print(\"Average message length (characters):\", np.mean(conversations['msg_length']))\n",
    "print(\"Average message length (words):\", np.mean(conversations['word_count']))\n",
    "print(\"Min message length (characters):\", np.min(conversations['msg_length']))\n",
    "print(\"Max message length (characters):\", np.max(conversations['msg_length']))\n",
    "print(\"Standard deviation (characters):\", np.std(conversations['msg_length']))\n",
    "\n",
    "# Word frequency analysis\n",
    "all_words = ' '.join(conversations['text']).lower()\n",
    "words = word_tokenize(all_words)\n",
    "freq_dist = FreqDist(words)\n",
    "print(\"Most common words:\", freq_dist.most_common(50))\n",
    "\n",
    "# Sentiment analysis\n",
    "conversations.loc[:, 'sentiment'] = conversations['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "print(\"Average sentiment (polarity):\", np.mean(conversations['sentiment']))\n",
    "print(\"Sentiment distribution:\", conversations['sentiment'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b190a4ec-cffb-471f-917f-3aa8a1eb98c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zero-length messages: 267\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>msg_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>153596</th>\n",
       "      <td></td>\n",
       "      <td>L128654</td>\n",
       "      <td>L128650</td>\n",
       "      <td>L128653</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153605</th>\n",
       "      <td></td>\n",
       "      <td>L128644</td>\n",
       "      <td>L128625</td>\n",
       "      <td>L128643</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153351</th>\n",
       "      <td></td>\n",
       "      <td>L129302</td>\n",
       "      <td>L129301</td>\n",
       "      <td>L129301</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168756</th>\n",
       "      <td></td>\n",
       "      <td>L166989</td>\n",
       "      <td>L166988</td>\n",
       "      <td>L166988</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153915</th>\n",
       "      <td></td>\n",
       "      <td>L128979</td>\n",
       "      <td>L128978</td>\n",
       "      <td>L128978</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153384</th>\n",
       "      <td></td>\n",
       "      <td>L129179</td>\n",
       "      <td>L129168</td>\n",
       "      <td>L129178</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154115</th>\n",
       "      <td></td>\n",
       "      <td>L128754</td>\n",
       "      <td>L128752</td>\n",
       "      <td>L128753</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153573</th>\n",
       "      <td></td>\n",
       "      <td>L128677</td>\n",
       "      <td>L128650</td>\n",
       "      <td>L128676</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271158</th>\n",
       "      <td></td>\n",
       "      <td>L556463</td>\n",
       "      <td>L556443</td>\n",
       "      <td>L556462</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299552</th>\n",
       "      <td></td>\n",
       "      <td>L649938</td>\n",
       "      <td>L649936</td>\n",
       "      <td>L649937</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text       id conversation_id reply_to  msg_length  word_count  \\\n",
       "153596       L128654         L128650  L128653           0           0   \n",
       "153605       L128644         L128625  L128643           0           0   \n",
       "153351       L129302         L129301  L129301           0           0   \n",
       "168756       L166989         L166988  L166988           0           0   \n",
       "153915       L128979         L128978  L128978           0           0   \n",
       "153384       L129179         L129168  L129178           0           0   \n",
       "154115       L128754         L128752  L128753           0           0   \n",
       "153573       L128677         L128650  L128676           0           0   \n",
       "271158       L556463         L556443  L556462           0           0   \n",
       "299552       L649938         L649936  L649937           0           0   \n",
       "\n",
       "        sentiment  \n",
       "153596        0.0  \n",
       "153605        0.0  \n",
       "153351        0.0  \n",
       "168756        0.0  \n",
       "153915        0.0  \n",
       "153384        0.0  \n",
       "154115        0.0  \n",
       "153573        0.0  \n",
       "271158        0.0  \n",
       "299552        0.0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating zero characters messages\n",
    "zero_length_messages = conversations[conversations['text'].apply(len) == 0]\n",
    "print(\"Number of zero-length messages:\", len(zero_length_messages))\n",
    "zero_length_messages.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde0265a-8eb2-4d16-b041-c898d500ea15",
   "metadata": {},
   "source": [
    "Same id and conversation_id with None in reply_to - these messages likely represent the start of a conversation. Removing them could impact the structure of the conversation as it might remove the entry point for a conversational thread.\n",
    "Different id and conversation_id with a specific reply_to - these are responses within a conversation. Their removal might disrupt the sequence, making it difficult to follow the flow of the conversation.\n",
    "Messages with a specific reply_to - these indicate replies within the conversation sequence. Removing these could create gaps in the conversation history. I've decided to leave zero text conversations for now, besides thera are only 267 of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6dc250b-588e-45ae-be67-0d49c8a41bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of long messages: 3151\n"
     ]
    }
   ],
   "source": [
    "# Calculating long messages\n",
    "long_messages = conversations[conversations['msg_length'] > 300]\n",
    "print(\"Number of long messages:\", len(long_messages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e2e503e-cb43-44d0-9a91-cb0c67b633b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, God...Oh, God...  Jim, excuse me...Ray, I told you, who he is is the senior vice- president American Express.  His family owns 32 per...Over the past years I've sold him...I can't tell you the dollar amount, but quite a lot of land.  I promised five weeks ago that I'd go to the wife's birthday party in Kenilworth tonight.  I have to go.  You understand. They treat me like a member of the family, so I have to go.\n"
     ]
    }
   ],
   "source": [
    "# Print sample long messages\n",
    "sampled_text = long_messages.sample(1)['text'].iloc[0]\n",
    "print(sampled_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c4217-6c54-47a0-994e-c9ed8bf57e2c",
   "metadata": {},
   "source": [
    "Decided to leave for now long messages - considering to use advanced NLP models such as BERT or GPT (from the transformer family), which are adept at understanding context over longer stretches of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee967afc-2af9-40b2-91c7-1ededc3643a4",
   "metadata": {},
   "source": [
    "## Conversations text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb176ba-116b-4e03-a983-39066c0244d9",
   "metadata": {},
   "source": [
    "### Normalize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6b4d4f4-2c73-404d-8886-aef6c698056a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it was warmer than i thought it would be, and the skin was softer than it looked. it's weird. thinking about it now, the organ itself seemed like a separate thing, a separate entity to me. i mean, after he pulled it out and i could look at it and touch it, i completely forgot that there was a guy attached to it. i remember literally being startled when the guy spoke to me.\n"
     ]
    }
   ],
   "source": [
    "conversations.loc[:, 'text'] = conversations['text'].str.lower().str.strip()\n",
    "long_messages = conversations[conversations['msg_length'] > 300]\n",
    "sampled_text = long_messages.sample(1)['text'].iloc[0]\n",
    "print(sampled_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de1fab-d8ae-4c74-bee6-42862a5dcfdf",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d6f74020-cf03-48cc-a39a-1711a31e8998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last spring i happened to walk past a house that i had once patronized there was a cool breeze blowing off the ocean and through the window i could see a bare leg the girl must have been taking a break between customers it was a strange moment for me because it reminded me of my mother and despite the fact that i was late for something already i just stayed there loving the atmosphere of it and my memory and the reason im telling you this epilogue is that i felt that id come full circle\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "conversations.loc[:, 'text'] = conversations['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "long_messages = conversations[conversations['msg_length'] > 300]\n",
    "sampled_text = long_messages.sample(1)['text'].iloc[0]\n",
    "print(sampled_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4354401-9e48-4b03-a10d-16a8628661e1",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c7c54d05-d740-40f9-9048-905c7ca9d6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uwouldau gone wouldnta thought twice trust thats kinda guy guy like momentary loss sanity wasnt thinking clearly listen im hero john want dough maybe little favor much didja spend already dogooder bullshit didnt spend uallu didja\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "conversations.loc[:, 'text'] = conversations['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "long_messages = conversations[conversations['msg_length'] > 300]\n",
    "sampled_text = long_messages.sample(1)['text'].iloc[0]\n",
    "print(sampled_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00370116-00cc-4f88-b20d-b175ab0639de",
   "metadata": {},
   "source": [
    "## Analyze again text of conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a9241cf4-d4ba-401b-b31f-9f52a33948af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average message length (characters): 31.95437674139272\n",
      "Average message length (words): 5.361769927768097\n",
      "Min message length (characters): 0\n",
      "Max message length (characters): 1836\n",
      "Standard deviation (characters): 38.741096498848194\n",
      "Most common words: [('dont', 24414), ('im', 22433), ('know', 21509), ('like', 14943), ('get', 14114), ('youre', 13615), ('got', 13268), ('well', 11813), ('want', 11030), ('thats', 10946), ('think', 10731), ('one', 10297), ('right', 9963), ('go', 9878), ('going', 8861), ('see', 8178), ('oh', 7721), ('yes', 7287), ('good', 7277), ('ill', 6922), ('yeah', 6871), ('tell', 6807), ('come', 6735), ('hes', 6691), ('cant', 6634), ('time', 6522), ('back', 6179), ('would', 6110), ('say', 6089), ('us', 5942), ('didnt', 5907), ('look', 5873), ('could', 5795), ('take', 5664), ('man', 5549), ('never', 5433), ('something', 5420), ('ive', 5293), ('na', 5279), ('mean', 5009), ('way', 4934), ('whats', 4875), ('make', 4736), ('really', 4558), ('okay', 4536), ('little', 4501), ('sure', 4348), ('gon', 4326), ('thing', 4100), ('said', 4061)]\n",
      "Average sentiment (polarity): 0.03328055298643672\n",
      "Sentiment distribution: count    304713.000000\n",
      "mean          0.033281\n",
      "std           0.247954\n",
      "min          -1.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max           1.000000\n",
      "Name: sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics\n",
    "conversations.loc[:, 'msg_length'] = conversations['text'].apply(len)\n",
    "conversations.loc[:, 'word_count'] = conversations['text'].apply(lambda x: len(word_tokenize(x)))\n",
    "\n",
    "\n",
    "print(\"Average message length (characters):\", np.mean(conversations['msg_length']))\n",
    "print(\"Average message length (words):\", np.mean(conversations['word_count']))\n",
    "print(\"Min message length (characters):\", np.min(conversations['msg_length']))\n",
    "print(\"Max message length (characters):\", np.max(conversations['msg_length']))\n",
    "print(\"Standard deviation (characters):\", np.std(conversations['msg_length']))\n",
    "\n",
    "# Word frequency analysis\n",
    "all_words = ' '.join(conversations['text']).lower()\n",
    "words = word_tokenize(all_words)\n",
    "freq_dist = FreqDist(words)\n",
    "print(\"Most common words:\", freq_dist.most_common(50))\n",
    "\n",
    "# Sentiment analysis\n",
    "conversations.loc[:, 'sentiment'] = conversations['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "print(\"Average sentiment (polarity):\", np.mean(conversations['sentiment']))\n",
    "print(\"Sentiment distribution:\", conversations['sentiment'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6d1f17-991c-44cf-ae52-bfd9a4340f07",
   "metadata": {},
   "source": [
    "### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "132d1c00-a51d-4923-a07e-64d19aa5bfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomui\\AppData\\Local\\Temp\\ipykernel_76440\\1884231413.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conversations['tokens'] = conversations['text'].apply(word_tokenize)\n"
     ]
    }
   ],
   "source": [
    "conversations.loc[:, 'tokens'] = conversations['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3c14ed20-528d-4365-a4ea-a45334f731c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>msg_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>czechoslovakia slavs fighting germans russians...</td>\n",
       "      <td>L2897</td>\n",
       "      <td>L2895</td>\n",
       "      <td>L2896</td>\n",
       "      <td>99</td>\n",
       "      <td>12</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>[czechoslovakia, slavs, fighting, germans, rus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>eastern europe like romania hungary</td>\n",
       "      <td>L2896</td>\n",
       "      <td>L2895</td>\n",
       "      <td>L2895</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[eastern, europe, like, romania, hungary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>maybe ritual thing someone trying send message...</td>\n",
       "      <td>L2895</td>\n",
       "      <td>L2895</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>[maybe, ritual, thing, someone, trying, send, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>look im even sure anything saw outside fire th...</td>\n",
       "      <td>L2893</td>\n",
       "      <td>L2892</td>\n",
       "      <td>L2892</td>\n",
       "      <td>106</td>\n",
       "      <td>17</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>[look, im, even, sure, anything, saw, outside,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>would call</td>\n",
       "      <td>L2892</td>\n",
       "      <td>L2892</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[would, call]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>says shes suspect</td>\n",
       "      <td>L2891</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2890</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[says, shes, suspect]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>maybe dont care either prettiest suspect ive a...</td>\n",
       "      <td>L2890</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2889</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[maybe, dont, care, either, prettiest, suspect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>hmmmm</td>\n",
       "      <td>L2889</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2888</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[hmmmm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>pretty</td>\n",
       "      <td>L2888</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2887</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>[pretty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>super said hed seen didnt live</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2887</td>\n",
       "      <td>None</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>0.234848</td>\n",
       "      <td>[super, said, hed, seen, didnt, live]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     id  \\\n",
       "1200  czechoslovakia slavs fighting germans russians...  L2897   \n",
       "1201                eastern europe like romania hungary  L2896   \n",
       "1202  maybe ritual thing someone trying send message...  L2895   \n",
       "1203  look im even sure anything saw outside fire th...  L2893   \n",
       "1204                                         would call  L2892   \n",
       "1205                                  says shes suspect  L2891   \n",
       "1206  maybe dont care either prettiest suspect ive a...  L2890   \n",
       "1207                                              hmmmm  L2889   \n",
       "1208                                             pretty  L2888   \n",
       "1209                     super said hed seen didnt live  L2887   \n",
       "\n",
       "     conversation_id reply_to  msg_length  word_count  sentiment  \\\n",
       "1200           L2895    L2896          99          12   0.100000   \n",
       "1201           L2895    L2895          35           5   0.000000   \n",
       "1202           L2895     None         150          21  -0.166667   \n",
       "1203           L2892    L2892         106          17   0.250000   \n",
       "1204           L2892     None          10           2   0.000000   \n",
       "1205           L2887    L2890          17           3   0.000000   \n",
       "1206           L2887    L2889          51           8   0.000000   \n",
       "1207           L2887    L2888           5           1   0.000000   \n",
       "1208           L2887    L2887           6           1   0.250000   \n",
       "1209           L2887     None          30           6   0.234848   \n",
       "\n",
       "                                                 tokens  \n",
       "1200  [czechoslovakia, slavs, fighting, germans, rus...  \n",
       "1201          [eastern, europe, like, romania, hungary]  \n",
       "1202  [maybe, ritual, thing, someone, trying, send, ...  \n",
       "1203  [look, im, even, sure, anything, saw, outside,...  \n",
       "1204                                      [would, call]  \n",
       "1205                              [says, shes, suspect]  \n",
       "1206  [maybe, dont, care, either, prettiest, suspect...  \n",
       "1207                                            [hmmmm]  \n",
       "1208                                           [pretty]  \n",
       "1209              [super, said, hed, seen, didnt, live]  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations[1200: 1210]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcb8e2e-120c-4573-9238-b5b6ca784862",
   "metadata": {},
   "source": [
    "### Lemmatize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8de3a380-ee83-4dcf-8bd5-eb62d4bf20d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>msg_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>czechoslovakia slavs fighting germans russians...</td>\n",
       "      <td>L2897</td>\n",
       "      <td>L2895</td>\n",
       "      <td>L2896</td>\n",
       "      <td>99</td>\n",
       "      <td>12</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>[czechoslovakia, slav, fighting, german, russi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>eastern europe like romania hungary</td>\n",
       "      <td>L2896</td>\n",
       "      <td>L2895</td>\n",
       "      <td>L2895</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[eastern, europe, like, romania, hungary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>maybe ritual thing someone trying send message...</td>\n",
       "      <td>L2895</td>\n",
       "      <td>L2895</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>[maybe, ritual, thing, someone, trying, send, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>look im even sure anything saw outside fire th...</td>\n",
       "      <td>L2893</td>\n",
       "      <td>L2892</td>\n",
       "      <td>L2892</td>\n",
       "      <td>106</td>\n",
       "      <td>17</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>[look, im, even, sure, anything, saw, outside,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>would call</td>\n",
       "      <td>L2892</td>\n",
       "      <td>L2892</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[would, call]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>says shes suspect</td>\n",
       "      <td>L2891</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2890</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[say, shes, suspect]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>maybe dont care either prettiest suspect ive a...</td>\n",
       "      <td>L2890</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2889</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[maybe, dont, care, either, prettiest, suspect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>hmmmm</td>\n",
       "      <td>L2889</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2888</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[hmmmm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>pretty</td>\n",
       "      <td>L2888</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2887</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>[pretty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>super said hed seen didnt live</td>\n",
       "      <td>L2887</td>\n",
       "      <td>L2887</td>\n",
       "      <td>None</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>0.234848</td>\n",
       "      <td>[super, said, hed, seen, didnt, live]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     id  \\\n",
       "1200  czechoslovakia slavs fighting germans russians...  L2897   \n",
       "1201                eastern europe like romania hungary  L2896   \n",
       "1202  maybe ritual thing someone trying send message...  L2895   \n",
       "1203  look im even sure anything saw outside fire th...  L2893   \n",
       "1204                                         would call  L2892   \n",
       "1205                                  says shes suspect  L2891   \n",
       "1206  maybe dont care either prettiest suspect ive a...  L2890   \n",
       "1207                                              hmmmm  L2889   \n",
       "1208                                             pretty  L2888   \n",
       "1209                     super said hed seen didnt live  L2887   \n",
       "\n",
       "     conversation_id reply_to  msg_length  word_count  sentiment  \\\n",
       "1200           L2895    L2896          99          12   0.100000   \n",
       "1201           L2895    L2895          35           5   0.000000   \n",
       "1202           L2895     None         150          21  -0.166667   \n",
       "1203           L2892    L2892         106          17   0.250000   \n",
       "1204           L2892     None          10           2   0.000000   \n",
       "1205           L2887    L2890          17           3   0.000000   \n",
       "1206           L2887    L2889          51           8   0.000000   \n",
       "1207           L2887    L2888           5           1   0.000000   \n",
       "1208           L2887    L2887           6           1   0.250000   \n",
       "1209           L2887     None          30           6   0.234848   \n",
       "\n",
       "                                                 tokens  \n",
       "1200  [czechoslovakia, slav, fighting, german, russi...  \n",
       "1201          [eastern, europe, like, romania, hungary]  \n",
       "1202  [maybe, ritual, thing, someone, trying, send, ...  \n",
       "1203  [look, im, even, sure, anything, saw, outside,...  \n",
       "1204                                      [would, call]  \n",
       "1205                               [say, shes, suspect]  \n",
       "1206  [maybe, dont, care, either, prettiest, suspect...  \n",
       "1207                                            [hmmmm]  \n",
       "1208                                           [pretty]  \n",
       "1209              [super, said, hed, seen, didnt, live]  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "conversations.loc[:, 'tokens'] = conversations['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "conversations[1200: 1210]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f3e9f4-e003-45df-a273-d4b4d06922da",
   "metadata": {},
   "source": [
    "## Preparing for training\n",
    "Format data to be suitable for chatbot training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
