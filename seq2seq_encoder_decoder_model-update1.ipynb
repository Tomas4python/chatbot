{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db509470-cb51-4c21-84ca-5f928073d68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Name: /device:GPU:0\n",
      "Memory Limit: 4158652416 bytes\n",
      "Description: device: 0, name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_gpu_details():\n",
    "    devices = device_lib.list_local_devices()\n",
    "    for device in devices:\n",
    "        if device.device_type == 'GPU':\n",
    "            print(f\"Device Name: {device.name}\")\n",
    "            print(f\"Memory Limit: {device.memory_limit} bytes\")\n",
    "            print(f\"Description: {device.physical_device_desc}\")\n",
    "\n",
    "get_gpu_details()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da176501-fac2-4cda-b1fa-da4a5a40e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da7e0a3-09ee-4768-a2a5-f95662ac28a6",
   "metadata": {},
   "source": [
    "## Loading the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a67cfeb-9428-4137-b946-d2d581b70af4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>tok_0_token</th>\n",
       "      <th>tok_0_tag</th>\n",
       "      <th>tok_0_dep</th>\n",
       "      <th>...</th>\n",
       "      <th>tok_121_dep</th>\n",
       "      <th>tok_122_token</th>\n",
       "      <th>tok_122_tag</th>\n",
       "      <th>tok_122_dep</th>\n",
       "      <th>tok_123_token</th>\n",
       "      <th>tok_123_tag</th>\n",
       "      <th>tok_123_dep</th>\n",
       "      <th>tok_124_token</th>\n",
       "      <th>tok_124_tag</th>\n",
       "      <th>tok_124_dep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1045</td>\n",
       "      <td>L1044</td>\n",
       "      <td>They do not!</td>\n",
       "      <td>u0</td>\n",
       "      <td>L1044</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>They</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "      <td>They do to!</td>\n",
       "      <td>u2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>They</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L985</td>\n",
       "      <td>L984</td>\n",
       "      <td>I hope so.</td>\n",
       "      <td>u0</td>\n",
       "      <td>L984</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "      <td>She okay?</td>\n",
       "      <td>u2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>She</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L925</td>\n",
       "      <td>L924</td>\n",
       "      <td>Let's go.</td>\n",
       "      <td>u0</td>\n",
       "      <td>L924</td>\n",
       "      <td>None</td>\n",
       "      <td>m0</td>\n",
       "      <td>Let</td>\n",
       "      <td>VB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 382 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id conversation_id          text speaker reply_to timestamp movie_id  \\\n",
       "0  L1045           L1044  They do not!      u0    L1044      None       m0   \n",
       "1  L1044           L1044   They do to!      u2     None      None       m0   \n",
       "2   L985            L984    I hope so.      u0     L984      None       m0   \n",
       "3   L984            L984     She okay?      u2     None      None       m0   \n",
       "4   L925            L924     Let's go.      u0     L924      None       m0   \n",
       "\n",
       "  tok_0_token tok_0_tag tok_0_dep  ... tok_121_dep tok_122_token tok_122_tag  \\\n",
       "0        They       PRP     nsubj  ...        None          None        None   \n",
       "1        They       PRP     nsubj  ...        None          None        None   \n",
       "2           I       PRP     nsubj  ...        None          None        None   \n",
       "3         She       PRP     nsubj  ...        None          None        None   \n",
       "4         Let        VB      ROOT  ...        None          None        None   \n",
       "\n",
       "  tok_122_dep tok_123_token tok_123_tag tok_123_dep tok_124_token tok_124_tag  \\\n",
       "0        None          None        None        None          None        None   \n",
       "1        None          None        None        None          None        None   \n",
       "2        None          None        None        None          None        None   \n",
       "3        None          None        None        None          None        None   \n",
       "4        None          None        None        None          None        None   \n",
       "\n",
       "  tok_124_dep  \n",
       "0        None  \n",
       "1        None  \n",
       "2        None  \n",
       "3        None  \n",
       "4        None  \n",
       "\n",
       "[5 rows x 382 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the DataFrame\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "file_path_parquet = os.path.join(data_dir, 'utterances.parquet')\n",
    "df_loaded_parquet = pd.read_parquet(file_path_parquet)\n",
    "\n",
    "df_loaded_parquet.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4870a0-f637-49e6-816f-679310801978",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d5f982-c2cb-40b8-b95c-54237d3020ca",
   "metadata": {},
   "source": [
    "### Leaving only necessary data for simple Sec2Seq model\n",
    "Id, conversation_id for tracking the flow of conversations and reply_to for understanding the sequence within the dialogue, and conversation text ofcourse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "830f8a5d-17a2-47ce-bee6-bace5e54cb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>reply_to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They do not!</td>\n",
       "      <td>L1045</td>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They do to!</td>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I hope so.</td>\n",
       "      <td>L985</td>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>She okay?</td>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Let's go.</td>\n",
       "      <td>L925</td>\n",
       "      <td>L924</td>\n",
       "      <td>L924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text     id conversation_id reply_to\n",
       "0  They do not!  L1045           L1044    L1044\n",
       "1   They do to!  L1044           L1044     None\n",
       "2    I hope so.   L985            L984     L984\n",
       "3     She okay?   L984            L984     None\n",
       "4     Let's go.   L925            L924     L924"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations = df_loaded_parquet[['text', 'id', 'conversation_id', 'reply_to']]\n",
    "conversations.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5150d2f-999d-45ad-b2f2-b9b65b883e89",
   "metadata": {},
   "source": [
    "## Create prepocessing functions for initial text and later response generation preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6648d492-f4e0-40a8-a3ff-012fa0a0d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "931e5fe2-f6ee-4399-b2cd-e688034a3a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4489e424-9de7-42e8-9d3b-1f6e842ea28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')  # Tokenizer\n",
    "nltk.download('wordnet')  # Lemmatizer\n",
    "nltk.download('stopwords')  # Stopwords\n",
    "nltk.download('omw-1.4') # Ensures multilingual contexts\n",
    "\n",
    "# Stopwords list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "initial_preprocessing = True\n",
    "\n",
    "# Load spaCy's English NLP model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0e8f237-5274-43ee-8ac2-964dd9214ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    # Normalize Unicode string to NFKD form, remove non-ASCII characters, and then decode it back to a UTF-8 string\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Add a space before any punctuation mark (., !, or ?)\n",
    "    text = re.sub(r\"([.!?])\", r\" \\1\", text)\n",
    "    # Handle contractions correctly by not adding space before apostrophe\n",
    "    text = re.sub(r\"(\\b\\w+)'(d|s|t|ll|ve|re)\", r\"\\1'\\2\", text)\n",
    "    # Replace any sequence of characters that are not letters, keep basic punctuation\n",
    "    text = re.sub(r\"[^a-z.,'!? ]\", ' ', text)\n",
    "    # Replace any sequence of whitespace characters with a single space and remove leading and trailing whitespace\n",
    "    text = re.sub(r\"\\s+\", r\" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_names(text: str) -> str:\n",
    "    # Use spaCy to detect and remove names from the text\n",
    "    doc = nlp(text)\n",
    "    filtered_text = ' '.join([token.text for token in doc if token.ent_type_ != 'PERSON']) # Takes really long time, exlude from chatbot input preprocessing\n",
    "    return filtered_text\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # Normalize text\n",
    "    text = normalize_text(text)\n",
    "    # Remove names using spaCy's NER\n",
    "    if initial_preprocessing:\n",
    "        text = remove_names(text)\n",
    "    # # Remove punctuation\n",
    "    # text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stopwords and tokenize\n",
    "    # words = word_tokenize(text) # More intelligent splitting\n",
    "    # filtered_words = [word for word in words if word not in stop_words]\n",
    "    # # Lemmatize words\n",
    "    # lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    # Add <SOS> and <EOS> tokens, and join the list into a single string\n",
    "    # return ' '.join(['sofs'] + lemmatized_words + ['eofs'])\n",
    "    return 'sofs ' + text + ' eofs' # Chosen ['sofs', 'eofs'] because tokenizer removes everthing what is in <> or || and are not in dataset vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d54043-f92c-487e-92cf-51258502844a",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56f48954-ee6f-45e9-9c64-aad22ba14caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomui\\AppData\\Local\\Temp\\ipykernel_15576\\772610508.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conversations['preprocessed_text'] = conversations['text'].apply(preprocess_text)\n"
     ]
    }
   ],
   "source": [
    "# Apply the preprocessing function to each row in the 'text' column\n",
    "conversations['preprocessed_text'] = conversations['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "194f30d2-b581-4171-9ab2-e4d13edd042f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>Why'd you help me back there with the Chief?  ...</td>\n",
       "      <td>L3229</td>\n",
       "      <td>L3229</td>\n",
       "      <td>None</td>\n",
       "      <td>sofs why 'd you help me back there with the ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>How you go out on a limb for somebody is by gi...</td>\n",
       "      <td>L3228</td>\n",
       "      <td>L3223</td>\n",
       "      <td>L3227</td>\n",
       "      <td>sofs how you go out on a limb for somebody is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>But, I mean, didn't you ever go out on a limb ...</td>\n",
       "      <td>L3227</td>\n",
       "      <td>L3223</td>\n",
       "      <td>L3226</td>\n",
       "      <td>sofs but , i mean , did n't you ever go out on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>Well, it's not up to you to decide whether she...</td>\n",
       "      <td>L3226</td>\n",
       "      <td>L3223</td>\n",
       "      <td>L3225</td>\n",
       "      <td>sofs well , it 's not up to you to decide whet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>I told you, you know, I thought I was doing th...</td>\n",
       "      <td>L3225</td>\n",
       "      <td>L3223</td>\n",
       "      <td>L3224</td>\n",
       "      <td>sofs i told you , you know , i thought i was d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>No, I don't think you were a fool, I just thin...</td>\n",
       "      <td>L3224</td>\n",
       "      <td>L3223</td>\n",
       "      <td>L3223</td>\n",
       "      <td>sofs no , i do n't think you were a fool , i j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>Yeah, just her in the shower.  Nothing happene...</td>\n",
       "      <td>L3223</td>\n",
       "      <td>L3223</td>\n",
       "      <td>None</td>\n",
       "      <td>sofs yeah , just her in the shower . nothing h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>Just a shower?</td>\n",
       "      <td>L3222</td>\n",
       "      <td>L3219</td>\n",
       "      <td>L3221</td>\n",
       "      <td>sofs just a shower ? eofs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>I took her there for a shower and that's it.</td>\n",
       "      <td>L3221</td>\n",
       "      <td>L3219</td>\n",
       "      <td>L3220</td>\n",
       "      <td>sofs i took her there for a shower and that 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>Well, you shoulda because nobody's gonna belie...</td>\n",
       "      <td>L3220</td>\n",
       "      <td>L3219</td>\n",
       "      <td>L3219</td>\n",
       "      <td>sofs well , you shoulda because nobody 's gon ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     id  \\\n",
       "1140  Why'd you help me back there with the Chief?  ...  L3229   \n",
       "1141  How you go out on a limb for somebody is by gi...  L3228   \n",
       "1142  But, I mean, didn't you ever go out on a limb ...  L3227   \n",
       "1143  Well, it's not up to you to decide whether she...  L3226   \n",
       "1144  I told you, you know, I thought I was doing th...  L3225   \n",
       "1145  No, I don't think you were a fool, I just thin...  L3224   \n",
       "1146  Yeah, just her in the shower.  Nothing happene...  L3223   \n",
       "1147                                     Just a shower?  L3222   \n",
       "1148       I took her there for a shower and that's it.  L3221   \n",
       "1149  Well, you shoulda because nobody's gonna belie...  L3220   \n",
       "\n",
       "     conversation_id reply_to  \\\n",
       "1140           L3229     None   \n",
       "1141           L3223    L3227   \n",
       "1142           L3223    L3226   \n",
       "1143           L3223    L3225   \n",
       "1144           L3223    L3224   \n",
       "1145           L3223    L3223   \n",
       "1146           L3223     None   \n",
       "1147           L3219    L3221   \n",
       "1148           L3219    L3220   \n",
       "1149           L3219    L3219   \n",
       "\n",
       "                                      preprocessed_text  \n",
       "1140  sofs why 'd you help me back there with the ch...  \n",
       "1141  sofs how you go out on a limb for somebody is ...  \n",
       "1142  sofs but , i mean , did n't you ever go out on...  \n",
       "1143  sofs well , it 's not up to you to decide whet...  \n",
       "1144  sofs i told you , you know , i thought i was d...  \n",
       "1145  sofs no , i do n't think you were a fool , i j...  \n",
       "1146  sofs yeah , just her in the shower . nothing h...  \n",
       "1147                          sofs just a shower ? eofs  \n",
       "1148  sofs i took her there for a shower and that 's...  \n",
       "1149  sofs well , you shoulda because nobody 's gon ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations[1140: 1150]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a48802-4be9-4ff0-a40e-f3875b3b2502",
   "metadata": {},
   "source": [
    "## Saving the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a0af574-6748-467b-8ba6-4da51794c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e4e2a98-f1d4-4f72-b95e-6a75f457e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0fc4148-3397-41bc-abd0-6fcee5150134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving the DataFrame\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "file_path_parquet = os.path.join(data_dir, 'preprocessed_s2s.parquet')\n",
    "conversations.to_parquet(file_path_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa83ea5-56e5-4411-8b15-aa9dd738e2bb",
   "metadata": {},
   "source": [
    "## Loading the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d5288d3-344d-4036-b8ab-1a6641f76150",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the DataFrame\n",
    "file_path_parquet = os.path.join(data_dir, 'preprocessed_s2s.parquet')\n",
    "conversations = pd.read_parquet(file_path_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02ebe768-9135-4cd1-a662-792d7465d98e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>Why'd you help me back there with the Chief?  ...</td>\n",
       "      <td>L3229</td>\n",
       "      <td>L3229</td>\n",
       "      <td>None</td>\n",
       "      <td>sofs why 'd you help me back there with the ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>How you go out on a limb for somebody is by gi...</td>\n",
       "      <td>L3228</td>\n",
       "      <td>L3223</td>\n",
       "      <td>L3227</td>\n",
       "      <td>sofs how you go out on a limb for somebody is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>But, I mean, didn't you ever go out on a limb ...</td>\n",
       "      <td>L3227</td>\n",
       "      <td>L3223</td>\n",
       "      <td>L3226</td>\n",
       "      <td>sofs but , i mean , did n't you ever go out on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>Well, it's not up to you to decide whether she...</td>\n",
       "      <td>L3226</td>\n",
       "      <td>L3223</td>\n",
       "      <td>L3225</td>\n",
       "      <td>sofs well , it 's not up to you to decide whet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>I told you, you know, I thought I was doing th...</td>\n",
       "      <td>L3225</td>\n",
       "      <td>L3223</td>\n",
       "      <td>L3224</td>\n",
       "      <td>sofs i told you , you know , i thought i was d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>No, I don't think you were a fool, I just thin...</td>\n",
       "      <td>L3224</td>\n",
       "      <td>L3223</td>\n",
       "      <td>L3223</td>\n",
       "      <td>sofs no , i do n't think you were a fool , i j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>Yeah, just her in the shower.  Nothing happene...</td>\n",
       "      <td>L3223</td>\n",
       "      <td>L3223</td>\n",
       "      <td>None</td>\n",
       "      <td>sofs yeah , just her in the shower . nothing h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>Just a shower?</td>\n",
       "      <td>L3222</td>\n",
       "      <td>L3219</td>\n",
       "      <td>L3221</td>\n",
       "      <td>sofs just a shower ? eofs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>I took her there for a shower and that's it.</td>\n",
       "      <td>L3221</td>\n",
       "      <td>L3219</td>\n",
       "      <td>L3220</td>\n",
       "      <td>sofs i took her there for a shower and that 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>Well, you shoulda because nobody's gonna belie...</td>\n",
       "      <td>L3220</td>\n",
       "      <td>L3219</td>\n",
       "      <td>L3219</td>\n",
       "      <td>sofs well , you shoulda because nobody 's gon ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     id  \\\n",
       "1140  Why'd you help me back there with the Chief?  ...  L3229   \n",
       "1141  How you go out on a limb for somebody is by gi...  L3228   \n",
       "1142  But, I mean, didn't you ever go out on a limb ...  L3227   \n",
       "1143  Well, it's not up to you to decide whether she...  L3226   \n",
       "1144  I told you, you know, I thought I was doing th...  L3225   \n",
       "1145  No, I don't think you were a fool, I just thin...  L3224   \n",
       "1146  Yeah, just her in the shower.  Nothing happene...  L3223   \n",
       "1147                                     Just a shower?  L3222   \n",
       "1148       I took her there for a shower and that's it.  L3221   \n",
       "1149  Well, you shoulda because nobody's gonna belie...  L3220   \n",
       "\n",
       "     conversation_id reply_to  \\\n",
       "1140           L3229     None   \n",
       "1141           L3223    L3227   \n",
       "1142           L3223    L3226   \n",
       "1143           L3223    L3225   \n",
       "1144           L3223    L3224   \n",
       "1145           L3223    L3223   \n",
       "1146           L3223     None   \n",
       "1147           L3219    L3221   \n",
       "1148           L3219    L3220   \n",
       "1149           L3219    L3219   \n",
       "\n",
       "                                      preprocessed_text  \n",
       "1140  sofs why 'd you help me back there with the ch...  \n",
       "1141  sofs how you go out on a limb for somebody is ...  \n",
       "1142  sofs but , i mean , did n't you ever go out on...  \n",
       "1143  sofs well , it 's not up to you to decide whet...  \n",
       "1144  sofs i told you , you know , i thought i was d...  \n",
       "1145  sofs no , i do n't think you were a fool , i j...  \n",
       "1146  sofs yeah , just her in the shower . nothing h...  \n",
       "1147                          sofs just a shower ? eofs  \n",
       "1148  sofs i took her there for a shower and that 's...  \n",
       "1149  sofs well , you shoulda because nobody 's gon ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations[1140: 1150]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ebdc30-1988-4230-8a02-e46978cff38e",
   "metadata": {},
   "source": [
    "## Initialize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5d26e21-a067-42cb-afaf-70f981f0d0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 15 most frequent words:\n",
      " [('sofs', 304713), ('eofs', 304713), ('you', 148729), ('i', 142169), ('the', 99290), ('to', 80761), ('a', 71534), (\"'s\", 66252), ('it', 66206), (\"n't\", 55106), ('do', 47246), ('that', 46706), ('and', 46114), ('of', 39474), ('what', 37876)]\n",
      "\n",
      "Last 100 words:\n",
      " [('nihilistic', 1), ('freelancing', 1), ('gatherer', 1), ('overview', 1), ('retardant', 1), ('deploys', 1), ('beastie', 1), ('ozzfest', 1), ('russkie', 1), ('shavers', 1), ('mersh', 1), ('slovo', 1), ('dawning', 1), ('tshirt', 1), ('dishonorably', 1), ('vandals', 1), ('grozny', 1), ('lamborghini', 1), ('genoa', 1), ('pizda', 1), ('filament', 1), ('replicate', 1), ('solider', 1), ('secaucus', 1), ('athletics', 1), ('herded', 1), ('wolverine', 1), ('absorbs', 1), ('definitively', 1), ('poppycock', 1), ('rumous', 1), ('disinfectant', 1), ('celery', 1), ('cerebrum', 1), ('unashamedly', 1), ('dien', 1), ('gerhart', 1), ('mending', 1), ('galvanism', 1), ('equalize', 1), ('cerebrospinal', 1), ('madein', 1), ('froderick', 1), ('blindingly', 1), ('desserts', 1), ('impossibilities', 1), ('recesses', 1), ('ascend', 1), ('thunders', 1), ('kites', 1), ('cookbooks', 1), ('diagnostician', 1), ('sockers', 1), ('fockers', 1), ('fredereck', 1), ('frodereck', 1), ('fronkon', 1), ('ereck', 1), ('dereck', 1), ('mmmmmmmmmm', 1), ('mmmmmmmmmmmmmmmmmmmmmmmmm', 1), ('pah', 1), ('trouper', 1), (\"diff'rent\", 1), ('mmmmmmmmmmm', 1), ('mmmmmmmmmmmmmnnnnnnmmmmmmmm', 1), ('transylvanian', 1), ('apfelstrudel', 1), ('mmmmmmmmmmmmmmm', 1), ('fuchsmachen', 1), ('liebe', 1), ('danc', 1), ('ul', 1), ('tra', 1), ('sacral', 1), ('minuteness', 1), ('schwanzstucker', 1), ('grrrhmmnnnjkjmmmnn', 1), ('roughhousing', 1), ('foooooood', 1), ('dooty', 1), ('lifebuoy', 1), ('saij', 1), ('peifecl', 1), ('meseif', 1), ('wellington', 1), ('lorj', 1), ('cetshwayo', 1), ('manoeuvre', 1), ('indeedldid', 1), ('mylord', 1), ('itwas', 1), ('ofthe', 1), ('adc', 1), ('ofnatal', 1), ('subaltern', 1), ('horsemanship', 1), ('splendil', 1), ('impi', 1), ('basutos', 1)]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from collections import OrderedDict\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(conversations['preprocessed_text'])  # <SOS> and <EOS> == sofs an eofs  == <start> and <end>\n",
    "\n",
    "# Sort the word_counts dictionary by frequency in descending order\n",
    "sorted_word_counts = OrderedDict(sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "print(f\"\\nTop 15 most frequent words:\\n {list(sorted_word_counts.items())[:15]}\")\n",
    "print(f\"\\nLast 100 words:\\n {list(sorted_word_counts.items())[-100:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e03796-546e-434a-8084-dbd98831c883",
   "metadata": {},
   "source": [
    "## Filter rare words - 10000 vocabulary OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09cbeacd-4976-484b-bf43-4679d99b4546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words before filtering: 47579\n",
      "Total words after filtering: 10338\n",
      "\n",
      "Top 15 most frequent words:\n",
      " [('sofs', 304713), ('eofs', 304713), ('you', 148729), ('i', 142169), ('the', 99290), ('to', 80761), ('a', 71534), (\"'s\", 66252), ('it', 66206), (\"n't\", 55106), ('do', 47246), ('that', 46706), ('and', 46114), ('of', 39474), ('what', 37876)]\n",
      "\n",
      "Last 100 words:\n",
      " [('warmed', 10), ('misty', 10), ('deviant', 10), ('rollo', 10), ('checkbook', 10), ('shelby', 10), ('cooperating', 10), ('puppets', 10), ('venza', 10), ('grable', 10), ('overwhelmed', 10), ('temporal', 10), ('beamed', 10), ('deflector', 10), ('cloaked', 10), ('antiques', 10), ('inoperative', 10), ('gracie', 10), (\"ba'ku\", 10), ('riker', 10), ('silo', 10), ('enthusiastic', 10), ('dispose', 10), ('endings', 10), ('signatures', 10), ('zander', 10), ('stirred', 10), ('sax', 10), ('roma', 10), ('commune', 10), ('eyelash', 10), ('georgina', 10), ('magneto', 10), ('dade', 10), ('throats', 10), ('productive', 10), ('kuato', 10), ('bixby', 10), ('dowd', 10), ('meth', 10), ('admittance', 10), ('anesthesiology', 10), ('disallowed', 10), ('rembrandt', 10), ('folded', 10), ('kung', 10), ('reserves', 10), ('terrence', 10), ('passwords', 10), ('tsch', 10), ('rem', 10), ('clair', 10), ('skye', 10), ('sol', 10), ('wand', 10), ('shemp', 10), ('mills', 10), ('garland', 10), ('beery', 10), ('wilfred', 10), ('pablo', 10), ('algeria', 10), ('vial', 10), ('brock', 10), ('timers', 10), ('randal', 10), ('waterfront', 10), ('nikko', 10), ('tucked', 10), ('vegetarian', 10), ('claremont', 10), ('gurney', 10), ('bracken', 10), ('tita', 10), ('daryl', 10), ('quota', 10), ('rennie', 10), ('brumby', 10), ('graff', 10), ('reign', 10), ('gregoire', 10), ('muncie', 10), ('coz', 10), ('dalai', 10), ('sixpack', 10), ('marvosa', 10), ('parrish', 10), ('noc', 10), ('shrubbery', 10), ('snyder', 10), ('mercutio', 10), ('burbage', 10), ('constable', 10), ('kastle', 10), ('teschmacher', 10), ('ottos', 10), ('helmuth', 10), ('hinkel', 10), ('clo', 10), ('raziel', 10)]\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Set a frequency threshold\n",
    "threshold = 10\n",
    "\n",
    "# Filter out rare words\n",
    "filtered_words = {word: count for word, count in sorted_word_counts.items() if count >= threshold}\n",
    "\n",
    "# Display the number of words before and after filtering\n",
    "print(f\"Total words before filtering: {len(sorted_word_counts)}\")\n",
    "print(f\"Total words after filtering: {len(filtered_words)}\")\n",
    "# Display the sorted word counts\n",
    "print(f\"\\nTop 15 most frequent words:\\n {list(filtered_words.items())[:15]}\")\n",
    "print(f\"\\nLast 100 words:\\n {list(filtered_words.items())[-100:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6028c735-d367-4027-a079-0aef5fffe556",
   "metadata": {},
   "source": [
    "## Save the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0b8d210-bea5-4d43-a822-105eebdaeebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved to C:\\Users\\tomui\\Desktop\\capstone_project\\data\\tokenizer.pickle\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Determine the directory where the tokenizer will be saved\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Save the tokenizer using pickle\n",
    "tokenizer_path = os.path.join(data_dir, 'tokenizer.pickle')\n",
    "with open(tokenizer_path, 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(f\"Tokenizer saved to {tokenizer_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b360f4-d0b8-423e-9405-6fb5f593f794",
   "metadata": {},
   "source": [
    "## Load the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22258bb1-9e70-45b1-9e11-e5082f64f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from file\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "tokenizer_path = os.path.join(data_dir, 'tokenizer.pickle')\n",
    "with open(tokenizer_path, 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75357d07-db84-4cef-87d6-84b6238b2208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index['sofs'], tokenizer.word_index['eofs']) # Checking if <start> and <end> tokens are in index (vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d0558-1fec-450f-b5d7-7f4bb4bfb150",
   "metadata": {},
   "source": [
    "## Pairing messages - input with responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92f8d5e7-8194-4470-935d-f73dfeb25de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the DataFrame with itself to form pairs\n",
    "pairs = pd.merge(\n",
    "    conversations, conversations,\n",
    "    left_on='id',\n",
    "    right_on='reply_to',\n",
    "    suffixes=('_input', '_response')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35930c1b-f970-48cb-8eb1-b11457a0de85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_input</th>\n",
       "      <th>id_input</th>\n",
       "      <th>conversation_id_input</th>\n",
       "      <th>reply_to_input</th>\n",
       "      <th>preprocessed_text_input</th>\n",
       "      <th>text_response</th>\n",
       "      <th>id_response</th>\n",
       "      <th>conversation_id_response</th>\n",
       "      <th>reply_to_response</th>\n",
       "      <th>preprocessed_text_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They do to!</td>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "      <td>None</td>\n",
       "      <td>sofs they do to ! eofs</td>\n",
       "      <td>They do not!</td>\n",
       "      <td>L1045</td>\n",
       "      <td>L1044</td>\n",
       "      <td>L1044</td>\n",
       "      <td>sofs they do not ! eofs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She okay?</td>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "      <td>None</td>\n",
       "      <td>sofs she okay ? eofs</td>\n",
       "      <td>I hope so.</td>\n",
       "      <td>L985</td>\n",
       "      <td>L984</td>\n",
       "      <td>L984</td>\n",
       "      <td>sofs i hope so . eofs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wow</td>\n",
       "      <td>L924</td>\n",
       "      <td>L924</td>\n",
       "      <td>None</td>\n",
       "      <td>sofs wow eofs</td>\n",
       "      <td>Let's go.</td>\n",
       "      <td>L925</td>\n",
       "      <td>L924</td>\n",
       "      <td>L924</td>\n",
       "      <td>sofs let 's go . eofs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>L871</td>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "      <td>sofs no eofs</td>\n",
       "      <td>Okay -- you're gonna need to learn how to lie.</td>\n",
       "      <td>L872</td>\n",
       "      <td>L870</td>\n",
       "      <td>L871</td>\n",
       "      <td>sofs okay you 're gon na need to learn how to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm kidding.  You know how sometimes you just ...</td>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "      <td>None</td>\n",
       "      <td>sofs i 'm kidding . you know how sometimes you...</td>\n",
       "      <td>No</td>\n",
       "      <td>L871</td>\n",
       "      <td>L870</td>\n",
       "      <td>L870</td>\n",
       "      <td>sofs no eofs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_input id_input  \\\n",
       "0                                        They do to!    L1044   \n",
       "1                                          She okay?     L984   \n",
       "2                                                Wow     L924   \n",
       "3                                                 No     L871   \n",
       "4  I'm kidding.  You know how sometimes you just ...     L870   \n",
       "\n",
       "  conversation_id_input reply_to_input  \\\n",
       "0                 L1044           None   \n",
       "1                  L984           None   \n",
       "2                  L924           None   \n",
       "3                  L870           L870   \n",
       "4                  L870           None   \n",
       "\n",
       "                             preprocessed_text_input  \\\n",
       "0                             sofs they do to ! eofs   \n",
       "1                               sofs she okay ? eofs   \n",
       "2                                      sofs wow eofs   \n",
       "3                                       sofs no eofs   \n",
       "4  sofs i 'm kidding . you know how sometimes you...   \n",
       "\n",
       "                                    text_response id_response  \\\n",
       "0                                    They do not!       L1045   \n",
       "1                                      I hope so.        L985   \n",
       "2                                       Let's go.        L925   \n",
       "3  Okay -- you're gonna need to learn how to lie.        L872   \n",
       "4                                              No        L871   \n",
       "\n",
       "  conversation_id_response reply_to_response  \\\n",
       "0                    L1044             L1044   \n",
       "1                     L984              L984   \n",
       "2                     L924              L924   \n",
       "3                     L870              L871   \n",
       "4                     L870              L870   \n",
       "\n",
       "                          preprocessed_text_response  \n",
       "0                            sofs they do not ! eofs  \n",
       "1                              sofs i hope so . eofs  \n",
       "2                              sofs let 's go . eofs  \n",
       "3  sofs okay you 're gon na need to learn how to ...  \n",
       "4                                       sofs no eofs  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52d11b65-699e-466f-bfe3-8bdb6dd9ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the needed columns including IDs\n",
    "training_data = pairs[['id_input', 'text_input', 'preprocessed_text_input', 'id_response', 'text_response', 'preprocessed_text_response']]\n",
    "\n",
    "# Renaming columns for clarity\n",
    "training_data.columns = ['ID_Input', 'Original_Text_Input', 'Text_Input', 'ID_Response', 'Original_Text_Response', 'Text_Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77360369-d243-4092-be7b-f766ddcd249b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Input</th>\n",
       "      <th>Original_Text_Input</th>\n",
       "      <th>Text_Input</th>\n",
       "      <th>ID_Response</th>\n",
       "      <th>Original_Text_Response</th>\n",
       "      <th>Text_Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1044</td>\n",
       "      <td>They do to!</td>\n",
       "      <td>sofs they do to ! eofs</td>\n",
       "      <td>L1045</td>\n",
       "      <td>They do not!</td>\n",
       "      <td>sofs they do not ! eofs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L984</td>\n",
       "      <td>She okay?</td>\n",
       "      <td>sofs she okay ? eofs</td>\n",
       "      <td>L985</td>\n",
       "      <td>I hope so.</td>\n",
       "      <td>sofs i hope so . eofs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L924</td>\n",
       "      <td>Wow</td>\n",
       "      <td>sofs wow eofs</td>\n",
       "      <td>L925</td>\n",
       "      <td>Let's go.</td>\n",
       "      <td>sofs let 's go . eofs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L871</td>\n",
       "      <td>No</td>\n",
       "      <td>sofs no eofs</td>\n",
       "      <td>L872</td>\n",
       "      <td>Okay -- you're gonna need to learn how to lie.</td>\n",
       "      <td>sofs okay you 're gon na need to learn how to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L870</td>\n",
       "      <td>I'm kidding.  You know how sometimes you just ...</td>\n",
       "      <td>sofs i 'm kidding . you know how sometimes you...</td>\n",
       "      <td>L871</td>\n",
       "      <td>No</td>\n",
       "      <td>sofs no eofs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221611</th>\n",
       "      <td>L666520</td>\n",
       "      <td>Well I assure you, Sir, I have no desire to cr...</td>\n",
       "      <td>sofs well i assure you , sir , i have no desir...</td>\n",
       "      <td>L666521</td>\n",
       "      <td>And I assure you, you do not In fact I'd be ob...</td>\n",
       "      <td>sofs and i assure you , you do not in fact i '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221612</th>\n",
       "      <td>L666371</td>\n",
       "      <td>Lord Chelmsford seems to want me to stay back ...</td>\n",
       "      <td>sofs lord chelmsford seems to want me to stay ...</td>\n",
       "      <td>L666372</td>\n",
       "      <td>I think Chelmsford wants a good man on the bor...</td>\n",
       "      <td>sofs i think chelmsford wants a good man on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221613</th>\n",
       "      <td>L666370</td>\n",
       "      <td>I'm to take the Sikali with the main column to...</td>\n",
       "      <td>sofs i 'm to take the sikali with the main col...</td>\n",
       "      <td>L666371</td>\n",
       "      <td>Lord Chelmsford seems to want me to stay back ...</td>\n",
       "      <td>sofs lord chelmsford seems to want me to stay ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221614</th>\n",
       "      <td>L666369</td>\n",
       "      <td>Your orders, Mr Vereker?</td>\n",
       "      <td>sofs your orders , mr vereker ? eofs</td>\n",
       "      <td>L666370</td>\n",
       "      <td>I'm to take the Sikali with the main column to...</td>\n",
       "      <td>sofs i 'm to take the sikali with the main col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221615</th>\n",
       "      <td>L666256</td>\n",
       "      <td>Colonel Durnford... William Vereker. I hear yo...</td>\n",
       "      <td>sofs colonel durnford . . . . i hear you ' ve ...</td>\n",
       "      <td>L666257</td>\n",
       "      <td>Good ones, yes, Mr Vereker. Gentlemen who can ...</td>\n",
       "      <td>sofs good ones , yes , mr vereker . gentlemen ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>221616 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID_Input                                Original_Text_Input  \\\n",
       "0         L1044                                        They do to!   \n",
       "1          L984                                          She okay?   \n",
       "2          L924                                                Wow   \n",
       "3          L871                                                 No   \n",
       "4          L870  I'm kidding.  You know how sometimes you just ...   \n",
       "...         ...                                                ...   \n",
       "221611  L666520  Well I assure you, Sir, I have no desire to cr...   \n",
       "221612  L666371  Lord Chelmsford seems to want me to stay back ...   \n",
       "221613  L666370  I'm to take the Sikali with the main column to...   \n",
       "221614  L666369                           Your orders, Mr Vereker?   \n",
       "221615  L666256  Colonel Durnford... William Vereker. I hear yo...   \n",
       "\n",
       "                                               Text_Input ID_Response  \\\n",
       "0                                  sofs they do to ! eofs       L1045   \n",
       "1                                    sofs she okay ? eofs        L985   \n",
       "2                                           sofs wow eofs        L925   \n",
       "3                                            sofs no eofs        L872   \n",
       "4       sofs i 'm kidding . you know how sometimes you...        L871   \n",
       "...                                                   ...         ...   \n",
       "221611  sofs well i assure you , sir , i have no desir...     L666521   \n",
       "221612  sofs lord chelmsford seems to want me to stay ...     L666372   \n",
       "221613  sofs i 'm to take the sikali with the main col...     L666371   \n",
       "221614               sofs your orders , mr vereker ? eofs     L666370   \n",
       "221615  sofs colonel durnford . . . . i hear you ' ve ...     L666257   \n",
       "\n",
       "                                   Original_Text_Response  \\\n",
       "0                                            They do not!   \n",
       "1                                              I hope so.   \n",
       "2                                               Let's go.   \n",
       "3          Okay -- you're gonna need to learn how to lie.   \n",
       "4                                                      No   \n",
       "...                                                   ...   \n",
       "221611  And I assure you, you do not In fact I'd be ob...   \n",
       "221612  I think Chelmsford wants a good man on the bor...   \n",
       "221613  Lord Chelmsford seems to want me to stay back ...   \n",
       "221614  I'm to take the Sikali with the main column to...   \n",
       "221615  Good ones, yes, Mr Vereker. Gentlemen who can ...   \n",
       "\n",
       "                                            Text_Response  \n",
       "0                                 sofs they do not ! eofs  \n",
       "1                                   sofs i hope so . eofs  \n",
       "2                                   sofs let 's go . eofs  \n",
       "3       sofs okay you 're gon na need to learn how to ...  \n",
       "4                                            sofs no eofs  \n",
       "...                                                   ...  \n",
       "221611  sofs and i assure you , you do not in fact i '...  \n",
       "221612  sofs i think chelmsford wants a good man on th...  \n",
       "221613  sofs lord chelmsford seems to want me to stay ...  \n",
       "221614  sofs i 'm to take the sikali with the main col...  \n",
       "221615  sofs good ones , yes , mr vereker . gentlemen ...  \n",
       "\n",
       "[221616 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5788cd03-4935-46a7-ba5b-d5b1db589dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 221616 entries, 0 to 221615\n",
      "Data columns (total 6 columns):\n",
      " #   Column                  Non-Null Count   Dtype \n",
      "---  ------                  --------------   ----- \n",
      " 0   ID_Input                221616 non-null  object\n",
      " 1   Original_Text_Input     221616 non-null  object\n",
      " 2   Text_Input              221616 non-null  object\n",
      " 3   ID_Response             221616 non-null  object\n",
      " 4   Original_Text_Response  221616 non-null  object\n",
      " 5   Text_Response           221616 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 10.1+ MB\n"
     ]
    }
   ],
   "source": [
    "training_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b27b835-1776-4f74-8ad3-f95ef61a2bb5",
   "metadata": {},
   "source": [
    "## Variables for configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e7db9ee-0add-4ad6-8956-553e60ba5a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 15 # Variable for padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da43088-9b1a-43f4-968c-ae195c5b65fe",
   "metadata": {},
   "source": [
    "## Converting to indices and input-output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43711765-e472-4819-b990-cc48f5499a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomui\\AppData\\Local\\Temp\\ipykernel_15576\\1992420447.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data['Padded_Input_Sequences'] = list(map(np.array, input_padded))\n",
      "C:\\Users\\tomui\\AppData\\Local\\Temp\\ipykernel_15576\\1992420447.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data['Padded_Target_Sequences'] = list(map(np.array, target_padded))\n"
     ]
    }
   ],
   "source": [
    "# Convert texts to sequences\n",
    "input_sequences = tokenizer.texts_to_sequences(training_data['Text_Input'])\n",
    "target_sequences = tokenizer.texts_to_sequences(training_data['Text_Response'])\n",
    "\n",
    "# Pad sequences with pre-padding and truncating\n",
    "input_padded = pad_sequences(input_sequences, maxlen=max_length, padding='pre', truncating='post')\n",
    "target_padded = pad_sequences(target_sequences, maxlen=max_length, padding='pre', truncating='post')\n",
    "\n",
    "# Store numpy arrays directly in the DataFrame\n",
    "training_data['Padded_Input_Sequences'] = list(map(np.array, input_padded))\n",
    "training_data['Padded_Target_Sequences'] = list(map(np.array, target_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90513ebe-543c-44c8-9eb5-21d9a7bdc9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Input</th>\n",
       "      <th>Original_Text_Input</th>\n",
       "      <th>Text_Input</th>\n",
       "      <th>ID_Response</th>\n",
       "      <th>Original_Text_Response</th>\n",
       "      <th>Text_Response</th>\n",
       "      <th>Padded_Input_Sequences</th>\n",
       "      <th>Padded_Target_Sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1044</td>\n",
       "      <td>They do to!</td>\n",
       "      <td>sofs they do to ! eofs</td>\n",
       "      <td>L1045</td>\n",
       "      <td>They do not!</td>\n",
       "      <td>sofs they do not ! eofs</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 37, 11, 6, 2]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 37, 11, 31, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L984</td>\n",
       "      <td>She okay?</td>\n",
       "      <td>sofs she okay ? eofs</td>\n",
       "      <td>L985</td>\n",
       "      <td>I hope so.</td>\n",
       "      <td>sofs i hope so . eofs</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 51, 111, 2]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 347, 46, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L924</td>\n",
       "      <td>Wow</td>\n",
       "      <td>sofs wow eofs</td>\n",
       "      <td>L925</td>\n",
       "      <td>Let's go.</td>\n",
       "      <td>sofs let 's go . eofs</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 897, 2]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 95, 8, 63, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L871</td>\n",
       "      <td>No</td>\n",
       "      <td>sofs no eofs</td>\n",
       "      <td>L872</td>\n",
       "      <td>Okay -- you're gonna need to learn how to lie.</td>\n",
       "      <td>sofs okay you 're gon na need to learn how to ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 32, 2]</td>\n",
       "      <td>[0, 0, 1, 111, 3, 26, 118, 117, 129, 6, 650, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L870</td>\n",
       "      <td>I'm kidding.  You know how sometimes you just ...</td>\n",
       "      <td>sofs i 'm kidding . you know how sometimes you...</td>\n",
       "      <td>L871</td>\n",
       "      <td>No</td>\n",
       "      <td>sofs no eofs</td>\n",
       "      <td>[1, 4, 24, 671, 3, 25, 55, 464, 3, 38, 711, 21...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 32, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221611</th>\n",
       "      <td>L666520</td>\n",
       "      <td>Well I assure you, Sir, I have no desire to cr...</td>\n",
       "      <td>sofs well i assure you , sir , i have no desir...</td>\n",
       "      <td>L666521</td>\n",
       "      <td>And I assure you, you do not In fact I'd be ob...</td>\n",
       "      <td>sofs and i assure you , you do not in fact i '...</td>\n",
       "      <td>[0, 1, 65, 4, 2406, 3, 151, 4, 23, 32, 2040, 6...</td>\n",
       "      <td>[1, 13, 4, 2406, 3, 3, 11, 31, 16, 517, 4, 80,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221612</th>\n",
       "      <td>L666371</td>\n",
       "      <td>Lord Chelmsford seems to want me to stay back ...</td>\n",
       "      <td>sofs lord chelmsford seems to want me to stay ...</td>\n",
       "      <td>L666372</td>\n",
       "      <td>I think Chelmsford wants a good man on the bor...</td>\n",
       "      <td>sofs i think chelmsford wants a good man on th...</td>\n",
       "      <td>[0, 0, 0, 1, 757, 539, 6, 56, 17, 6, 244, 94, ...</td>\n",
       "      <td>[1, 4, 58, 334, 7, 74, 100, 30, 5, 2433, 68, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221613</th>\n",
       "      <td>L666370</td>\n",
       "      <td>I'm to take the Sikali with the main column to...</td>\n",
       "      <td>sofs i 'm to take the sikali with the main col...</td>\n",
       "      <td>L666371</td>\n",
       "      <td>Lord Chelmsford seems to want me to stay back ...</td>\n",
       "      <td>sofs lord chelmsford seems to want me to stay ...</td>\n",
       "      <td>[0, 1, 4, 24, 6, 103, 5, 36, 5, 1473, 3579, 6,...</td>\n",
       "      <td>[0, 0, 0, 1, 757, 539, 6, 56, 17, 6, 244, 94, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221614</th>\n",
       "      <td>L666369</td>\n",
       "      <td>Your orders, Mr Vereker?</td>\n",
       "      <td>sofs your orders , mr vereker ? eofs</td>\n",
       "      <td>L666370</td>\n",
       "      <td>I'm to take the Sikali with the main column to...</td>\n",
       "      <td>sofs i 'm to take the sikali with the main col...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 28, 1074, 13...</td>\n",
       "      <td>[0, 1, 4, 24, 6, 103, 5, 36, 5, 1473, 3579, 6,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221615</th>\n",
       "      <td>L666256</td>\n",
       "      <td>Colonel Durnford... William Vereker. I hear yo...</td>\n",
       "      <td>sofs colonel durnford . . . . i hear you ' ve ...</td>\n",
       "      <td>L666257</td>\n",
       "      <td>Good ones, yes, Mr Vereker. Gentlemen who can ...</td>\n",
       "      <td>sofs good ones , yes , mr vereker . gentlemen ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 1175, 4, 226, 3, 61, 2017, 93,...</td>\n",
       "      <td>[0, 0, 0, 1, 74, 736, 75, 134, 1048, 69, 54, 6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>221616 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID_Input                                Original_Text_Input  \\\n",
       "0         L1044                                        They do to!   \n",
       "1          L984                                          She okay?   \n",
       "2          L924                                                Wow   \n",
       "3          L871                                                 No   \n",
       "4          L870  I'm kidding.  You know how sometimes you just ...   \n",
       "...         ...                                                ...   \n",
       "221611  L666520  Well I assure you, Sir, I have no desire to cr...   \n",
       "221612  L666371  Lord Chelmsford seems to want me to stay back ...   \n",
       "221613  L666370  I'm to take the Sikali with the main column to...   \n",
       "221614  L666369                           Your orders, Mr Vereker?   \n",
       "221615  L666256  Colonel Durnford... William Vereker. I hear yo...   \n",
       "\n",
       "                                               Text_Input ID_Response  \\\n",
       "0                                  sofs they do to ! eofs       L1045   \n",
       "1                                    sofs she okay ? eofs        L985   \n",
       "2                                           sofs wow eofs        L925   \n",
       "3                                            sofs no eofs        L872   \n",
       "4       sofs i 'm kidding . you know how sometimes you...        L871   \n",
       "...                                                   ...         ...   \n",
       "221611  sofs well i assure you , sir , i have no desir...     L666521   \n",
       "221612  sofs lord chelmsford seems to want me to stay ...     L666372   \n",
       "221613  sofs i 'm to take the sikali with the main col...     L666371   \n",
       "221614               sofs your orders , mr vereker ? eofs     L666370   \n",
       "221615  sofs colonel durnford . . . . i hear you ' ve ...     L666257   \n",
       "\n",
       "                                   Original_Text_Response  \\\n",
       "0                                            They do not!   \n",
       "1                                              I hope so.   \n",
       "2                                               Let's go.   \n",
       "3          Okay -- you're gonna need to learn how to lie.   \n",
       "4                                                      No   \n",
       "...                                                   ...   \n",
       "221611  And I assure you, you do not In fact I'd be ob...   \n",
       "221612  I think Chelmsford wants a good man on the bor...   \n",
       "221613  Lord Chelmsford seems to want me to stay back ...   \n",
       "221614  I'm to take the Sikali with the main column to...   \n",
       "221615  Good ones, yes, Mr Vereker. Gentlemen who can ...   \n",
       "\n",
       "                                            Text_Response  \\\n",
       "0                                 sofs they do not ! eofs   \n",
       "1                                   sofs i hope so . eofs   \n",
       "2                                   sofs let 's go . eofs   \n",
       "3       sofs okay you 're gon na need to learn how to ...   \n",
       "4                                            sofs no eofs   \n",
       "...                                                   ...   \n",
       "221611  sofs and i assure you , you do not in fact i '...   \n",
       "221612  sofs i think chelmsford wants a good man on th...   \n",
       "221613  sofs lord chelmsford seems to want me to stay ...   \n",
       "221614  sofs i 'm to take the sikali with the main col...   \n",
       "221615  sofs good ones , yes , mr vereker . gentlemen ...   \n",
       "\n",
       "                                   Padded_Input_Sequences  \\\n",
       "0         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 37, 11, 6, 2]   \n",
       "1        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 51, 111, 2]   \n",
       "2         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 897, 2]   \n",
       "3          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 32, 2]   \n",
       "4       [1, 4, 24, 671, 3, 25, 55, 464, 3, 38, 711, 21...   \n",
       "...                                                   ...   \n",
       "221611  [0, 1, 65, 4, 2406, 3, 151, 4, 23, 32, 2040, 6...   \n",
       "221612  [0, 0, 0, 1, 757, 539, 6, 56, 17, 6, 244, 94, ...   \n",
       "221613  [0, 1, 4, 24, 6, 103, 5, 36, 5, 1473, 3579, 6,...   \n",
       "221614  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 28, 1074, 13...   \n",
       "221615  [0, 0, 0, 0, 1, 1175, 4, 226, 3, 61, 2017, 93,...   \n",
       "\n",
       "                                  Padded_Target_Sequences  \n",
       "0        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 37, 11, 31, 2]  \n",
       "1        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 347, 46, 2]  \n",
       "2         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 95, 8, 63, 2]  \n",
       "3       [0, 0, 1, 111, 3, 26, 118, 117, 129, 6, 650, 5...  \n",
       "4          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 32, 2]  \n",
       "...                                                   ...  \n",
       "221611  [1, 13, 4, 2406, 3, 3, 11, 31, 16, 517, 4, 80,...  \n",
       "221612  [1, 4, 58, 334, 7, 74, 100, 30, 5, 2433, 68, 2...  \n",
       "221613  [0, 0, 0, 1, 757, 539, 6, 56, 17, 6, 244, 94, ...  \n",
       "221614  [0, 1, 4, 24, 6, 103, 5, 36, 5, 1473, 3579, 6,...  \n",
       "221615  [0, 0, 0, 1, 74, 736, 75, 134, 1048, 69, 54, 6...  \n",
       "\n",
       "[221616 rows x 8 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06082790-3c51-47d0-9e2e-c4cd22cc6ebe",
   "metadata": {},
   "source": [
    "## Checking if conversion was successfull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1da54ad0-2522-45a8-9b20-50106969b6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: But doesn't the Son of Sam Law prevent criminals from profiting from their crimes? \n",
      "Reconstructed Text: sofs but does n't the son of law prevent criminals from from their crimes eofs\n",
      "\n",
      "Original Text: That doesn't apply to me because I'm not a criminal.  I'm not a criminal!  I wasn't convicted. \n",
      "Reconstructed Text: sofs that does n't apply to me because i 'm not a criminal i 'm\n",
      "Original Text: We're in negotiations, that's correct. \n",
      "Reconstructed Text: sofs we 're in negotiations that 's correct eofs\n",
      "\n",
      "Original Text: But doesn't the Son of Sam Law prevent criminals from profiting from their crimes? \n",
      "Reconstructed Text: sofs but does n't the son of law prevent criminals from from their crimes eofs\n",
      "Original Text: And isn't there a movie in the works about you? \n",
      "Reconstructed Text: sofs and is n't there a movie in the works about you eofs\n",
      "\n",
      "Original Text: We're in negotiations, that's correct. \n",
      "Reconstructed Text: sofs we 're in negotiations that 's correct eofs\n",
      "Original Text: Look, I'm in here.  You call this a career move? \n",
      "Reconstructed Text: sofs look i 'm in here you call this a career move eofs\n",
      "\n",
      "Original Text: And isn't there a movie in the works about you? \n",
      "Reconstructed Text: sofs and is n't there a movie in the works about you eofs\n",
      "Original Text: Permanently disrupted?  Aren't you selling paintings now for quite a lot of money?  Hasn't this 'incident' as you call it, jump started your career as an artist? \n",
      "Reconstructed Text: sofs permanently are n't you selling paintings now for quite a lot of money has\n",
      "\n",
      "Original Text: Look, I'm in here.  You call this a career move? \n",
      "Reconstructed Text: sofs look i 'm in here you call this a career move eofs\n"
     ]
    }
   ],
   "source": [
    "def sequences_to_text(sequence):\n",
    "    index_to_word = {index: word for word, index in tokenizer.word_index.items()}\n",
    "    # Directly map sequence of indices back to words\n",
    "    return ' '.join(index_to_word.get(idx, '') for idx in sequence if idx != 0)\n",
    "\n",
    "# Print original and reverse-tokenized text for entries\n",
    "for index, row in training_data[1130:1135].iterrows():\n",
    "    print(\"Original Text:\", row['Original_Text_Input'], \n",
    "          \"\\nReconstructed Text:\", sequences_to_text(row['Padded_Input_Sequences']))\n",
    "    print(\"\\nOriginal Text:\", row['Original_Text_Response'], \n",
    "          \"\\nReconstructed Text:\", sequences_to_text(row['Padded_Target_Sequences']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0fd035f-e78c-4aa8-9322-f5bd25f6b264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the needed columns including IDs\n",
    "training_data_final = training_data[['ID_Input', 'Padded_Input_Sequences', 'ID_Response', 'Padded_Target_Sequences']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a3073d-c4d0-4791-bc90-99f3ff74ef00",
   "metadata": {},
   "source": [
    "## Saving the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e6e6f6b-5b0a-4ad3-a3a3-22c25180fe74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving the DataFrame\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "file_path_parquet = os.path.join(data_dir, 'training_df_s2s.parquet')\n",
    "training_data_final.to_parquet(file_path_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec98bfa-c44a-42b5-8e62-c0dddebc67b9",
   "metadata": {},
   "source": [
    "## Loading the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e3699bc1-d5ec-41d6-ba0f-de02b3905615",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Input</th>\n",
       "      <th>Padded_Input_Sequences</th>\n",
       "      <th>ID_Response</th>\n",
       "      <th>Padded_Target_Sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1044</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 37, 11, 6, 2]</td>\n",
       "      <td>L1045</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 37, 11, 31, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L984</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 51, 111, 2]</td>\n",
       "      <td>L985</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 347, 46, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L924</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 897, 2]</td>\n",
       "      <td>L925</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 95, 8, 63, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L871</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 32, 2]</td>\n",
       "      <td>L872</td>\n",
       "      <td>[0, 0, 1, 111, 3, 26, 118, 117, 129, 6, 650, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L870</td>\n",
       "      <td>[1, 4, 24, 671, 3, 25, 55, 464, 3, 38, 711, 21...</td>\n",
       "      <td>L871</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 32, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>L868</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 224, 3, 2]</td>\n",
       "      <td>L869</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 40, 29, 885, 14, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>L867</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 15, 74, 309, 2]</td>\n",
       "      <td>L868</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 224, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>L866</td>\n",
       "      <td>[0, 0, 0, 1, 4, 772, 3, 80, 44, 6, 5, 74, 309,...</td>\n",
       "      <td>L867</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 15, 74, 309, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>L864</td>\n",
       "      <td>[0, 0, 0, 0, 1, 17, 21, 6852, 2510, 4, 24, 40,...</td>\n",
       "      <td>L865</td>\n",
       "      <td>[0, 1, 210, 197, 49, 4, 102, 6, 226, 57, 115, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>L863</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 15, 1075, 2]</td>\n",
       "      <td>L864</td>\n",
       "      <td>[0, 0, 0, 0, 1, 17, 21, 6852, 2510, 4, 24, 40,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_Input                             Padded_Input_Sequences ID_Response  \\\n",
       "0    L1044    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 37, 11, 6, 2]       L1045   \n",
       "1     L984   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 51, 111, 2]        L985   \n",
       "2     L924    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 897, 2]        L925   \n",
       "3     L871     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 32, 2]        L872   \n",
       "4     L870  [1, 4, 24, 671, 3, 25, 55, 464, 3, 38, 711, 21...        L871   \n",
       "5     L868    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 224, 3, 2]        L869   \n",
       "6     L867  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 15, 74, 309, 2]        L868   \n",
       "7     L866  [0, 0, 0, 1, 4, 772, 3, 80, 44, 6, 5, 74, 309,...        L867   \n",
       "8     L864  [0, 0, 0, 0, 1, 17, 21, 6852, 2510, 4, 24, 40,...        L865   \n",
       "9     L863  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 15, 1075, 2]        L864   \n",
       "\n",
       "                             Padded_Target_Sequences  \n",
       "0   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 37, 11, 31, 2]  \n",
       "1   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 347, 46, 2]  \n",
       "2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 95, 8, 63, 2]  \n",
       "3  [0, 0, 1, 111, 3, 26, 118, 117, 129, 6, 650, 5...  \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 32, 2]  \n",
       "5  [0, 0, 0, 0, 0, 0, 0, 0, 1, 40, 29, 885, 14, 9...  \n",
       "6    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 224, 3, 2]  \n",
       "7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 15, 74, 309, 2]  \n",
       "8  [0, 1, 210, 197, 49, 4, 102, 6, 226, 57, 115, ...  \n",
       "9  [0, 0, 0, 0, 1, 17, 21, 6852, 2510, 4, 24, 40,...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the DataFrame\n",
    "file_path_parquet = os.path.join(data_dir, 'training_df_s2s.parquet')\n",
    "training_data_final = pd.read_parquet(file_path_parquet)\n",
    "\n",
    "training_data_final.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1ca701-3079-4276-a1b2-8fdbacf72d90",
   "metadata": {},
   "source": [
    "## Checking if GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ac58279-25ab-4b1f-8ab3-45ccbcc54428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Name: /device:GPU:0\n",
      "Memory Limit: 4158652416 bytes\n",
      "Description: device: 0, name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_gpu_details():\n",
    "    devices = device_lib.list_local_devices()\n",
    "    for device in devices:\n",
    "        if device.device_type == 'GPU':\n",
    "            print(f\"Device Name: {device.name}\")\n",
    "            print(f\"Memory Limit: {device.memory_limit} bytes\")\n",
    "            print(f\"Description: {device.physical_device_desc}\")\n",
    "\n",
    "get_gpu_details()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f5e549-9e15-443d-b2ab-9e7b580b1263",
   "metadata": {},
   "source": [
    "# Encoder-decoder architecture with Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3546465-0e0b-45a7-bfba-2cb5b408d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, Concatenate, Layer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad227400-0e1d-469a-acf5-cb8c627b5eb4",
   "metadata": {},
   "source": [
    "## Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5191e6bb-9f36-4224-88a0-68eac62551c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(query_with_time_axis)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = Attention(units)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        decoder_outputs, encoder_outputs = inputs\n",
    "        context_vectors, _ = tf.map_fn(lambda x: self.attention(x[0], x[1]),\n",
    "                                       (decoder_outputs, tf.tile(tf.expand_dims(encoder_outputs, axis=1),\n",
    "                                                                 [1, tf.shape(decoder_outputs)[1], 1, 1])),\n",
    "                                       fn_output_signature=(tf.TensorSpec(shape=(None, encoder_outputs.shape[-1]), dtype=tf.float32),\n",
    "                                                            tf.TensorSpec(shape=(None, None, 1), dtype=tf.float32)))\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf33696-972b-427a-9fb4-5a755bc01ad9",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f74888f0-84ab-42c3-be58-7ead8d17f44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)        (None, None, 50)     2379000     ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " bidirectional_3 (Bidirectional  [(None, None, 512),  628736     ['embedding_6[0][0]']            \n",
      " )                               (None, 256),                                                     \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)        (None, None, 50)     2379000     ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 512)          0           ['bidirectional_3[0][1]',        \n",
      "                                                                  'bidirectional_3[0][3]']        \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 512)          0           ['bidirectional_3[0][2]',        \n",
      "                                                                  'bidirectional_3[0][4]']        \n",
      "                                                                                                  \n",
      " lstm_7 (LSTM)                  [(None, None, 512),  1153024     ['embedding_7[0][0]',            \n",
      "                                 (None, 512),                     'concatenate_9[0][0]',          \n",
      "                                 (None, 512)]                     'concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " attention_layer_3 (AttentionLa  (None, None, 512)   10271       ['lstm_7[0][0]',                 \n",
      " yer)                                                             'bidirectional_3[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, None, 1024)   0           ['attention_layer_3[0][0]',      \n",
      "                                                                  'lstm_7[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, None, 47580)  48769500    ['concatenate_11[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 55,319,531\n",
      "Trainable params: 55,319,531\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_sequences = np.array(training_data_final['Padded_Input_Sequences'].tolist())\n",
    "target_sequences = np.array(training_data_final['Padded_Target_Sequences'].tolist())\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "input_train, input_val, target_train, target_val = train_test_split(input_sequences, target_sequences, test_size=0.1, random_state=22)\n",
    "\n",
    "# Building the model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding = Embedding(vocab_size, 50, mask_zero=True)(encoder_inputs)\n",
    "encoder_lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional(\n",
    "    LSTM(256, return_state=True, return_sequences=True))(encoder_embedding)\n",
    "encoder_states = [Concatenate()([forward_h, backward_h]), Concatenate()([forward_c, backward_c])]\n",
    "encoder_outputs = encoder_lstm\n",
    "\n",
    "# Attention Mechanism\n",
    "attention_units = 10\n",
    "attention_layer = AttentionLayer(attention_units)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(vocab_size, 50, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = LSTM(512, return_sequences=True, return_state=True)\n",
    "decoder_lstm_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "# Apply attention to each time step in the decoder\n",
    "context_vectors = attention_layer([decoder_lstm_outputs, encoder_outputs])\n",
    "\n",
    "decoder_concat_input = Concatenate(axis=-1)([context_vectors, decoder_lstm_outputs])\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Main Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce04b26-b9b0-4bcf-a788-2e9fa8264b2e",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433a1252-2b3a-4bfa-8313-ecf45ff5873f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/12\n",
      "1233/3117 [==========>...................] - ETA: 55:02 - loss: 3.1457 - accuracy: 0.1371"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 12\n",
    "\n",
    "# Prepare decoder input data that just contains the start token\n",
    "decoder_input_train = np.hstack([np.zeros((target_train.shape[0], 1)), target_train[:, :-1]])\n",
    "decoder_input_val = np.hstack([np.zeros((target_val.shape[0], 1)), target_val[:, :-1]])\n",
    "\n",
    "# Ensure targets are expanded in dimension to match the output shape expected by sparse_categorical_crossentropy\n",
    "target_train_exp = np.expand_dims(target_train, -1)\n",
    "target_val_exp = np.expand_dims(target_val, -1)\n",
    "\n",
    "# Checkpoint callback\n",
    "checkpoint_filepath = 'model_checkpoint_epoch_{epoch:02d}.h5'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    save_freq='epoch',\n",
    "    period=5\n",
    ")\n",
    "\n",
    "# Fit the model using the original integer labels\n",
    "model.fit(\n",
    "    [input_train, decoder_input_train], target_train_exp,\n",
    "    validation_data=([input_val, decoder_input_val], target_val_exp),\n",
    "    epochs=epochs, batch_size=batch_size, verbose=1,\n",
    "    callbacks=[model_checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe098033-d3a7-41aa-8e97-67e5068e64de",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ea0fca-f6c8-45ca-b14f-72847d67c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "file_path_h5 = os.path.join(data_dir, 's2s_model.h5')\n",
    "model.save(file_path_h5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef4dca2-80ae-4779-8c53-658e5e426980",
   "metadata": {},
   "source": [
    "## Generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8463baba-3764-464e-8442-4af331332e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_preprocessing = False # Excepts spaCy to detect and remove names from the text\n",
    "\n",
    "def generate_response(input_text: str) -> str:\n",
    "    processed_text = preprocess_text(input_text)\n",
    "    input_seq = tokenizer.texts_to_sequences([processed_text])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "    # Get the encoder states and encoder outputs\n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_model.predict(input_seq)\n",
    "    state_h = np.concatenate([forward_h, backward_h], axis=-1)\n",
    "    state_c = np.concatenate([forward_c, backward_c], axis=-1)\n",
    "    states_value = [state_h, state_c]\n",
    "\n",
    "    # Prepare the target sequence with the start token\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = tokenizer.word_index['sofs']  # Start token index\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    tokens_generated = 0\n",
    "\n",
    "    while not stop_condition:\n",
    "        decoder_output, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        context_vector, _ = attention_layer(decoder_output, encoder_outputs)\n",
    "        decoder_output_with_context = np.concatenate([context_vector, decoder_output], axis=-1)\n",
    "        \n",
    "        sampled_token_index = np.argmax(decoder_output_with_context[0, -1, :])\n",
    "        sampled_char = tokenizer.index_word.get(sampled_token_index, '')\n",
    "\n",
    "        if sampled_token_index == tokenizer.word_index['eofs'] or tokens_generated > 10:  # Stop condition\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence += ' ' + sampled_char\n",
    "            tokens_generated += 1\n",
    "\n",
    "            target_seq = np.zeros((1, 1))\n",
    "            target_seq[0, 0] = sampled_token_index\n",
    "            states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c524f-4f67-4cb3-8ef1-324660822b28",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7328971-b016-4854-bcea-e49fffbd2336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "print(\"\\nUser:     Is she okay?\")\n",
    "print(\"Bot:          \", generate_response('she okay?'))\n",
    "print(\"-----------------------------\")\n",
    "print(\"\\nUser:     How are you feeling today?\")\n",
    "print(\"Bot:          \", generate_response('How are you feeling today?'))\n",
    "print(\"-----------------------------\")\n",
    "print(\"\\nUser:     Hi there!\")\n",
    "print(\"Bot:          \", generate_response('Hi there!'))\n",
    "print(\"-----------------------------\")\n",
    "print(\"\\nUser:     Can you tell me the weather forecast for today?\")\n",
    "print(\"Bot:          \", generate_response('Can you tell me the weather forecast for today?'))\n",
    "print(\"-----------------------------\")\n",
    "print(\"\\nUser:     I think artificial intelligence is changing the world.\")\n",
    "print(\"Bot:          \", generate_response('I think artificial intelligence is changing the world.'))\n",
    "print(\"-----------------------------\")\n",
    "print(\"\\nUser:     Any good movie recommendations?\")\n",
    "print(\"Bot:          \", generate_response('Any good movie recommendations?'))\n",
    "print(\"-----------------------------\")\n",
    "print(\"\\nUser:     What do you mean by that?\")\n",
    "print(\"Bot:          \", generate_response('What do you mean by that?'))\n",
    "print(\"-----------------------------\")\n",
    "print(\"\\nUser:     I'm feeling really sad today.\")\n",
    "print(\"Bot:          \", generate_response(\"I'm feeling really sad today.\"))\n",
    "print(\"-----------------------------\")\n",
    "print(\"\\nUser:     What are the implications of quantum computing on cybersecurity?\")\n",
    "print(\"Bot:          \", generate_response('What are the implications of quantum computing on cybersecurity?'))\n",
    "print(\"-----------------------------\")\n",
    "print(\"\\nUser:     Why did the chicken cross the road?\")\n",
    "print(\"Bot:          \", generate_response('Why did the chicken cross the road?'))\n",
    "print(\"-----------------------------\")\n",
    "print(\"\\nUser:     Can you explain the plot of The Matrix?\")\n",
    "print(\"Bot:          \", generate_response('Can you explain the plot of The Matrix?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5e598c-98e8-4336-8e47-015473ec4dde",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68f9dcc-f8b8-404b-904e-726bc55d3ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "file_path_h5 = os.path.join(data_dir, 's2s_model_2.h5')\n",
    "model.save(file_path_h5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
