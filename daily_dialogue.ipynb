{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90199c4d-e447-46b3-9c7a-e633a3c02f09",
   "metadata": {},
   "source": [
    "# TESTING SIMPLE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14260f0-4962-42e2-8527-f5189a158c76",
   "metadata": {
    "id": "b0ZiOK0MWE7d"
   },
   "source": [
    "## GPU info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63f70056-2151-4051-8974-714b3e237e43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1715927490489,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "bhvFI7ztVLRy",
    "outputId": "e4fb02b2-7ebf-4f3e-b2a8-f9f189a25c47"
   },
   "outputs": [],
   "source": [
    "# gpu_info = !nvidia-smi\n",
    "# gpu_info = '\\n'.join(gpu_info)\n",
    "# if gpu_info.find('failed') >= 0:\n",
    "#   print('Not connected to a GPU')\n",
    "# else:\n",
    "#   print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a58ec6-07dc-4261-ae09-67ab59ed3fb6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23496,
     "status": "ok",
     "timestamp": 1715927517410,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "RMth1TfbAys6",
    "outputId": "766cb4f5-edfa-4cd7-ad09-825604e4c786"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d5d74b-5e87-4198-b112-9f91b7f65d0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4075,
     "status": "ok",
     "timestamp": 1715927650106,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "db509470-cb51-4c21-84ca-5f928073d68e",
    "outputId": "95b90920-4333-45db-f52c-0f88f1ec6a5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Name: /device:GPU:0\n",
      "Memory Limit: 4158652416 bytes\n",
      "Description: device: 0, name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_gpu_details():\n",
    "    devices = device_lib.list_local_devices()\n",
    "    for device in devices:\n",
    "        if device.device_type == 'GPU':\n",
    "            print(f\"Device Name: {device.name}\")\n",
    "            print(f\"Memory Limit: {device.memory_limit} bytes\")\n",
    "            print(f\"Description: {device.physical_device_desc}\")\n",
    "\n",
    "get_gpu_details()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72de3458-7b40-491d-a66f-d8355a455243",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1715927650106,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "6648d492-f4e0-40a8-a3ff-012fa0a0d293"
   },
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfe7f8ff-d35e-416c-8252-fc19fc3570e2",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1715927650106,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "8a0af574-6748-467b-8ba6-4da51794c5b9"
   },
   "outputs": [],
   "source": [
    "# !pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50d83013-6a91-4f74-bee7-da28b5c4f085",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1715927650106,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "7e4e2a98-f1d4-4f72-b95e-6a75f457e8e8"
   },
   "outputs": [],
   "source": [
    "# !pip install fastparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0510248e-c233-485a-b707-7a7eb9856939",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a59de0-817a-4089-9a14-0b45925b6298",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 15 # Length of input and target sequences, padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c8162-54ab-45da-b4f0-2d5472decdd8",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e231807-af60-4571-be49-655d80d8eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, Concatenate, Layer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e56f5d37-9a67-4e51-b0b8-404df9d734a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1715930801381,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "y5srV6bjOpFb",
    "outputId": "b74db1e8-1afe-4348-be1a-23c150492567"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')  # Tokenizer\n",
    "nltk.download('wordnet')  # Lemmatizer\n",
    "nltk.download('stopwords')  # Stopwords\n",
    "nltk.download('omw-1.4') # Ensures multilingual contexts\n",
    "\n",
    "# Stopwords list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "initial_preprocessing = True\n",
    "\n",
    "# Load spaCy's English NLP model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b393a656-df75-4deb-af28-d8ba36f526eb",
   "metadata": {
    "id": "f5150d2f-999d-45ad-b2f2-b9b65b883e89"
   },
   "source": [
    "## Create prepocessing functions for initial text and later response generation preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75edfc9e-3560-4ab7-9992-963b8df9f537",
   "metadata": {
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1715930818326,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "f0e8f237-5274-43ee-8ac2-964dd9214ae5"
   },
   "outputs": [],
   "source": [
    "contractions = {\n",
    "    \"’\": \"'\",\n",
    "    \"‘\": \"'\",\n",
    "    \"“\": '\"',\n",
    "    \"”\": '\"',\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"n't\": \" not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"thats's\": \"that is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"that'll\": \"that will\",\n",
    "    \"there'll\": \"there will\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"ain't\": \"is not\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"daren't\": \"dare not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"usedn't\": \"used not\"\n",
    "}\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    # Normalize Unicode string to NFKD form, remove non-ASCII characters, and then decode it back to a UTF-8 string\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove spaces around apostrophes\n",
    "    text = re.sub(r\"\\s*'\\s*\", \"'\", text)\n",
    "    # Add a space before and after any punctuation mark (., !, or ?)\n",
    "    text = re.sub(r\"\\s*([.!?])\\s*\", r\" \\1 \", text)\n",
    "    # Correct contractions\n",
    "    for contraction, replacement in contractions.items():\n",
    "        text = re.sub(re.escape(contraction), replacement, text)\n",
    "    # Replace any sequence of characters that are not letters, basic punctuation\n",
    "    text = re.sub(r\"[^a-z' ]\", ' ', text) # re.sub(r\"[^a-z.,'!? ]\", ' ', text)\n",
    "    # Replace any sequence of whitespace characters with a single space and remove leading and trailing whitespace\n",
    "    text = re.sub(r\"\\s+\", ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_names(text: str) -> str:\n",
    "    # Use spaCy to detect and remove names from the text\n",
    "    doc = nlp(text)\n",
    "    filtered_text = ' '.join([token.text for token in doc if token.ent_type_ != 'PERSON']) # Takes really long time, exlude from chatbot input preprocessing\n",
    "    return filtered_text\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # Normalize text\n",
    "    text = normalize_text(text)\n",
    "    # Remove names using spaCy's NER\n",
    "    if initial_preprocessing:\n",
    "        text = remove_names(text)\n",
    "    # # Remove punctuation\n",
    "    # text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stopwords and tokenize\n",
    "    # words = word_tokenize(text) # More intelligent splitting\n",
    "    # filtered_words = [word for word in words if word not in stop_words]\n",
    "    # # Lemmatize words\n",
    "    # lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    # Add <SOS> and <EOS> tokens, and join the list into a single string\n",
    "    # return ' '.join(['sofs'] + lemmatized_words + ['eofs'])\n",
    "        # Trim the text to the desired length\n",
    "    words = text.split()[:max_length]\n",
    "    trimmed_text = ' '.join(words)  # Consider to remove trimming, if you want pad later on max length\n",
    "    return trimmed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13a57e-6fc6-48ad-8326-becd2e579083",
   "metadata": {},
   "source": [
    "### Load data\n",
    "https://huggingface.co/datasets/daily_dialog/tree/refs%2Fconvert%2Fparquet/default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17b8c9fb-fe6c-4bca-a255-45e7c9a9a831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of data_dd/0000.parquet:\n",
      "                                              dialog  \\\n",
      "0  [Say , Jim , how about going for a few beers a...   \n",
      "1  [Can you do push-ups ? ,  Of course I can . It...   \n",
      "2  [Can you study with the radio on ? ,  No , I l...   \n",
      "3  [Are you all right ? ,  I will be all right so...   \n",
      "4  [Hey John , nice skates . Are they new ? ,  Ye...   \n",
      "\n",
      "                              act                         emotion  \n",
      "0  [3, 4, 2, 2, 2, 3, 4, 1, 3, 4]  [0, 0, 0, 0, 0, 0, 4, 4, 4, 4]  \n",
      "1              [2, 1, 2, 2, 1, 1]              [0, 0, 6, 0, 0, 0]  \n",
      "2                 [2, 1, 2, 1, 1]                 [0, 0, 0, 0, 0]  \n",
      "3                    [2, 1, 1, 1]                    [0, 0, 0, 0]  \n",
      "4     [2, 1, 2, 1, 1, 2, 1, 3, 4]     [0, 0, 0, 0, 0, 6, 0, 6, 0]   \n",
      "\n",
      "Contents of data_dd/default_validation_0000.parquet:\n",
      "                                              dialog  \\\n",
      "0  [Good morning , sir . Is there a bank near her...   \n",
      "1  [Good afternoon . This is Michelle Li speaking...   \n",
      "2  [What qualifications should a reporter have ? ...   \n",
      "3  [Hi , good morning , Miss ? what can I help yo...   \n",
      "4  [Excuse me , ma'am . Can you tell me where the...   \n",
      "\n",
      "                                                act  \\\n",
      "0                             [2, 1, 3, 2, 1, 2, 1]   \n",
      "1                    [2, 1, 1, 1, 1, 2, 3, 2, 3, 4]   \n",
      "2                                      [2, 1, 2, 1]   \n",
      "3  [2, 3, 2, 2, 1, 2, 1, 2, 1, 1, 1, 3, 2, 1, 3, 4]   \n",
      "4                          [3, 4, 2, 1, 2, 1, 1, 1]   \n",
      "\n",
      "                                            emotion  \n",
      "0                             [0, 0, 0, 0, 0, 0, 0]  \n",
      "1                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "2                                      [0, 0, 0, 0]  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]  \n",
      "4                          [0, 0, 0, 0, 0, 0, 4, 4]   \n",
      "\n",
      "Contents of data_dd/default_test_0000.parquet:\n",
      "                                              dialog  \\\n",
      "0  [Hey man , you wanna buy some weed ? ,  Some w...   \n",
      "1  [The taxi drivers are on strike again . ,  Wha...   \n",
      "2  [We've managed to reduce our energy consumptio...   \n",
      "3  [Believe it or not , tea is the most popular b...   \n",
      "4  [What are your personal weaknesses ? ,  I ’ m ...   \n",
      "\n",
      "                                          act  \\\n",
      "0        [3, 2, 3, 4, 3, 4, 3, 2, 3, 4, 2, 3]   \n",
      "1                                [1, 2, 1, 1]   \n",
      "2                       [1, 2, 1, 2, 1, 2, 1]   \n",
      "3  [1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 3, 4, 3]   \n",
      "4                    [2, 1, 2, 1, 2, 1, 2, 1]   \n",
      "\n",
      "                                      emotion  \n",
      "0        [0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0]  \n",
      "1                                [0, 0, 0, 0]  \n",
      "2                       [0, 0, 0, 0, 0, 0, 0]  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 4]  \n",
      "4                    [0, 0, 0, 0, 0, 0, 0, 4]   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_parquet_files(file_paths: Dict[str, str]) -> Dict[str, pd.DataFrame]:\n",
    "    dataframes = {}\n",
    "    for key, file_path in file_paths.items():\n",
    "        try:\n",
    "            df = pd.read_parquet(file_path)\n",
    "            dataframes[key] = df\n",
    "            print(f\"Contents of {file_path}:\")\n",
    "            print(df.head(), \"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while loading {file_path}: {e}\")\n",
    "    return dataframes\n",
    "\n",
    "file_paths = {\n",
    "    'train': 'data_dd/0000.parquet',\n",
    "    'validation': 'data_dd/default_validation_0000.parquet',\n",
    "    'test': 'data_dd/default_test_0000.parquet'\n",
    "}\n",
    "\n",
    "dataframes = load_parquet_files(file_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631dd47f-5120-435e-b7ae-27fde4ee31df",
   "metadata": {},
   "source": [
    "### Review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d739cdf-8425-416e-a55a-3e502c4b70d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation', 'test'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c79dba0-1db0-4f10-a3e2-65098b1c9d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11118 1000 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialog</th>\n",
       "      <th>act</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Say , Jim , how about going for a few beers a...</td>\n",
       "      <td>[3, 4, 2, 2, 2, 3, 4, 1, 3, 4]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 4, 4, 4, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Can you do push-ups ? ,  Of course I can . It...</td>\n",
       "      <td>[2, 1, 2, 2, 1, 1]</td>\n",
       "      <td>[0, 0, 6, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Can you study with the radio on ? ,  No , I l...</td>\n",
       "      <td>[2, 1, 2, 1, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Are you all right ? ,  I will be all right so...</td>\n",
       "      <td>[2, 1, 1, 1]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Hey John , nice skates . Are they new ? ,  Ye...</td>\n",
       "      <td>[2, 1, 2, 1, 1, 2, 1, 3, 4]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 6, 0, 6, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11113</th>\n",
       "      <td>[Hello , I bought a pen in your shop just befo...</td>\n",
       "      <td>[1, 1, 1, 2, 3, 2, 1, 4, 1]</td>\n",
       "      <td>[0, 4, 0, 0, 0, 0, 0, 0, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11114</th>\n",
       "      <td>[Do you have any seats available ? ,  Yes . Th...</td>\n",
       "      <td>[2, 1, 2, 1, 3, 4]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11115</th>\n",
       "      <td>[Uncle Ben , how did the Forbidden City get th...</td>\n",
       "      <td>[2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 3, 4]</td>\n",
       "      <td>[0, 0, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11116</th>\n",
       "      <td>[May I help you , sir ? ,  I want a pair of lo...</td>\n",
       "      <td>[2, 3, 4, 3]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11117</th>\n",
       "      <td>[Could I have the check , please ? ,  Okay . I...</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>[0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11118 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  dialog  \\\n",
       "0      [Say , Jim , how about going for a few beers a...   \n",
       "1      [Can you do push-ups ? ,  Of course I can . It...   \n",
       "2      [Can you study with the radio on ? ,  No , I l...   \n",
       "3      [Are you all right ? ,  I will be all right so...   \n",
       "4      [Hey John , nice skates . Are they new ? ,  Ye...   \n",
       "...                                                  ...   \n",
       "11113  [Hello , I bought a pen in your shop just befo...   \n",
       "11114  [Do you have any seats available ? ,  Yes . Th...   \n",
       "11115  [Uncle Ben , how did the Forbidden City get th...   \n",
       "11116  [May I help you , sir ? ,  I want a pair of lo...   \n",
       "11117  [Could I have the check , please ? ,  Okay . I...   \n",
       "\n",
       "                                                    act  \\\n",
       "0                        [3, 4, 2, 2, 2, 3, 4, 1, 3, 4]   \n",
       "1                                    [2, 1, 2, 2, 1, 1]   \n",
       "2                                       [2, 1, 2, 1, 1]   \n",
       "3                                          [2, 1, 1, 1]   \n",
       "4                           [2, 1, 2, 1, 1, 2, 1, 3, 4]   \n",
       "...                                                 ...   \n",
       "11113                       [1, 1, 1, 2, 3, 2, 1, 4, 1]   \n",
       "11114                                [2, 1, 2, 1, 3, 4]   \n",
       "11115  [2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 3, 4]   \n",
       "11116                                      [2, 3, 4, 3]   \n",
       "11117                                            [3, 4]   \n",
       "\n",
       "                                                emotion  \n",
       "0                        [0, 0, 0, 0, 0, 0, 4, 4, 4, 4]  \n",
       "1                                    [0, 0, 6, 0, 0, 0]  \n",
       "2                                       [0, 0, 0, 0, 0]  \n",
       "3                                          [0, 0, 0, 0]  \n",
       "4                           [0, 0, 0, 0, 0, 6, 0, 6, 0]  \n",
       "...                                                 ...  \n",
       "11113                       [0, 4, 0, 0, 0, 0, 0, 0, 4]  \n",
       "11114                                [0, 0, 0, 0, 0, 4]  \n",
       "11115  [0, 0, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0]  \n",
       "11116                                      [0, 0, 0, 0]  \n",
       "11117                                            [0, 0]  \n",
       "\n",
       "[11118 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = dataframes['train']\n",
    "val_df = dataframes['validation']\n",
    "test_df = dataframes['test']\n",
    "print(len(train_df),len(val_df), len(test_df))\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4defcd4b-38db-4f28-8cbc-3a27bd2db229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Can you tell me where the pots and pans are ? '\n",
      " ' Pots and pans are right over there . ' ' Oh , thank you . '\n",
      " ' Could I interest you in our store credit card ? '\n",
      " ' No , thanks . I already have credit cards . '\n",
      " ' But our credit card saves you 10 percent . '\n",
      " \" That's a nice discount . \"\n",
      " ' Here . Let me give you an application form . '\n",
      " \" Thank you , but I'm just browsing today . \"\n",
      " ' Okay . Enjoy your browsing . ']\n",
      "['Here is the fish counter . Look at the lobsters and crabs . Shall we have some ? '\n",
      " \" I'm allergic to these things , you know . \"\n",
      " ' Sorry , I forgot . I don ’ t like seafood , neither . '\n",
      " ' Let ’ s go over there and get some milk , a couple dozen eggs and some orange juice . '\n",
      " \" Let's get frozen juice . It is really good . We ’ Ve got enough food . Let ’ s go over to the check-out stand . \"\n",
      " ' OK . But just let me pick up a bottle of cooking wine and oil as we go by . ']\n",
      "['Good morning , may I speak with Professor Clark , please ? '\n",
      " ' You are speaking with Professor Clark . '\n",
      " ' Professor , I am Kalina from your morning literature class . '\n",
      " ' Yes , how can I help you ? '\n",
      " ' I ran my car into a tree yesterday and need to miss a few days of school . '\n",
      " ' Oh , my God ! I hope you are all right . '\n",
      " ' I have a concussion , but I will be OK . '\n",
      " ' How much school will you miss ? '\n",
      " ' I only need to take this week off . '\n",
      " ' I appreciate you calling and telling me that you won ’ t be in class . See you next week ! ']\n",
      "['Hello , Parker . How ’ s everything ? ' ' Can ’ t complain . And you ? '\n",
      " ' Business is booming . I understand you want to meet up with me next week . How ’ s your schedule looking ? '\n",
      " ' Let me see . I can come out and see you first thing Wednesday . '\n",
      " ' Great . ']\n",
      "['How come it is slow as a snail today ? '\n",
      " ' You mean the network connection ? '\n",
      " ' Yes , I wanted to look for some information on the company page just now . It took me almost one minute to open it . Then there is no response to any click . '\n",
      " ' I have the same question . I can ’ t send out mails . We ’ d better call the IT department and ask them to check it immediately . '\n",
      " ' Ok . ']\n",
      "['Honey , I need to have a talk with you . '\n",
      " ' Dad , I have to do my homework . '\n",
      " \" No , honey , why didn't you go to cram school last night ? \"\n",
      " \" Dad , I don't want to talk about it now . \"\n",
      " \" Honey , if you don't want to go to cram school , you should tell me the reason why . \"\n",
      " \" I'm sorry , dad . But I would rather stay at school than go to cram school . \"]\n",
      "[\"What's wrong with you , young man ? \"\n",
      " ' Doctor , I have a bad cough and a headache . '\n",
      " ' Do you have a fever ? ' \" I don't know , but I feel terrible . \"\n",
      " \" Let me examine you . Don't worry . It's nothing serious . \"\n",
      " ' Do you think I should lie in bed ? '\n",
      " ' Yes , stay in bed and drink a lot of water . Your fever will be gone in a day or two . '\n",
      " ' OK . Do you think I can play football tomorrow ? '\n",
      " ' Of course not . You need a good rest . ' \" OK , I'll listen to you . \"]\n",
      "['I want something sweet after dinner . ' ' What do you have in mind ? '\n",
      " ' A dessert sounds nice . ' ' What kind are you thinking of getting ? '\n",
      " ' I want to get some pie . ' ' What kind of pie do you want ? '\n",
      " ' I have no idea . ' ' Do you want to know what kind of pie I like ? '\n",
      " ' Sure , what kind do you like ? ' ' I love apple pie . '\n",
      " ' Oh , I love apple pie too . ' ' There you go . Problem solved . ']\n",
      "[\"Let's go now . \" \" I'll be with you in a minute . \"]\n"
     ]
    }
   ],
   "source": [
    "for dialogue in test_df['dialog'][101: 110]:\n",
    "    print(dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3388fda7-6bb1-47a7-9859-29e32e2f1ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    11118.000000\n",
      "mean         7.840439\n",
      "std          4.007963\n",
      "min          2.000000\n",
      "25%          4.000000\n",
      "50%          7.000000\n",
      "75%         10.000000\n",
      "max         35.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Cheking lengths of dialogs\n",
    "lengths = []\n",
    "for dialogue in train_df['dialog']:\n",
    "    length = len(dialogue)\n",
    "    lengths.append(length)\n",
    "\n",
    "lengths_series = pd.Series(lengths)\n",
    "print(lengths_series.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89db0e3-aa1c-4889-9e39-aabc148ee2ca",
   "metadata": {},
   "source": [
    "### Clean data\n",
    "For the first such project I will not use additional data provided such as 'act' and 'emotion'. I will use my own data split, therefore I will concatinate all data and take only dialogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "069c8ab6-7013-44c0-88ac-e194286af9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Say , Jim , how about going for a few beers a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Can you do push-ups ? ,  Of course I can . It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Can you study with the radio on ? ,  No , I l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Are you all right ? ,  I will be all right so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Hey John , nice skates . Are they new ? ,  Ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13113</th>\n",
       "      <td>[Frank ’ s getting married , do you believe th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13114</th>\n",
       "      <td>[OK . Come back into the classroom , class . ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13115</th>\n",
       "      <td>[Do you have any hobbies ? ,  Yes , I like col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13116</th>\n",
       "      <td>[Jenny , what's wrong with you ? Why do you ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13117</th>\n",
       "      <td>[What a nice day ! ,  yes . How about going ou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13118 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  dialog\n",
       "0      [Say , Jim , how about going for a few beers a...\n",
       "1      [Can you do push-ups ? ,  Of course I can . It...\n",
       "2      [Can you study with the radio on ? ,  No , I l...\n",
       "3      [Are you all right ? ,  I will be all right so...\n",
       "4      [Hey John , nice skates . Are they new ? ,  Ye...\n",
       "...                                                  ...\n",
       "13113  [Frank ’ s getting married , do you believe th...\n",
       "13114  [OK . Come back into the classroom , class . ,...\n",
       "13115  [Do you have any hobbies ? ,  Yes , I like col...\n",
       "13116  [Jenny , what's wrong with you ? Why do you ke...\n",
       "13117  [What a nice day ! ,  yes . How about going ou...\n",
       "\n",
       "[13118 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dialogs = pd.concat([train_df['dialog'], val_df['dialog'], test_df['dialog']], ignore_index=True)\n",
    "data = pd.DataFrame({'dialog': all_dialogs})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e04d54-13e8-444c-a08b-a16880b5abb1",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d06d6cf2-c98b-433b-9746-939d4b24ebfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of single dialog: <class 'numpy.ndarray'>\n",
      "The type of the sentence within dialog: <class 'str'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 13118 entries, 0 to 13117\n",
      "Series name: dialog\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "13118 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 102.6+ KB\n"
     ]
    }
   ],
   "source": [
    "print(f\"The type of single dialog: {type(data['dialog'][0])}\")\n",
    "print(f\"The type of the sentence within dialog: {type(data['dialog'][0][0])}\")\n",
    "data['dialog'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed01a1ad-359f-4db9-b2b5-9f810676cf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dialogs are ndarrays, my preprocessing funcions are for strings\n",
    "def preprocess_text_array(arr):\n",
    "    dialog = arr.tolist()\n",
    "    return [preprocess_text(text) for text in dialog]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b4a97ad-9000-42e9-ab96-8ee621bda8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialog</th>\n",
       "      <th>preprocessed_dialog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Say , Jim , how about going for a few beers a...</td>\n",
       "      <td>[say how about going for a few beers after din...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Can you do push-ups ? ,  Of course I can . It...</td>\n",
       "      <td>[can you do push ups, of course i can it is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Can you study with the radio on ? ,  No , I l...</td>\n",
       "      <td>[can you study with the radio on, no i listen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Are you all right ? ,  I will be all right so...</td>\n",
       "      <td>[are you all right, i will be all right soon i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Hey John , nice skates . Are they new ? ,  Ye...</td>\n",
       "      <td>[skates are they new, yeah i just got them i s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13113</th>\n",
       "      <td>[Frank ’ s getting married , do you believe th...</td>\n",
       "      <td>[getting married do you believe this, is he re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13114</th>\n",
       "      <td>[OK . Come back into the classroom , class . ,...</td>\n",
       "      <td>[ok come back into the classroom class, does t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13115</th>\n",
       "      <td>[Do you have any hobbies ? ,  Yes , I like col...</td>\n",
       "      <td>[do you have any hobbies, yes i like collectin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13116</th>\n",
       "      <td>[Jenny , what's wrong with you ? Why do you ke...</td>\n",
       "      <td>[what is wrong with you why do you keep weepin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13117</th>\n",
       "      <td>[What a nice day ! ,  yes . How about going ou...</td>\n",
       "      <td>[what a nice day, yes how about going out and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13118 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  dialog  \\\n",
       "0      [Say , Jim , how about going for a few beers a...   \n",
       "1      [Can you do push-ups ? ,  Of course I can . It...   \n",
       "2      [Can you study with the radio on ? ,  No , I l...   \n",
       "3      [Are you all right ? ,  I will be all right so...   \n",
       "4      [Hey John , nice skates . Are they new ? ,  Ye...   \n",
       "...                                                  ...   \n",
       "13113  [Frank ’ s getting married , do you believe th...   \n",
       "13114  [OK . Come back into the classroom , class . ,...   \n",
       "13115  [Do you have any hobbies ? ,  Yes , I like col...   \n",
       "13116  [Jenny , what's wrong with you ? Why do you ke...   \n",
       "13117  [What a nice day ! ,  yes . How about going ou...   \n",
       "\n",
       "                                     preprocessed_dialog  \n",
       "0      [say how about going for a few beers after din...  \n",
       "1      [can you do push ups, of course i can it is a ...  \n",
       "2      [can you study with the radio on, no i listen ...  \n",
       "3      [are you all right, i will be all right soon i...  \n",
       "4      [skates are they new, yeah i just got them i s...  \n",
       "...                                                  ...  \n",
       "13113  [getting married do you believe this, is he re...  \n",
       "13114  [ok come back into the classroom class, does t...  \n",
       "13115  [do you have any hobbies, yes i like collectin...  \n",
       "13116  [what is wrong with you why do you keep weepin...  \n",
       "13117  [what a nice day, yes how about going out and ...  \n",
       "\n",
       "[13118 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[:, \"preprocessed_dialog\"] = data.loc[:, \"dialog\"].apply(preprocess_text_array)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6073ee61-beb8-4c5f-b169-64660f0abfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['please excuse me but i really have to be going', 'yes of course it was nice to see you', 'it was nice to see you too and please give my regards to mrs robbins']\n",
      "['excuse me is this seat taken', 'i am afraid so']\n",
      "['what do you think of the coming match', 'winning is a piece of cake to me', 'you are bragging again']\n",
      "['what would you reckon the taxing increases', 'well the state will benefit a lot i suppose', 'but what do most people think about it', 'ah it s hard to say']\n",
      "['are you still coming to my place for dinner tomorrow night', 'of course is the dinner still on', 'yes i was just wondering how you and your roommate were planning on coming to', 'we were planning on walking both ways since the weather is still nice', \"that 's what i thought you would do listen i live in a bit of\", 'it can not be that bad', 'i wish it was not but there is actually a lot of crime and prostitution', 'really i never would have guessed the criminals must only come out in the evenings', \"do me a favor and take a taxi it 'd make me feel a lot\", 'ok we will how do you get around in the evenings', 'when i first moved in i walked everywhere but within a week i had my', 'has anything else happened to you', 'nothing else has happened to me but i have seen quite a few fights on', 'well we will be careful thanks for letting me know']\n",
      "['wonders whether likes him or not', 'why does not he ask her', 'he is too scared to ask her', 'he is a chicken guy']\n",
      "['i do not understand why some parents keep beefing and complaining about their daughters not', \"yeah li na 's mother has been building a fire under her since her neighbour\", 'if i were na i would ask her if she had done that', 'she is as meek as a lamb she never goes against anyone or anything she']\n",
      "['where is i can not find him anywhere', 'have not you heard that he is in prison', 'what beg your pardon', 'is in prison now he was copped outstealing', 'i just can not believe my ears']\n",
      "['what do you need', 'i need to use the internet', 'you have your library card right', 'yes i do', 'there is a wait right now to use the computers', 'that s fine', 'would you please write your name on this list', 'then what', 'i will call you when a computer is free', 'how do i log on to the computer', 'use the number on the back of your library card', 'thanks i ll be sitting over there']\n"
     ]
    }
   ],
   "source": [
    "for dialogue in data['preprocessed_dialog'][121: 130]:\n",
    "    print(dialogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1662a9-d4df-47e1-aa98-9327309fa488",
   "metadata": {},
   "source": [
    "### Pairing messages - input with responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f7c7c5c-5ded-472c-a5dd-1823a6085050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create input-response pairs\n",
    "def create_pairs(dialogues):\n",
    "    input_responses = []\n",
    "    for dialogue in dialogues:\n",
    "        for i in range(len(dialogue) - 1):\n",
    "            input_responses.append((dialogue[i], dialogue[i + 1]))\n",
    "    return input_responses\n",
    "\n",
    "# Create input-response pairs\n",
    "pairs = create_pairs(data['preprocessed_dialog'])\n",
    "\n",
    "# Convert pairs to DataFrame\n",
    "pairs_df = pd.DataFrame(pairs, columns=['input', 'response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bd0c93a-101d-4ef5-af34-9324b1be7a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>say how about going for a few beers after dinner</td>\n",
       "      <td>you know that is tempting but is really not go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you know that is tempting but is really not go...</td>\n",
       "      <td>what do you mean it will help us to relax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what do you mean it will help us to relax</td>\n",
       "      <td>do you really think so i do not it will just m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>do you really think so i do not it will just m...</td>\n",
       "      <td>i guess you are right but what shall we do i d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i guess you are right but what shall we do i d...</td>\n",
       "      <td>i suggest a walk over to the gym where we can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89856</th>\n",
       "      <td>why not go again to celebrate out one year ann...</td>\n",
       "      <td>are you kidding can you afford it do you think...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89857</th>\n",
       "      <td>are you kidding can you afford it do you think...</td>\n",
       "      <td>never mind that i will take care of it are you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89858</th>\n",
       "      <td>never mind that i will take care of it are you...</td>\n",
       "      <td>yeah i think so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89859</th>\n",
       "      <td>yeah i think so</td>\n",
       "      <td>ok i will make the arrangements it will be great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89860</th>\n",
       "      <td>ok i will make the arrangements it will be great</td>\n",
       "      <td>wonderful i will start packing our suitcases</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89861 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input  \\\n",
       "0       say how about going for a few beers after dinner   \n",
       "1      you know that is tempting but is really not go...   \n",
       "2              what do you mean it will help us to relax   \n",
       "3      do you really think so i do not it will just m...   \n",
       "4      i guess you are right but what shall we do i d...   \n",
       "...                                                  ...   \n",
       "89856  why not go again to celebrate out one year ann...   \n",
       "89857  are you kidding can you afford it do you think...   \n",
       "89858  never mind that i will take care of it are you...   \n",
       "89859                                    yeah i think so   \n",
       "89860   ok i will make the arrangements it will be great   \n",
       "\n",
       "                                                response  \n",
       "0      you know that is tempting but is really not go...  \n",
       "1              what do you mean it will help us to relax  \n",
       "2      do you really think so i do not it will just m...  \n",
       "3      i guess you are right but what shall we do i d...  \n",
       "4      i suggest a walk over to the gym where we can ...  \n",
       "...                                                  ...  \n",
       "89856  are you kidding can you afford it do you think...  \n",
       "89857  never mind that i will take care of it are you...  \n",
       "89858                                    yeah i think so  \n",
       "89859   ok i will make the arrangements it will be great  \n",
       "89860       wonderful i will start packing our suitcases  \n",
       "\n",
       "[89861 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac0374a5-80f3-4d8f-9f8f-234190f4e2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    89861.000000\n",
      "mean         9.574799\n",
      "std          4.314846\n",
      "min          0.000000\n",
      "25%          6.000000\n",
      "50%          9.000000\n",
      "75%         15.000000\n",
      "max         15.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Cheking length of sentences\n",
    "lengths = []\n",
    "for sentence in pairs_df['input']:\n",
    "    length = len(sentence.split())\n",
    "    lengths.append(length)\n",
    "\n",
    "lengths_series = pd.Series(lengths)\n",
    "print(lengths_series.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2abf70bf-f25f-4102-9793-a6a8c7f244f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_monologues = pairs_df[pairs_df['input'].str.split().str.len() > 30]\n",
    "# print(long_monologues)\n",
    "# print(pairs_df['input'][89769])\n",
    "# len(long_monologues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eed6d9-5b38-439b-8a95-bd835bb4d588",
   "metadata": {},
   "source": [
    "### Initialize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d16fef4-a5db-45e2-99c5-a71e9fecfcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of <START> token: 1\n",
      "Index of <END> token: 2\n",
      "Index of <OOV> token: 3\n",
      "15384\n",
      "10000\n",
      "\n",
      "Top 15 most frequent words:\n",
      " [('i', 86051), ('you', 73409), ('the', 52476), ('to', 44476), ('a', 39594), ('is', 37170), ('it', 36511), ('that', 24652), ('have', 23424), ('do', 23403), ('and', 21134), ('not', 20913), ('of', 18558), ('are', 18413), ('what', 17623)]\n",
      "\n",
      "Last 100 words:\n",
      " [('wong', 1), ('glitches', 1), ('communicational', 1), ('assiduously', 1), ('rarest', 1), ('momma', 1), ('codes', 1), ('dongle', 1), ('multitasking', 1), ('constipation', 1), ('recarpeted', 1), ('uncite', 1), ('dutton', 1), ('goodnight', 1), ('reunification', 1), ('yearning', 1), ('informing', 1), ('robson', 1), ('gaston', 1), ('transition', 1), ('macchiato', 1), ('backers', 1), ('montezuma', 1), ('thans', 1), ('ultra', 1), ('brushed', 1), ('titanium', 1), ('kaohsiung', 1), ('mousaka', 1), ('adjustments', 1), ('floral', 1), ('apologise', 1), ('distributed', 1), ('judgement', 1), ('transported', 1), ('trucks', 1), ('donut', 1), ('outsmart', 1), ('thrilled', 1), ('tienda', 1), ('maroon', 1), ('familiarise', 1), ('chunks', 1), ('upfront', 1), ('cctv', 1), ('discreet', 1), ('demographics', 1), ('byeb', 1), ('formalities', 1), ('chute', 1), ('draws', 1), ('neighborhoods', 1), ('redouble', 1), ('transplant', 1), ('fossil', 1), ('fuels', 1), ('hydro', 1), ('distractions', 1), ('gustave', 1), ('hells', 1), ('canvas', 1), ('crescive', 1), ('symbolize', 1), ('longevity', 1), ('lpt', 1), ('oversee', 1), ('evades', 1), ('detection', 1), ('await', 1), ('trional', 1), ('fortnightly', 1), ('trifle', 1), ('overtake', 1), ('aider', 1), ('smashing', 1), ('actives', 1), ('copywriters', 1), ('translations', 1), ('aspirins', 1), ('crashes', 1), ('streeter', 1), ('kleenex', 1), ('noticeable', 1), ('innovation', 1), ('coordination', 1), ('sealed', 1), ('grownups', 1), ('telecommuting', 1), ('lifelike', 1), ('simulated', 1), ('incarnate', 1), ('extraordinaire', 1), ('engages', 1), ('interact', 1), ('apes', 1), ('wined', 1), ('dined', 1), ('scuba', 1), ('flavorings', 1), ('hopeless', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>') # , filters=' ') - default filters remove punctuation\n",
    "tokenizer.fit_on_texts(pairs_df['input'].tolist() + pairs_df['response'].tolist())\n",
    "\n",
    "# Define special tokens\n",
    "start_token = '<START>'\n",
    "end_token = '<END>'\n",
    "\n",
    "old_word_index = len(tokenizer.word_index)\n",
    "\n",
    "# Add special tokens to the tokenizer and ensure they are within the top 10,000 words\n",
    "tokenizer.word_index = {k: (i+3) for i, (k, v) in enumerate(tokenizer.word_index.items()) if i < old_word_index}\n",
    "tokenizer.word_index[start_token] = 1\n",
    "tokenizer.word_index[end_token] = 2\n",
    "tokenizer.word_index[tokenizer.oov_token] = 3\n",
    "\n",
    "# Verify the indices\n",
    "print(\"Index of <START> token:\", tokenizer.word_index['<START>'])\n",
    "print(\"Index of <END> token:\", tokenizer.word_index['<END>'])\n",
    "print(\"Index of <OOV> token:\", tokenizer.word_index['<OOV>'])\n",
    "\n",
    "print(len(tokenizer.word_index))\n",
    "print(tokenizer.num_words)\n",
    "\n",
    "sorted_word_counts = OrderedDict(sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "print(f\"\\nTop 15 most frequent words:\\n {list(sorted_word_counts.items())[:15]}\")\n",
    "print(f\"\\nLast 100 words:\\n {list(sorted_word_counts.items())[-100:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a276c-6264-4bee-a0fb-067f04b6b4c7",
   "metadata": {},
   "source": [
    "### Filter rare words - 10000 vocabulary OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9dd5de7a-fe72-4754-92cc-76b214d7fe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words before filtering: 15381\n",
      "Total words after filtering: 10416\n",
      "\n",
      "Top 15 most frequent words:\n",
      " [('i', 86051), ('you', 73409), ('the', 52476), ('to', 44476), ('a', 39594), ('is', 37170), ('it', 36511), ('that', 24652), ('have', 23424), ('do', 23403), ('and', 21134), ('not', 20913), ('of', 18558), ('are', 18413), ('what', 17623)]\n",
      "\n",
      "Last 100 words:\n",
      " [('solomon', 3), ('believer', 3), ('truant', 3), ('clarity', 3), ('blanca', 3), ('bellhop', 3), ('frappuccino', 3), ('trail', 3), ('flames', 3), ('sanitary', 3), ('unfit', 3), ('allison', 3), ('biannually', 3), ('pamphlets', 3), ('hasty', 3), ('expire', 3), ('overcoming', 3), ('banked', 3), ('undue', 3), ('scaring', 3), ('stylus', 3), ('advisor', 3), ('realise', 3), ('occasional', 3), ('lithium', 3), ('eva', 3), ('bluemingdails', 3), ('spectator', 3), ('fang', 3), ('insight', 3), ('spotless', 3), ('prosperous', 3), ('deduct', 3), ('al', 3), ('booster', 3), ('stefan', 3), ('pedicure', 3), ('discoveries', 3), ('acidic', 3), ('judging', 3), ('ation', 3), ('bristles', 3), ('bidders', 3), ('ahem', 3), ('arranging', 3), ('inventors', 3), ('convertible', 3), ('turnaround', 3), ('handcrafts', 3), ('interrupted', 3), ('firmly', 3), ('precision', 3), ('corresponding', 3), ('speedy', 3), (\"i'li\", 3), ('ministry', 3), ('weed', 3), ('scented', 3), ('talker', 3), ('worship', 3), ('kiwis', 3), ('typewriters', 3), ('metropolis', 3), ('certainty', 3), ('mentality', 3), ('ratings', 3), ('usher', 3), ('scottish', 3), ('herbal', 3), ('confiscated', 3), ('clears', 3), ('juices', 3), ('technoledge', 3), ('bicycles', 3), ('possession', 3), ('poem', 3), ('tutoring', 3), ('lifts', 3), ('fortunes', 3), ('groves', 3), ('carter', 3), ('chessboard', 3), ('hurdle', 3), ('aesthetics', 3), ('pronounces', 3), ('beliefs', 3), ('warned', 3), ('habitable', 3), ('exhilarating', 3), ('pitches', 3), ('whiter', 3), ('stimulate', 3), ('smiles', 3), ('salesmen', 3), ('visits', 3), ('cartridge', 3), ('mcbride', 3), ('pullover', 3), ('cushion', 3), ('banged', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Just reviewing what are the last words in vocabulary - do they still usable and recognizable\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Set a frequency threshold\n",
    "threshold = 3\n",
    "\n",
    "# Filter out rare words\n",
    "filtered_words = {word: count for word, count in sorted_word_counts.items() if count >= threshold}\n",
    "\n",
    "# Display the number of words before and after filtering\n",
    "print(f\"Total words before filtering: {len(sorted_word_counts)}\")\n",
    "print(f\"Total words after filtering: {len(filtered_words)}\")\n",
    "# Display the sorted word counts\n",
    "print(f\"\\nTop 15 most frequent words:\\n {list(filtered_words.items())[:15]}\")\n",
    "print(f\"\\nLast 100 words:\\n {list(filtered_words.items())[-100:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb4a376-b87e-4630-a88c-12d22b9d3a2b",
   "metadata": {},
   "source": [
    "### Save the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca8be1de-b1f7-4d93-897b-6ef1908fc475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved to C:\\Users\\tomui\\Desktop\\daily_dialogue\\data_dd\\tokenizer_dd.pickle\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Determine the directory where the tokenizer will be saved\n",
    "data_dir = os.path.join(os.getcwd(), 'data_dd')\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Save the tokenizer using pickle\n",
    "tokenizer_path = os.path.join(data_dir, 'tokenizer_dd.pickle')\n",
    "with open(tokenizer_path, 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(f\"Tokenizer saved to {tokenizer_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5deb6d2-695b-4025-b6b1-72428d051053",
   "metadata": {},
   "source": [
    "### Load the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5428c87-4ce6-480a-bb32-48f7a6063e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from file\n",
    "data_dir = os.path.join(os.getcwd(), 'data_dd')\n",
    "tokenizer_path = os.path.join(data_dir, 'tokenizer_dd.pickle')\n",
    "with open(tokenizer_path, 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9df411-ff49-4a67-802e-bd0dbca3ba3f",
   "metadata": {},
   "source": [
    "### Converting to indices and input-target sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df1d74ff-4be8-4b14-8083-ddbce2032e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input data shape: (89861, 15)\n",
      "Decoder input data shape: (89861, 16)\n",
      "Decoder output data shape: (89861, 16)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and pad the encoder input\n",
    "input_sequences = tokenizer.texts_to_sequences(pairs_df['input'].tolist())\n",
    "max_len_input = max(len(seq) for seq in input_sequences)  # Leve for later possibility pad without trimming in preprocessing stage\n",
    "encoder_input_data = pad_sequences(input_sequences, maxlen=max_len_input, padding='pre', truncating='post')\n",
    "\n",
    "# Tokenize the decoder input and output\n",
    "output_sequences = tokenizer.texts_to_sequences(pairs_df['response'].tolist())\n",
    "\n",
    "# Add start and end tokens\n",
    "start_token_index = tokenizer.word_index[start_token]\n",
    "end_token_index = tokenizer.word_index[end_token] \n",
    "decoder_input_sequences = [[start_token_index] + seq for seq in output_sequences]\n",
    "decoder_output_sequences = [seq + [end_token_index] for seq in output_sequences]\n",
    "\n",
    "# Pad the decoder input sequences\n",
    "max_len_output = max(len(seq) for seq in decoder_input_sequences)  # Leve for later possibility pad without trimming in preprocessing stage\n",
    "decoder_input_data = pad_sequences(decoder_input_sequences, maxlen=max_len_output, padding='pre', truncating='post')\n",
    "\n",
    "# Pad the decoder output sequences\n",
    "decoder_output_data = pad_sequences(decoder_output_sequences, maxlen=max_len_output, padding='pre', truncating='post')\n",
    "\n",
    "print(f'Encoder input data shape: {encoder_input_data.shape}')\n",
    "print(f'Decoder input data shape: {decoder_input_data.shape}')\n",
    "print(f'Decoder output data shape: {decoder_output_data.shape}')\n",
    "\n",
    "# Store numpy arrays directly in the DataFrame\n",
    "pairs_df['encoder_input_data'] = encoder_input_data.tolist()\n",
    "pairs_df['decoder_input_data'] = decoder_input_data.tolist()\n",
    "pairs_df['decoder_output_data'] = decoder_output_data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "685d7313-700b-4e5f-a1c4-34d33b8e0155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>response</th>\n",
       "      <th>encoder_input_data</th>\n",
       "      <th>decoder_input_data</th>\n",
       "      <th>decoder_output_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>say how about going for a few beers after dinner</td>\n",
       "      <td>you know that is tempting but is really not go...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 138, 33, 37, 75, 20, 8, 206, 3...</td>\n",
       "      <td>[0, 0, 1, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 4...</td>\n",
       "      <td>[0, 0, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 47, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you know that is tempting but is really not go...</td>\n",
       "      <td>what do you mean it will help us to relax</td>\n",
       "      <td>[0, 0, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 47, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 18, 13, 5, 161, 10, 23, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 18, 13, 5, 161, 10, 23, 101, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what do you mean it will help us to relax</td>\n",
       "      <td>do you really think so i do not it will just m...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 18, 13, 5, 161, 10, 23, 101, 9...</td>\n",
       "      <td>[1, 13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, ...</td>\n",
       "      <td>[13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, 102...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>do you really think so i do not it will just m...</td>\n",
       "      <td>i guess you are right but what shall we do i d...</td>\n",
       "      <td>[13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, 102...</td>\n",
       "      <td>[1, 4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4,...</td>\n",
       "      <td>[4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i guess you are right but what shall we do i d...</td>\n",
       "      <td>i suggest a walk over to the gym where we can ...</td>\n",
       "      <td>[4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4, 13...</td>\n",
       "      <td>[1, 4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 2...</td>\n",
       "      <td>[4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 21, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89856</th>\n",
       "      <td>why not go again to celebrate out one year ann...</td>\n",
       "      <td>are you kidding can you afford it do you think...</td>\n",
       "      <td>[88, 15, 59, 204, 7, 1602, 84, 56, 203, 1776, ...</td>\n",
       "      <td>[1, 17, 5, 618, 21, 5, 974, 10, 13, 5, 43, 22,...</td>\n",
       "      <td>[17, 5, 618, 21, 5, 974, 10, 13, 5, 43, 22, 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89857</th>\n",
       "      <td>are you kidding can you afford it do you think...</td>\n",
       "      <td>never mind that i will take care of it are you...</td>\n",
       "      <td>[17, 5, 618, 21, 5, 974, 10, 13, 5, 43, 22, 21...</td>\n",
       "      <td>[0, 1, 174, 211, 11, 4, 23, 72, 351, 16, 10, 1...</td>\n",
       "      <td>[0, 174, 211, 11, 4, 23, 72, 351, 16, 10, 17, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89858</th>\n",
       "      <td>never mind that i will take care of it are you...</td>\n",
       "      <td>yeah i think so</td>\n",
       "      <td>[0, 174, 211, 11, 4, 23, 72, 351, 16, 10, 17, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 112, 4, 4...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 112, 4, 43, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89859</th>\n",
       "      <td>yeah i think so</td>\n",
       "      <td>ok i will make the arrangements it will be great</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 112, 4, 43, 36]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 71, 4, 23, 102, 6, 3756, 10...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 71, 4, 23, 102, 6, 3756, 10, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89860</th>\n",
       "      <td>ok i will make the arrangements it will be great</td>\n",
       "      <td>wonderful i will start packing our suitcases</td>\n",
       "      <td>[0, 0, 0, 0, 0, 71, 4, 23, 102, 6, 3756, 10, 2...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 418, 4, 23, 271, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 418, 4, 23, 271, 1710...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89861 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input  \\\n",
       "0       say how about going for a few beers after dinner   \n",
       "1      you know that is tempting but is really not go...   \n",
       "2              what do you mean it will help us to relax   \n",
       "3      do you really think so i do not it will just m...   \n",
       "4      i guess you are right but what shall we do i d...   \n",
       "...                                                  ...   \n",
       "89856  why not go again to celebrate out one year ann...   \n",
       "89857  are you kidding can you afford it do you think...   \n",
       "89858  never mind that i will take care of it are you...   \n",
       "89859                                    yeah i think so   \n",
       "89860   ok i will make the arrangements it will be great   \n",
       "\n",
       "                                                response  \\\n",
       "0      you know that is tempting but is really not go...   \n",
       "1              what do you mean it will help us to relax   \n",
       "2      do you really think so i do not it will just m...   \n",
       "3      i guess you are right but what shall we do i d...   \n",
       "4      i suggest a walk over to the gym where we can ...   \n",
       "...                                                  ...   \n",
       "89856  are you kidding can you afford it do you think...   \n",
       "89857  never mind that i will take care of it are you...   \n",
       "89858                                    yeah i think so   \n",
       "89859   ok i will make the arrangements it will be great   \n",
       "89860       wonderful i will start packing our suitcases   \n",
       "\n",
       "                                      encoder_input_data  \\\n",
       "0      [0, 0, 0, 0, 0, 138, 33, 37, 75, 20, 8, 206, 3...   \n",
       "1      [0, 0, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 47, ...   \n",
       "2      [0, 0, 0, 0, 0, 18, 13, 5, 161, 10, 23, 101, 9...   \n",
       "3      [13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, 102...   \n",
       "4      [4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4, 13...   \n",
       "...                                                  ...   \n",
       "89856  [88, 15, 59, 204, 7, 1602, 84, 56, 203, 1776, ...   \n",
       "89857  [17, 5, 618, 21, 5, 974, 10, 13, 5, 43, 22, 21...   \n",
       "89858  [0, 174, 211, 11, 4, 23, 72, 351, 16, 10, 17, ...   \n",
       "89859  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 112, 4, 43, 36]   \n",
       "89860  [0, 0, 0, 0, 0, 71, 4, 23, 102, 6, 3756, 10, 2...   \n",
       "\n",
       "                                      decoder_input_data  \\\n",
       "0      [0, 0, 1, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 4...   \n",
       "1      [0, 0, 0, 0, 0, 1, 18, 13, 5, 161, 10, 23, 101...   \n",
       "2      [1, 13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, ...   \n",
       "3      [1, 4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4,...   \n",
       "4      [1, 4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 2...   \n",
       "...                                                  ...   \n",
       "89856  [1, 17, 5, 618, 21, 5, 974, 10, 13, 5, 43, 22,...   \n",
       "89857  [0, 1, 174, 211, 11, 4, 23, 72, 351, 16, 10, 1...   \n",
       "89858  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 112, 4, 4...   \n",
       "89859  [0, 0, 0, 0, 0, 1, 71, 4, 23, 102, 6, 3756, 10...   \n",
       "89860  [0, 0, 0, 0, 0, 0, 0, 0, 1, 418, 4, 23, 271, 1...   \n",
       "\n",
       "                                     decoder_output_data  \n",
       "0      [0, 0, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 47, ...  \n",
       "1      [0, 0, 0, 0, 0, 18, 13, 5, 161, 10, 23, 101, 9...  \n",
       "2      [13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, 102...  \n",
       "3      [4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4, 13...  \n",
       "4      [4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 21, ...  \n",
       "...                                                  ...  \n",
       "89856  [17, 5, 618, 21, 5, 974, 10, 13, 5, 43, 22, 21...  \n",
       "89857  [0, 174, 211, 11, 4, 23, 72, 351, 16, 10, 17, ...  \n",
       "89858  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 112, 4, 43, ...  \n",
       "89859  [0, 0, 0, 0, 0, 71, 4, 23, 102, 6, 3756, 10, 2...  \n",
       "89860  [0, 0, 0, 0, 0, 0, 0, 0, 418, 4, 23, 271, 1710...  \n",
       "\n",
       "[89861 rows x 5 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43feafb6-2b5a-4710-b070-258258f63e30",
   "metadata": {},
   "source": [
    "### Checking if conversion was successfull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26d4a424-9e01-44d3-a6a5-4df7eb46bf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Encoder Input: no so it is usually boring to join my friends in the afternoon at a \n",
      "Reconstructed Input: no so it is usually boring to join my friends in the afternoon at a\n",
      "\n",
      "Original Decoder Input: what kind of things would you like to see on the menu \n",
      "Reconstructed Text: <START> what kind of things would you like to see on the menu\n",
      "\n",
      "Original Decoder Output: what kind of things would you like to see on the menu \n",
      "Reconstructed Text: what kind of things would you like to see on the menu <END>\n",
      "\n",
      "Original Encoder Input: what kind of things would you like to see on the menu \n",
      "Reconstructed Input: what kind of things would you like to see on the menu\n",
      "\n",
      "Original Decoder Input: maybe a fruit salad and a few different hot sandwiches at least \n",
      "Reconstructed Text: <START> maybe a fruit salad and a few different hot sandwiches at least\n",
      "\n",
      "Original Decoder Output: maybe a fruit salad and a few different hot sandwiches at least \n",
      "Reconstructed Text: maybe a fruit salad and a few different hot sandwiches at least <END>\n",
      "\n",
      "Original Encoder Input: maybe a fruit salad and a few different hot sandwiches at least \n",
      "Reconstructed Input: maybe a fruit salad and a few different hot sandwiches at least\n",
      "\n",
      "Original Decoder Input: that should not be too difficult since this is a small neighborhood maybe they will \n",
      "Reconstructed Text: <START> that should not be too difficult since this is a small neighborhood maybe they will\n",
      "\n",
      "Original Decoder Output: that should not be too difficult since this is a small neighborhood maybe they will \n",
      "Reconstructed Text: that should not be too difficult since this is a small neighborhood maybe they will <END>\n",
      "\n",
      "Original Encoder Input: that should not be too difficult since this is a small neighborhood maybe they will \n",
      "Reconstructed Input: that should not be too difficult since this is a small neighborhood maybe they will\n",
      "\n",
      "Original Decoder Input: let us try it \n",
      "Reconstructed Text: <START> let us try it\n",
      "\n",
      "Original Decoder Output: let us try it \n",
      "Reconstructed Text: let us try it <END>\n",
      "\n",
      "Original Encoder Input: darling i have news for you and his wife evelyn are going to have a \n",
      "Reconstructed Input: darling i have news for you and his wife evelyn are going to have a\n",
      "\n",
      "Original Decoder Input: really i thought his wife couldn t have a baby \n",
      "Reconstructed Text: <START> really i thought his wife couldn t have a baby\n",
      "\n",
      "Original Decoder Output: really i thought his wife couldn t have a baby \n",
      "Reconstructed Text: really i thought his wife couldn t have a baby <END>\n"
     ]
    }
   ],
   "source": [
    "def sequences_to_text(sequence):\n",
    "    index_to_word = {index: word for word, index in tokenizer.word_index.items()}\n",
    "    # Directly map sequence of indices back to words\n",
    "    return ' '.join(index_to_word.get(idx, '') for idx in sequence if idx != 0)\n",
    "\n",
    "# Print original and reverse-tokenized text for entries\n",
    "for index, row in pairs_df[1130:1135].iterrows():\n",
    "    print(\"\\nOriginal Encoder Input:\", row['input'], \n",
    "          \"\\nReconstructed Input:\", sequences_to_text(row['encoder_input_data']))\n",
    "    print(\"\\nOriginal Decoder Input:\", row['response'], \n",
    "          \"\\nReconstructed Text:\", sequences_to_text(row['decoder_input_data']))\n",
    "    print(\"\\nOriginal Decoder Output:\", row['response'], \n",
    "          \"\\nReconstructed Text:\", sequences_to_text(row['decoder_output_data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f96c6b5-a666-499a-b578-9639350909e3",
   "metadata": {},
   "source": [
    "### Saving the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3dddbeaa-4b10-44e1-a6f9-f3f2056ce421",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving the DataFrame\n",
    "data_dir = os.path.join(os.getcwd(), 'data_dd')\n",
    "file_path_parquet = os.path.join(data_dir, 'training_df_dd.parquet')\n",
    "pairs_df.to_parquet(file_path_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e0ec68-2723-436d-a101-ad18df9f0a63",
   "metadata": {},
   "source": [
    "### Loading the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8036e60-f3a3-4beb-9f96-da6496797413",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>response</th>\n",
       "      <th>encoder_input_data</th>\n",
       "      <th>decoder_input_data</th>\n",
       "      <th>decoder_output_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>say how about going for a few beers after dinner</td>\n",
       "      <td>you know that is tempting but is really not go...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 138, 33, 37, 75, 20, 8, 206, 3...</td>\n",
       "      <td>[0, 0, 1, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 4...</td>\n",
       "      <td>[0, 0, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 47, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you know that is tempting but is really not go...</td>\n",
       "      <td>what do you mean it will help us to relax</td>\n",
       "      <td>[0, 0, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 47, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 18, 13, 5, 161, 10, 23, 101...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 18, 13, 5, 161, 10, 23, 101, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what do you mean it will help us to relax</td>\n",
       "      <td>do you really think so i do not it will just m...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 18, 13, 5, 161, 10, 23, 101, 9...</td>\n",
       "      <td>[1, 13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, ...</td>\n",
       "      <td>[13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, 102...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>do you really think so i do not it will just m...</td>\n",
       "      <td>i guess you are right but what shall we do i d...</td>\n",
       "      <td>[13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, 102...</td>\n",
       "      <td>[1, 4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4,...</td>\n",
       "      <td>[4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i guess you are right but what shall we do i d...</td>\n",
       "      <td>i suggest a walk over to the gym where we can ...</td>\n",
       "      <td>[4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4, 13...</td>\n",
       "      <td>[1, 4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 2...</td>\n",
       "      <td>[4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 21, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i suggest a walk over to the gym where we can ...</td>\n",
       "      <td>that 's a good idea i hear mary and sally ofte...</td>\n",
       "      <td>[4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 21, ...</td>\n",
       "      <td>[1, 11, 38, 8, 47, 179, 4, 237, 441, 14, 3323,...</td>\n",
       "      <td>[11, 38, 8, 47, 179, 4, 237, 441, 14, 3323, 30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>that 's a good idea i hear mary and sally ofte...</td>\n",
       "      <td>sounds great to me if they are willing we coul...</td>\n",
       "      <td>[11, 38, 8, 47, 179, 4, 237, 441, 14, 3323, 30...</td>\n",
       "      <td>[1, 154, 99, 7, 26, 57, 54, 17, 1083, 22, 79, ...</td>\n",
       "      <td>[154, 99, 7, 26, 57, 54, 17, 1083, 22, 79, 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sounds great to me if they are willing we coul...</td>\n",
       "      <td>good let us go now</td>\n",
       "      <td>[154, 99, 7, 26, 57, 54, 17, 1083, 22, 79, 200...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 47, 74, 93, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 47, 74, 93, 59,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>good let us go now</td>\n",
       "      <td>all right</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 47, 74, 93, 59,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 50,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>can you do push ups</td>\n",
       "      <td>of course i can it is a piece of cake believe ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 5, 13, 1635...</td>\n",
       "      <td>[1, 16, 125, 4, 21, 10, 9, 8, 773, 16, 899, 25...</td>\n",
       "      <td>[16, 125, 4, 21, 10, 9, 8, 773, 16, 899, 254, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0   say how about going for a few beers after dinner   \n",
       "1  you know that is tempting but is really not go...   \n",
       "2          what do you mean it will help us to relax   \n",
       "3  do you really think so i do not it will just m...   \n",
       "4  i guess you are right but what shall we do i d...   \n",
       "5  i suggest a walk over to the gym where we can ...   \n",
       "6  that 's a good idea i hear mary and sally ofte...   \n",
       "7  sounds great to me if they are willing we coul...   \n",
       "8                                 good let us go now   \n",
       "9                                can you do push ups   \n",
       "\n",
       "                                            response  \\\n",
       "0  you know that is tempting but is really not go...   \n",
       "1          what do you mean it will help us to relax   \n",
       "2  do you really think so i do not it will just m...   \n",
       "3  i guess you are right but what shall we do i d...   \n",
       "4  i suggest a walk over to the gym where we can ...   \n",
       "5  that 's a good idea i hear mary and sally ofte...   \n",
       "6  sounds great to me if they are willing we coul...   \n",
       "7                                 good let us go now   \n",
       "8                                          all right   \n",
       "9  of course i can it is a piece of cake believe ...   \n",
       "\n",
       "                                  encoder_input_data  \\\n",
       "0  [0, 0, 0, 0, 0, 138, 33, 37, 75, 20, 8, 206, 3...   \n",
       "1  [0, 0, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 47, ...   \n",
       "2  [0, 0, 0, 0, 0, 18, 13, 5, 161, 10, 23, 101, 9...   \n",
       "3  [13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, 102...   \n",
       "4  [4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4, 13...   \n",
       "5  [4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 21, ...   \n",
       "6  [11, 38, 8, 47, 179, 4, 237, 441, 14, 3323, 30...   \n",
       "7  [154, 99, 7, 26, 57, 54, 17, 1083, 22, 79, 200...   \n",
       "8  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 47, 74, 93, 59,...   \n",
       "9  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 5, 13, 1635...   \n",
       "\n",
       "                                  decoder_input_data  \\\n",
       "0  [0, 0, 1, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 4...   \n",
       "1  [0, 0, 0, 0, 0, 1, 18, 13, 5, 161, 10, 23, 101...   \n",
       "2  [1, 13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, ...   \n",
       "3  [1, 4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4,...   \n",
       "4  [1, 4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 2...   \n",
       "5  [1, 11, 38, 8, 47, 179, 4, 237, 441, 14, 3323,...   \n",
       "6  [1, 154, 99, 7, 26, 57, 54, 17, 1083, 22, 79, ...   \n",
       "7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 47, 74, 93, ...   \n",
       "8  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 50,...   \n",
       "9  [1, 16, 125, 4, 21, 10, 9, 8, 773, 16, 899, 25...   \n",
       "\n",
       "                                 decoder_output_data  \n",
       "0  [0, 0, 5, 46, 11, 9, 3717, 29, 9, 60, 15, 47, ...  \n",
       "1  [0, 0, 0, 0, 0, 18, 13, 5, 161, 10, 23, 101, 9...  \n",
       "2  [13, 5, 60, 43, 36, 4, 13, 15, 10, 23, 48, 102...  \n",
       "3  [4, 226, 5, 17, 53, 29, 18, 325, 22, 13, 4, 13...  \n",
       "4  [4, 593, 8, 423, 140, 7, 6, 973, 105, 22, 21, ...  \n",
       "5  [11, 38, 8, 47, 179, 4, 237, 441, 14, 3323, 30...  \n",
       "6  [154, 99, 7, 26, 57, 54, 17, 1083, 22, 79, 200...  \n",
       "7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 47, 74, 93, 59,...  \n",
       "8  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 53...  \n",
       "9  [16, 125, 4, 21, 10, 9, 8, 773, 16, 899, 254, ...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the DataFrame\n",
    "data_dir = os.path.join(os.getcwd(), 'data_dd')\n",
    "file_path_parquet = os.path.join(data_dir, 'training_df_dd.parquet')\n",
    "training_data_final = pd.read_parquet(file_path_parquet)\n",
    "\n",
    "training_data_final.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad6aab19-4df8-461d-9dc1-393d1157a83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15384\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.word_index))\n",
    "print(tokenizer.num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3fbffc-0d50-455a-8d1a-2608dad745e3",
   "metadata": {},
   "source": [
    "### Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39ff9d2f-e6b8-4c47-aeb5-5af9071ad308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80874, 15)\n",
      "(8987, 15)\n",
      "(80874, 16)\n",
      "(8987, 16)\n",
      "(80874, 16)\n",
      "(8987, 16)\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data = np.array(training_data_final['encoder_input_data'].tolist())\n",
    "decoder_input_data = np.array(training_data_final['decoder_input_data'].tolist())\n",
    "decoder_output_data = np.array(training_data_final['decoder_output_data'].tolist())\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "encoder_input_train, encoder_input_val, decoder_input_train, decoder_input_val, decoder_output_train, decoder_output_val = train_test_split(\n",
    "    encoder_input_data, decoder_input_data, decoder_output_data, test_size=0.1, random_state=22)\n",
    "print(encoder_input_train.shape)\n",
    "print(encoder_input_val.shape)\n",
    "print(decoder_input_train.shape)\n",
    "print(decoder_input_val.shape)\n",
    "print(decoder_output_train.shape)\n",
    "print(decoder_output_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076347eb-91fb-4830-a259-27e6241faa67",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5feee9c-140d-4bf3-bd4b-fd7f576ce7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 100)    1538500     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  [(None, 200),        160800      ['embedding[0][0]']              \n",
      "                                 (None, 100),                                                     \n",
      "                                 (None, 100),                                                     \n",
      "                                 (None, 100),                                                     \n",
      "                                 (None, 100)]                                                     \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 200)    3077000     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 200)          0           ['bidirectional[0][1]',          \n",
      "                                                                  'bidirectional[0][3]']          \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 200)          0           ['bidirectional[0][2]',          \n",
      "                                                                  'bidirectional[0][4]']          \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 200),  320800      ['embedding_1[0][0]',            \n",
      "                                 (None, 200),                     'concatenate[0][0]',            \n",
      "                                 (None, 200)]                     'concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 15385)  3092385     ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,189,485\n",
      "Trainable params: 8,189,485\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "latent_dim = 100\n",
    "num_encoder_tokens = len(tokenizer.word_index) + 1\n",
    "num_decoder_tokens = len(tokenizer.word_index) + 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define model parameters\n",
    "latent_dim = 100\n",
    "\n",
    "# Define encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding = Embedding(num_encoder_tokens, latent_dim, mask_zero=True)(encoder_inputs)\n",
    "# Make the LSTM layer bidirectional\n",
    "encoder_lstm = Bidirectional(LSTM(latent_dim, return_state=True, dropout=0.2, kernel_regularizer=l2(0.01)))  # , recurrent_dropout=0.2) Removed recurrent_dropout for cuDNN compatibility\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedding)\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Update latent_dim to match the concatenated states\n",
    "latent_dim *= 2\n",
    "\n",
    "# Define decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(num_decoder_tokens, latent_dim, mask_zero=True)\n",
    "decoder_embedded = decoder_embedding(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.2, kernel_regularizer=l2(0.01))  # , recurrent_dropout=0.2) Removed recurrent_dropout for cuDNN compatibility\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedded, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Define checkpoint callback\n",
    "checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/models/seq2seq_dd_model.h5', save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6a4f9-b6f4-4a00-a92b-dcb869e5dda7",
   "metadata": {},
   "source": [
    "### Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d9067870-1a68-4a69-bdd4-d8abc6cce21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the directory and file path\n",
    "# data_dir = os.path.join(os.getcwd(), 'data_dd')\n",
    "# file_path_h5 = os.path.join(data_dir, 'seq2seq_dd_model_9 ep_val_loss.h5')\n",
    "\n",
    "# # Load the model\n",
    "# model = load_model(file_path_h5)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe71e88b-5724-4c14-ada4-e7f7f2ceb383",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e86cf1fe-85e1-4142-8c54-04d178b499e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1264/1264 [==============================] - 355s 281ms/step - loss: 2.3762 - val_loss: 2.6068\n",
      "Epoch 2/10\n",
      "1264/1264 [==============================] - 543s 430ms/step - loss: 2.3255 - val_loss: 2.5946\n",
      "Epoch 3/10\n",
      "1264/1264 [==============================] - 562s 445ms/step - loss: 2.2790 - val_loss: 2.5907\n",
      "Epoch 4/10\n",
      "1264/1264 [==============================] - 458s 362ms/step - loss: 2.2369 - val_loss: 2.5824\n",
      "Epoch 5/10\n",
      "1264/1264 [==============================] - 464s 367ms/step - loss: 2.1969 - val_loss: 2.5796\n",
      "Epoch 6/10\n",
      "1264/1264 [==============================] - 446s 353ms/step - loss: 2.1601 - val_loss: 2.5762\n",
      "Epoch 7/10\n",
      "1264/1264 [==============================] - 426s 337ms/step - loss: 2.1263 - val_loss: 2.5761\n",
      "Epoch 8/10\n",
      "1264/1264 [==============================] - 422s 334ms/step - loss: 2.0938 - val_loss: 2.5742\n",
      "Epoch 9/10\n",
      "1264/1264 [==============================] - 422s 333ms/step - loss: 2.0636 - val_loss: 2.5743\n",
      "Epoch 10/10\n",
      "1264/1264 [==============================] - 415s 328ms/step - loss: 2.0355 - val_loss: 2.5765\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    [encoder_input_train, decoder_input_train],\n",
    "    decoder_output_train[:, :, np.newaxis],  # Sparse categorical crossentropy expects a 3D target array\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    validation_data=([encoder_input_val, decoder_input_val], decoder_output_val[:, :, np.newaxis]),\n",
    "    # callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cda22363-777c-4bac-b55c-b8ba47fd7175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABh8ElEQVR4nO3dd3hUZd7G8e+k90JJSIOE0HtHYGmKVBEURRGFoOgKAUVXV10VAUtWQWWFV+xgQxRXwBUQASFIExDpRWoSSggtnbSZ8/4RMhKBIUDIJJP7c11zZeaZU36TGZg7z/Occ0yGYRiIiIiIOAgnexcgIiIiUpoUbkRERMShKNyIiIiIQ1G4EREREYeicCMiIiIOReFGREREHIrCjYiIiDgUhRsRERFxKAo3IiIi4lAUbkTKUExMDJGRkde07oQJEzCZTKVbUDlz+PBhTCYTs2bNKvN9m0wmJkyYYH08a9YsTCYThw8fvuK6kZGRxMTElGo91/NZEansFG5EKPxiK8lt5cqV9i610nvssccwmUzs37//sss8//zzmEwmtm3bVoaVXb1jx44xYcIEtmzZYu9SrIoC5pQpU+xdisg1c7F3ASLlweeff17s8WeffcbSpUsvam/YsOF17efDDz/EYrFc07ovvPACzz777HXt3xEMHTqUadOmMXv2bMaPH3/JZb766iuaNm1Ks2bNrnk/DzzwAPfeey/u7u7XvI0rOXbsGBMnTiQyMpIWLVoUe+56PisilZ3CjQhw//33F3u8fv16li5delH7X2VnZ+Pl5VXi/bi6ul5TfQAuLi64uOifbPv27alTpw5fffXVJcPNunXrOHToEP/+97+vaz/Ozs44Oztf1zaux/V8VkQqOw1LiZRQt27daNKkCb/99htdunTBy8uLf/3rXwAsWLCAfv36ERoairu7O9HR0bz88suYzeZi2/jrPIoLhwA++OADoqOjcXd3p23btmzcuLHYupeac2MymRgzZgzz58+nSZMmuLu707hxY3788ceL6l+5ciVt2rTBw8OD6Oho3n///RLP4/nll1+4++67qVmzJu7u7kRERPDEE09w7ty5i16fj48PR48eZeDAgfj4+FC9enWeeuqpi34XqampxMTE4O/vT0BAAMOHDyc1NfWKtUBh782ePXvYvHnzRc/Nnj0bk8nEkCFDyMvLY/z48bRu3Rp/f3+8vb3p3LkzK1asuOI+LjXnxjAMXnnlFcLDw/Hy8qJ79+7s3LnzonXPnDnDU089RdOmTfHx8cHPz48+ffqwdetW6zIrV66kbdu2AIwYMcI69Fk03+hSc26ysrL4xz/+QUREBO7u7tSvX58pU6ZgGEax5a7mc3GtUlJSeOihhwgODsbDw4PmzZvz6aefXrTcnDlzaN26Nb6+vvj5+dG0aVP+85//WJ/Pz89n4sSJ1K1bFw8PD6pWrcrf/vY3li5dWmq1SuWjPwNFrsLp06fp06cP9957L/fffz/BwcFA4Rehj48PTz75JD4+Pvz888+MHz+e9PR0Jk+efMXtzp49m4yMDP7+979jMpl44403uPPOOzl48OAV/4JfvXo13333HaNHj8bX15d33nmHQYMGkZiYSNWqVQH4/fff6d27NyEhIUycOBGz2cykSZOoXr16iV733Llzyc7OZtSoUVStWpUNGzYwbdo0jhw5wty5c4stazab6dWrF+3bt2fKlCksW7aMN998k+joaEaNGgUUhoQBAwawevVqHn30URo2bMi8efMYPnx4ieoZOnQoEydOZPbs2bRq1arYvr/55hs6d+5MzZo1OXXqFB999BFDhgzh4YcfJiMjg48//phevXqxYcOGi4aCrmT8+PG88sor9O3bl759+7J582Z69uxJXl5eseUOHjzI/Pnzufvuu4mKiuLEiRO8//77dO3alV27dhEaGkrDhg2ZNGkS48eP55FHHqFz584AdOzY8ZL7NgyD22+/nRUrVvDQQw/RokULlixZwtNPP83Ro0d5++23iy1fks/FtTp37hzdunVj//79jBkzhqioKObOnUtMTAypqak8/vjjACxdupQhQ4Zwyy238PrrrwOwe/du1qxZY11mwoQJxMXFMXLkSNq1a0d6ejqbNm1i8+bN3HrrrddVp1RihohcJDY21vjrP4+uXbsagPHee+9dtHx2dvZFbX//+98NLy8vIycnx9o2fPhwo1atWtbHhw4dMgCjatWqxpkzZ6ztCxYsMADjf//7n7XtpZdeuqgmwHBzczP2799vbdu6dasBGNOmTbO29e/f3/Dy8jKOHj1qbdu3b5/h4uJy0TYv5VKvLy4uzjCZTEZCQkKx1wcYkyZNKrZsy5YtjdatW1sfz58/3wCMN954w9pWUFBgdO7c2QCMmTNnXrGmtm3bGuHh4YbZbLa2/fjjjwZgvP/++9Zt5ubmFlvv7NmzRnBwsPHggw8WaweMl156yfp45syZBmAcOnTIMAzDSElJMdzc3Ix+/foZFovFuty//vUvAzCGDx9ubcvJySlWl2EUvtfu7u7FfjcbN2687Ov962el6Hf2yiuvFFvurrvuMkwmU7HPQEk/F5dS9JmcPHnyZZeZOnWqARhffPGFtS0vL8/o0KGD4ePjY6SnpxuGYRiPP/644efnZxQUFFx2W82bNzf69etnsyaRq6VhKZGr4O7uzogRIy5q9/T0tN7PyMjg1KlTdO7cmezsbPbs2XPF7d5zzz0EBgZaHxf9FX/w4MErrtujRw+io6Otj5s1a4afn591XbPZzLJlyxg4cCChoaHW5erUqUOfPn2uuH0o/vqysrI4deoUHTt2xDAMfv/994uWf/TRR4s97ty5c7HXsmjRIlxcXKw9OVA4x2Xs2LElqgcK50kdOXKEVatWWdtmz56Nm5sbd999t3Wbbm5uAFgsFs6cOUNBQQFt2rS55JCWLcuWLSMvL4+xY8cWG8obN27cRcu6u7vj5FT436vZbOb06dP4+PhQv379q95vkUWLFuHs7Mxjjz1WrP0f//gHhmGwePHiYu1X+lxcj0WLFlGjRg2GDBlibXN1deWxxx4jMzOT+Ph4AAICAsjKyrI5xBQQEMDOnTvZt2/fddclUkThRuQqhIWFWb8sL7Rz507uuOMO/P398fPzo3r16tbJyGlpaVfcbs2aNYs9Lgo6Z8+evep1i9YvWjclJYVz585Rp06di5a7VNulJCYmEhMTQ5UqVazzaLp27Qpc/Po8PDwuGu66sB6AhIQEQkJC8PHxKbZc/fr1S1QPwL333ouzszOzZ88GICcnh3nz5tGnT59iQfHTTz+lWbNm1vkc1atXZ+HChSV6Xy6UkJAAQN26dYu1V69evdj+oDBIvf3229StWxd3d3eqVatG9erV2bZt21Xv98L9h4aG4uvrW6y96Ai+ovqKXOlzcT0SEhKoW7euNcBdrpbRo0dTr149+vTpQ3h4OA8++OBF834mTZpEamoq9erVo2nTpjz99NPl/hB+Kf8UbkSuwoU9GEVSU1Pp2rUrW7duZdKkSfzvf/9j6dKl1jkGJTmc93JH5Rh/mSha2uuWhNls5tZbb2XhwoU888wzzJ8/n6VLl1onvv719ZXVEUZBQUHceuut/Pe//yU/P5///e9/ZGRkMHToUOsyX3zxBTExMURHR/Pxxx/z448/snTpUm6++eYbepj1a6+9xpNPPkmXLl344osvWLJkCUuXLqVx48Zldnj3jf5clERQUBBbtmzh+++/t84X6tOnT7G5VV26dOHAgQN88sknNGnShI8++ohWrVrx0UcflVmd4ng0oVjkOq1cuZLTp0/z3Xff0aVLF2v7oUOH7FjVn4KCgvDw8LjkSe9snQivyPbt2/njjz/49NNPGTZsmLX9eo5mqVWrFsuXLyczM7NY783evXuvajtDhw7lxx9/ZPHixcyePRs/Pz/69+9vff7bb7+ldu3afPfdd8WGkl566aVrqhlg37591K5d29p+8uTJi3pDvv32W7p3787HH39crD01NZVq1apZH1/NGadr1arFsmXLyMjIKNZ7UzTsWVRfWahVqxbbtm3DYrEU6725VC1ubm7079+f/v37Y7FYGD16NO+//z4vvviiteewSpUqjBgxghEjRpCZmUmXLl2YMGECI0eOLLPXJI5FPTci16noL+QL/yLOy8vj3XfftVdJxTg7O9OjRw/mz5/PsWPHrO379++/aJ7G5daH4q/PMIxih/Nerb59+1JQUMCMGTOsbWazmWnTpl3VdgYOHIiXlxfvvvsuixcv5s4778TDw8Nm7b/++ivr1q276pp79OiBq6sr06ZNK7a9qVOnXrSss7PzRT0kc+fO5ejRo8XavL29AUp0CHzfvn0xm81Mnz69WPvbb7+NyWQq8fyp0tC3b1+Sk5P5+uuvrW0FBQVMmzYNHx8f65Dl6dOni63n5ORkPbFibm7uJZfx8fGhTp061udFroV6bkSuU8eOHQkMDGT48OHWSwN8/vnnZdr9fyUTJkzgp59+olOnTowaNcr6JdmkSZMrnvq/QYMGREdH89RTT3H06FH8/Pz473//e11zN/r370+nTp149tlnOXz4MI0aNeK777676vkoPj4+DBw40Drv5sIhKYDbbruN7777jjvuuIN+/fpx6NAh3nvvPRo1akRmZuZV7avofD1xcXHcdttt9O3bl99//53FixcX640p2u+kSZMYMWIEHTt2ZPv27Xz55ZfFenwAoqOjCQgI4L333sPX1xdvb2/at29PVFTURfvv378/3bt35/nnn+fw4cM0b96cn376iQULFjBu3Lhik4dLw/Lly8nJybmofeDAgTzyyCO8//77xMTE8NtvvxEZGcm3337LmjVrmDp1qrVnaeTIkZw5c4abb76Z8PBwEhISmDZtGi1atLDOz2nUqBHdunWjdevWVKlShU2bNvHtt98yZsyYUn09UsnY5yAtkfLtcoeCN27c+JLLr1mzxrjpppsMT09PIzQ01PjnP/9pLFmyxACMFStWWJe73KHglzrslr8cmny5Q8FjY2MvWrdWrVrFDk02DMNYvny50bJlS8PNzc2Ijo42PvroI+Mf//iH4eHhcZnfwp927dpl9OjRw/Dx8TGqVatmPPzww9ZDiy88jHn48OGGt7f3RetfqvbTp08bDzzwgOHn52f4+/sbDzzwgPH777+X+FDwIgsXLjQAIyQk5KLDry0Wi/Haa68ZtWrVMtzd3Y2WLVsaP/zww0Xvg2Fc+VBwwzAMs9lsTJw40QgJCTE8PT2Nbt26GTt27Ljo952Tk2P84x//sC7XqVMnY926dUbXrl2Nrl27FtvvggULjEaNGlkPyy967ZeqMSMjw3jiiSeM0NBQw9XV1ahbt64xefLkYoemF72Wkn4u/qroM3m52+eff24YhmGcOHHCGDFihFGtWjXDzc3NaNq06UXv27fffmv07NnTCAoKMtzc3IyaNWsaf//7343jx49bl3nllVeMdu3aGQEBAYanp6fRoEED49VXXzXy8vJs1ilii8kwytGflyJSpgYOHKjDcEXE4WjOjUgl8ddLJezbt49FixbRrVs3+xQkInKDqOdGpJIICQkhJiaG2rVrk5CQwIwZM8jNzeX333+/6NwtIiIVmSYUi1QSvXv35quvviI5ORl3d3c6dOjAa6+9pmAjIg5HPTciIiLiUDTnRkRERByKwo2IiIg4lEo358ZisXDs2DF8fX2v6tTnIiIiYj+GYZCRkUFoaOhFF239q0oXbo4dO0ZERIS9yxAREZFrkJSURHh4uM1lKl24KToteFJSEn5+fnauRkREREoiPT2diIiIYheOvZxKF26KhqL8/PwUbkRERCqYkkwp0YRiERERcSgKNyIiIuJQFG5ERETEoSjciIiIiENRuBERERGHonAjIiIiDkXhRkRERByKwo2IiIg4FIUbERERcSgKNyIiIuJQFG5ERETEoSjciIiIiENRuCktBXmQvAPysuxdiYiISKVW6a4KfsOc3g/vdSq87xsCVWr/easa/ed9N2/71ikiIuLg7Bpu4uLi+O6779izZw+enp507NiR119/nfr169tcLzU1leeff57vvvuOM2fOUKtWLaZOnUrfvn3LqPJLyD4NnoFw7ixkHC+8Jay5eDmfGufDTtT5wHNB8HH3Kfu6RUREHIxdw018fDyxsbG0bduWgoIC/vWvf9GzZ0927dqFt/elezjy8vK49dZbCQoK4ttvvyUsLIyEhAQCAgLKtvi/iuoMzxyG7DNw5hCcOQhnDhT+PH3+57kzkJlceLtk8An+M+xUrX1B+IkCd98yf0kiIiIVkckwDMPeRRQ5efIkQUFBxMfH06VLl0su89577zF58mT27NmDq6vrVe8jPT0df39/0tLS8PPzu96Sr865s+fDzsGLw8+5M7bX9QkuHnYuHOpS8BEREQd3Nd/f5Src7N+/n7p167J9+3aaNGlyyWX69u1LlSpV8PLyYsGCBVSvXp377ruPZ555Bmdn54uWz83NJTc31/o4PT2diIgI+4QbW4qCz5lDf/b0FAWg7NO21/UOumBuT1TxoS6PcvQaRURErtHVhJtyM6HYYrEwbtw4OnXqdNlgA3Dw4EF+/vlnhg4dyqJFi9i/fz+jR48mPz+fl1566aLl4+LimDhx4o0svXR4BkJY68LbX51LvSDsHCw+1JV9CrJSCm9J6y9e17v6ZYa6FHxERMQxlZuem1GjRrF48WJWr15NeHj4ZZerV68eOTk5HDp0yNpT89ZbbzF58mSOHz9+0fIVpufmWp1LhbNFvT2Hig91ZZ+yva5XtQuGt/4y3OXhXybli4iIlESF67kZM2YMP/zwA6tWrbIZbABCQkJwdXUtNgTVsGFDkpOTycvLw83Nrdjy7u7uuLu735C6ywXPAPBsCaEtL34uJ+2Cnp6DxYe6sk4Whp/sU5D068XrungWHrZ+0c0HXL2KP3bzuuC+9/nnfS5e19ULTKYb/isREZHKza7hxjAMxo4dy7x581i5ciVRUVFXXKdTp07Mnj0bi8WCk1PhOQj/+OMPQkJCLgo2lZ6Hf2HouWTwSS8edi6c65OVAgXnCm9X6v25KqbiQadYAPrLY1fvkgcoF3eFJhERsbLrsNTo0aOZPXs2CxYsKHZuG39/fzw9PQEYNmwYYWFhxMXFAZCUlETjxo0ZPnw4Y8eOZd++fTz44IM89thjPP/881fcp12PlqoocjMKD2nPyyq85Wf9eT8vE/KyL7h/qWX+slz+DT5rs8n5LwHogh4mz4DCI818axSeY8g3+M+f7n4KRSIiFUSFGZaaMWMGAN26dSvWPnPmTGJiYgBITEy09tAAREREsGTJEp544gmaNWtGWFgYjz/+OM8880xZle343H1L9/ByiwXyswtvFwaiSwWl/EsEp7/eioJUQU7h9g0z5KYV3q6Gi2fxsPPXn74hhfe9qigEiYhUIOVmQnFZUc+NAzEXXNBjdKnglFV4/qCME4UnTrzwZ15Gyffj5Hq+98dGEPKpUXhkmnO5mMYmIuJwKkzPjch1cXYBZ/9rO7IrLwsykiHzhO2f586AJR/SjxTebDIVBpyLwk+NC4bGzv90ceBJ7iIidqZwI5WTm3fhYe9Vo20vV5ALmSkXhJ7kS/cEZaWAYfnznENst71dj4BLhx6f88NhRfd1vTERkaumcCNii4s7BEQU3myxmAsPr79Sb1DmCTDnQU5q4e3kHtvbdfMpDDleVcDFA1w9C2tyOf/T1bOw3cUDXD3+vF+ixxds64J5bSIiFZ3CjUhpcHIu7G3xrWF7OcM4f+V4G71ART/zz88dOpNZeLj+jeTsdokg9NcQVcLHtkKVq+f5m1fhPjVRW0RuAIUbkbJkMhX2wnhVgeBGtpfNzfgz7Jw7WzhEln+u8Cixolt+jo3HRcvnnj9v0V8eWwr+3Jc5r/CWm35jX/+FTE6FIefCwFPsZ0nbLvfc+ftOF19zTkQcm8KNSHlVdEh+tTo3Zvvmgj9D0KXCz0XByUZQKvbYRvDKzy48dB8K5yjlZRbebiRn9+sMT5doc/G4oNfpgt6nS/VEFWu71LKXW990dW3XtE3T+Yn5boU3BUG5FMMAcz6Yc8//zCv8t25tyyu8X5D75x9K7n4Q2cluJSvciFRWzi7g7FP2k5bN+efPe3Tugp/nLtFm67kStFn3l1t4y0kt29dZEZmcCk994OwGzq7nb25//nT6a1tR+wUB6a/rOf1lG8XWu8z2rvjchYHsBs8XKzpbivWsKcbFz3Ety1xmvaLwYM6Dgrw/71vbcq8+aFx2W7bacovXcrUiboKHllz9eqVE4UZEypaz67Ufwl9ShvFnb9NVhaaShqdzRTu6YJ8X3SnZl11ZtZWEYfkzDFYURYHMZLpycCj22MYycgWmwjl21kDrBi5F990L/41Xq2vXChVuRMTxmEx/DilRxd7VlC8Xfrkblj//OrcUXPAXfP4F7fnF/4I3X7Dc5dYx559fL6942yW3aWu9v24jj4sCSFEgcygXhgfXPwODzbZLBY2StpVk+0VtbhXiZKXlv0IRESk9F87BMTkXzrNx9bBfPVfLYr50WLKyMQfJ5pykksxxutQy17GdS7UVzX3SkYTXReFGREQqDidncCrqlRO5NJ25S0RERByKwo2IiIg4FIUbERERcSgKNyIiIuJQFG5ERETEoSjciIiIiENRuBERERGHonAjIiIiDkXhRkRERByKwo2IiIg4FIUbERERcSgKNyIiIuJQFG5ERETEoSjciIiIiENRuBERERGHonAjIiIiDkXhRkRERByKwo2IiIg4FIUbERERcSgKNyIiIuJQFG5ERETEoSjciIiIiENRuBERERGHonAjIiIiDkXhRkRERByKwo2IiIg4FIUbERERcSgKNyIiIuJQFG5ERETEoSjciIiIiENRuBERERGHonAjIiIiDkXhRkRERByKwo2IiIg4FIUbERERcSgKNyIiIuJQ7Bpu4uLiaNu2Lb6+vgQFBTFw4ED27t1rc51Zs2ZhMpmK3Tw8PMqoYhERESnv7Bpu4uPjiY2NZf369SxdupT8/Hx69uxJVlaWzfX8/Pw4fvy49ZaQkFBGFYuIiEh552LPnf/444/FHs+aNYugoCB+++03unTpctn1TCYTNWrUuNHliYiISAVUrubcpKWlAVClShWby2VmZlKrVi0iIiIYMGAAO3fuLIvyREREpAIoN+HGYrEwbtw4OnXqRJMmTS67XP369fnkk09YsGABX3zxBRaLhY4dO3LkyJFLLp+bm0t6enqxm4iIiDguk2EYhr2LABg1ahSLFy9m9erVhIeHl3i9/Px8GjZsyJAhQ3j55Zcven7ChAlMnDjxova0tDT8/Pyuq2YREREpG+np6fj7+5fo+7tc9NyMGTOGH374gRUrVlxVsAFwdXWlZcuW7N+//5LPP/fcc6SlpVlvSUlJpVGyiIiIlFN2nVBsGAZjx45l3rx5rFy5kqioqKvehtlsZvv27fTt2/eSz7u7u+Pu7n69pYqIiEgFYddwExsby+zZs1mwYAG+vr4kJycD4O/vj6enJwDDhg0jLCyMuLg4ACZNmsRNN91EnTp1SE1NZfLkySQkJDBy5Ei7vQ4REREpP+wabmbMmAFAt27dirXPnDmTmJgYABITE3Fy+nP07OzZszz88MMkJycTGBhI69atWbt2LY0aNSqrskVERKQcKzcTisvK1UxIEhERkfKhwk0oFhERESktCjciIiLiUBRuRERExKEo3IiIiIhDUbgRERERh6JwIyIiIg5F4UZEREQcisKNiIiIOBSFGxEREXEoCjciIiLiUBRuRERExKEo3IiIiIhDUbgRERERh6JwIyIiIg5F4UZEREQcisKNiIiIOBSFGxEREXEoCjciIiLiUBRuRERExKEo3IiIiIhDUbgRERERh6JwIyIiIg5F4UZEREQcisKNiIiIOBSFGxEREXEoCjciIiLiUBRuRERExKEo3IiIiIhDUbgRERERh6JwIyIiIg5F4UZEREQcisKNiIiIOBSFGxEREXEoCjciIiLiUBRuRERExKEo3IiIiIhDUbgRERERh6JwIyIiIg5F4UZEREQcisKNiIiIOBSFGxEREXEoCjciIiLiUBRuRERExKEo3IiIiIhDUbgRERERh6JwIyIiIg5F4UZEREQcisKNiIiIOBS7hpu4uDjatm2Lr68vQUFBDBw4kL1795Z4/Tlz5mAymRg4cOCNK1JEREQqFLuGm/j4eGJjY1m/fj1Lly4lPz+fnj17kpWVdcV1Dx8+zFNPPUXnzp3LoFIRERGpKFzsufMff/yx2ONZs2YRFBTEb7/9RpcuXS67ntlsZujQoUycOJFffvmF1NTUG1ypiIiIVBTlas5NWloaAFWqVLG53KRJkwgKCuKhhx4qi7JERESkArFrz82FLBYL48aNo1OnTjRp0uSyy61evZqPP/6YLVu2lGi7ubm55ObmWh+np6dfb6kiIiJSjpWbnpvY2Fh27NjBnDlzLrtMRkYGDzzwAB9++CHVqlUr0Xbj4uLw9/e33iIiIkqrZBERESmHTIZhGPYuYsyYMSxYsIBVq1YRFRV12eW2bNlCy5YtcXZ2trZZLBYAnJyc2Lt3L9HR0cXWuVTPTUREBGlpafj5+ZXyKxEREZEbIT09HX9//xJ9f9t1WMowDMaOHcu8efNYuXKlzWAD0KBBA7Zv316s7YUXXiAjI4P//Oc/l+yVcXd3x93dvVTrFhERkfLLruEmNjaW2bNns2DBAnx9fUlOTgbA398fT09PAIYNG0ZYWBhxcXF4eHhcNB8nICAAwOY8HREREak87BpuZsyYAUC3bt2Ktc+cOZOYmBgAEhMTcXIqN1ODREREpJwrF3NuytLVjNmJiIhI+XA139/qEhERERGHonAjIiIiDkXhRkRERByKwo2IiIg4FIUbERERcSgKNyIiIuJQFG5KiWEYPPPtNtbuP2XvUkRERCo1hZtS8v3WY3y9KYn7PvqV2NmbOZZ6zt4liYiIVEoKN6WkW70ghneohZMJFm47zi1vxvN/K/aTW2C2d2kiIiKVis5QXMp2HUvnpe93sPHwWQCiqnnzUv9GdKsfVOr7EhERqSyu5vtb4eYGMAyD+VuO8tqiPZzMyAXg1kbBjL+tERFVvG7IPkVERByZLr9gZyaTiTtahvPzP7rycOcoXJxMLN11gh5vxfP20j/IyddQlYiIyI2inpsysO9EBi99v5O1B04DEB7oyfjbGnFro2BMJlOZ1CAiIlKRaVjKBntdFdwwDBZtT+aVhbs4npYDQNd61Zlwe2OiqnmXWR0iIiIVkcKNDfYKN0Wy8wqY/vN+PvzlIPlmAzdnJ0Z2jmLMzXXwcnMp83pEREQqAoUbG+wdboocPJnJxP/tIv6PkwCE+HvwfL+G9GsaoqEqERGRv1C4saG8hBsoHKpatjuFif/byZGzhSf96xhdlYm3N6ZusK9daxMRESlPFG5sKE/hpkhOvpn34g8wY+UBcgssuDiZiOkYyeM96uLr4Wrv8kREROxOh4JXMB6uzozrUY9lT3alZ6NgCiwGH60+RPcp8Xy3+QiVLH+KiIhcF/XclEMr96Yw8X+7OHQqC4A2tQKZOKAxjUP97VyZiIiIfWhYyoaKEG4AcgvMfLz6ENOW7+dcvhknE9x/Uy3+cWt9/L00VCUiIpWLhqUcgLuLM6O71WH5P7pyW7MQLAZ8ti6B7m+uZM6GRCyWSpVJRURESkw9NxXE2gOneGnBTvalZALQPNyfSQOa0DwiwL6FiYiIlAENS9lQUcMNQL7ZwqdrDzN12T4ycwswmeCeNhE83as+VX3c7V2eiIjIDaNhKQfl6uzEyM61+fmprtzZKgzDgDkbk+g+ZSWfrTuMWUNVIiIi6rmpyDYdPsP4BTvZdTwdgEYhfkwa0Jg2kVXsXJmIiEjp0rCUDY4UbgDMFoPZvyYwecle0nMKALizZRjP9mlAkJ+HnasTEREpHRqWqkScnUw80CGSFU91Y0i7CEwm+O73o9z8Zjwf/XKQfLPF3iWKiIiUqWsKN0lJSRw5csT6eMOGDYwbN44PPvig1AqTq1PVx524O5sxb3Qnmof7k5lbwCsLd9P3P7+wdv8pe5cnIiJSZq4p3Nx3332sWLECgOTkZG699VY2bNjA888/z6RJk0q1QLk6LSICmDe6E68PakoVbzf2pWRy30e/Ejt7M8dSz9m7PBERkRvumsLNjh07aNeuHQDffPMNTZo0Ye3atXz55ZfMmjWrNOuTa+DkZOKetjVZ8Y9uDO9QCycTLNx2nFvejOf/Vuwnt8Bs7xJFRERumGsKN/n5+bi7F55XZdmyZdx+++0ANGjQgOPHj5dedXJd/L1cmTigCf8b+zfa1ArkXL6ZyUv20nvqL6zcm2Lv8kRERG6Iawo3jRs35r333uOXX35h6dKl9O7dG4Bjx45RtWrVUi1Qrl/jUH/mPtqBt+9pTnVfdw6dyiJm5kYe/mwTSWey7V2eiIhIqbqmcPP666/z/vvv061bN4YMGULz5s0B+P77763DVVK+mEwm7mgZzs//6MrIv0Xh7GRi6a4T9HgrnreX/kFOvoaqRETEMVzzeW7MZjPp6ekEBgZa2w4fPoyXlxdBQUGlVmBpc7Tz3FyrP05kMOH7naw9cBqA8EBPxt/WiFsbBWMymexcnYiISHE3/Dw3586dIzc31xpsEhISmDp1Knv37i3XwUb+VC/Yly9Htuf/7mtFiL8HR86e45HPfyNm5kYOncqyd3kiIiLX7JrCzYABA/jss88ASE1NpX379rz55psMHDiQGTNmlGqBcuOYTCb6NQth2ZNdGd0tGldnE/F/nKTX26uIW7yb9Jx8e5coIiJy1a4p3GzevJnOnTsD8O233xIcHExCQgKfffYZ77zzTqkWKDeet7sL/+zdgCXjutC1XnXyzBbejz9I1zdW8MnqQzp0XEREKpRrCjfZ2dn4+voC8NNPP3HnnXfi5OTETTfdREJCQqkWKGWndnUfZo1oy0fD2lAnyIez2flM+mEXPd6K5/utx7DoquMiIlIBXFO4qVOnDvPnzycpKYklS5bQs2dPAFJSUir1JF1HYDKZ6NEomB8f70zcnU2p7utO0plzPPbV7wx8dw1rD+hSDiIiUr5dU7gZP348Tz31FJGRkbRr144OHToAhb04LVu2LNUCxT5cnJ0Y0q4m8U9348lb6+Ht5sy2I2nc9+GvjJi5gb3JGfYuUURE5JKu+VDw5ORkjh8/TvPmzXFyKsxIGzZswM/PjwYNGpRqkaVJh4Jfm1OZubyzfB+zf02kwGLgZIK7Wofz5K31qeHvYe/yRETEwV3N9/c1h5siRVcHDw8Pv57NlBmFm+tz8GQmk5fsZfGOZAA8XJ146G9R/L1rNH4ernauTkREHNUNP8+NxWJh0qRJ+Pv7U6tWLWrVqkVAQAAvv/wyFovlmoqWiqF2dR9m3N+a/47qSJtageTkW/i/FQfo+sYKZq45RF6B3n8REbGva+q5ee655/j444+ZOHEinTp1AmD16tVMmDCBhx9+mFdffbXUCy0t6rkpPYZhsHTXCf794x4Oniw88V/NKl78s3d9+jUN0ZmORUSk1NzwYanQ0FDee+8969XAiyxYsIDRo0dz9OjRq91kmVG4KX0FZgtfb0ri7aX7OJWZC0DzcH+e69uQm2rrQqoiInL9bviw1JkzZy45abhBgwacOXPmWjYpFZiLsxND29ci/uluPNGjHl5uzmw9ksa9H6znoVkb+eOEjqwSEZGyc03hpnnz5kyfPv2i9unTp9OsWbPrLkoqJm93Fx7vUZf4p7tz/001cXYysXxPCr2nruLZ/27jRHqOvUsUEZFK4JrCzRtvvMEnn3xCo0aNeOihh3jooYdo1KgRs2bNYsqUKSXeTlxcHG3btsXX15egoCAGDhzI3r17ba7z3Xff0aZNGwICAvD29qZFixZ8/vnn1/Iy5Aap7uvOKwOb8tMTXejduAYWA+ZsTKLr5BVMWbKXDF2zSkREbqBrCjddu3bljz/+4I477iA1NZXU1FTuvPNOdu7ceVVBIz4+ntjYWNavX8/SpUvJz8+nZ8+eZGVd/qrUVapU4fnnn2fdunVs27aNESNGMGLECJYsWXItL0VuoOjqPrz3QGv+O6oDrc8fWTV9xX66Tl7Jp2sP68gqERG5Ia77PDcX2rp1K61atcJsvrYLLZ48eZKgoCDi4+Pp0qVLiddr1aoV/fr14+WXX77isppQbB+GYbBk5wne+HEPB08VhtfIql483asBfZvW0JFVIiJi0w2fUHyjpKWlAYW9MyVhGAbLly9n7969lw1Dubm5pKenF7tJ2TOZTPRuUoMlT3ThlYFNqObjxuHT2cTO3swd765lwyFNRBcRkdJRbsKNxWJh3LhxdOrUiSZNmthcNi0tDR8fH9zc3OjXrx/Tpk3j1ltvveSycXFx+Pv7W28RERE3onwpIVdnJ+6/qRYrn+7O47fUxcvNmS1JqQx+fx0jP93E/hQdWSUiIten3AxLjRo1isWLF7N69eorXsrBYrFw8OBBMjMzWb58OS+//DLz58+nW7duFy2bm5tLbm6u9XF6ejoREREalionUjJy+M+yfczZmIT5/DWr7mlbkyd61CXIT9esEhGRQjfsJH533nmnzedTU1OJj4+/6nAzZswYFixYwKpVq4iKirqqdQFGjhxJUlJSiSYVa85N+bQ/JZM3ftzDT7tOAODp6szDnaN4pGs0Pu4udq5ORETs7Wq+v6/qW8Pf3/+Kzw8bNqzE2zMMg7FjxzJv3jxWrlx5TcEGCntyLuydkYqnTpAPHwxrw8bDZ3ht0W5+T0zlnZ/38+WviTzeoy5D2tXE1bncjKKKiEg5VqrDUldr9OjRzJ49mwULFlC/fn1ru7+/P56engAMGzaMsLAw4uLigMI5NG3atCE6Oprc3FwWLVrEs88+y4wZMxg5cuQV96mem/LPMAx+3JHMG0v2cuj8kVVR1bz5Z6/69G6iI6tERCqjG9ZzU9pmzJgBcNFcmZkzZxITEwNAYmIiTk5//sWelZXF6NGjOXLkCJ6enjRo0IAvvviCe+65p6zKlhvMZDLRp2kIPRoFM2dDIlOX7ePQqSxGfbmZVjUDeK5vQ9pGluyIOhERqXzs2nNjD+q5qXgycwv4IP4AH/5yiHP5hfO5ejYK5p+9G1AnyMfO1YmISFm44VcFr8gUbiqulPQc3l62j683JmIxwNnJxD1tIxjXoy5BvjqySkTEkSnc2KBwU/HtT8ng34v3smx34ZFVXm7OPNy5Ng93qa0jq0REHJTCjQ0KN47j14OneW3xHrYmpQJQzcedcT3qck/bCB1ZJSLiYBRubFC4cSyGYbB4RzJv/LiHw6ezAahdzZt/9m5Ar8bBOrJKRMRBKNzYoHDjmPIKLHy1IZF3lu/jdFYeAK1rBfJcnwa00ZFVIiIVnsKNDQo3ji0jJ58PVh3kw18OkpNvAeBvdarx2C11aRelkCMiUlEp3NigcFM5nEjPYeqyP5i76QgFlsKPeIfaVXm8R11uql3VztWJiMjVUrixQeGmckk6k82M+APM3ZREvrnwo94uqgrjbqlLh+iqmpMjIlJBKNzYoHBTOR1NPceMlfv5ZuMR8syFw1VtIwN57Ja6/K1ONYUcEZFyTuHGBoWbyu142jneW3mArzYmkVdQGHJa1QzgsVvq0rVedYUcEZFySuHGBoUbgcI5Oe/FH2D2r4nkng85zSMCePyWOnSvH6SQIyJSzijc2KBwIxdKSc/hg1UH+eLXBOvRVU3D/Hnslrr0aKiQIyJSXijc2KBwI5dyMiOXj345yGfrEqwX52wc6sdjt9Tl1obBODkp5IiI2JPCjQ0KN2LL6cxcPlp9iM/WHiYrrzDkNKjhy+O31KVX4xoKOSIidqJwY4PCjZTE2aw8Pl59iFlrD5OZWwBA/WBfxt5Shz5NQnBWyBERKVMKNzYo3MjVSM3O45M1h5m55hAZOYUhp06QD2NvrsNtzUIVckREyojCjQ0KN3It0s7lM2vNYT5efZD08yGndnVvxt5ch/7NQnHRVchFRG4ohRsbFG7keqTn5PPpmsN8tPoQaefyAYiq5k1s9zoMbKGQIyJyoyjc2KBwI6UhIyefz9Yl8NEvBzmbXRhyalbxYkz3OtzRKgxXhRwRkVKlcGODwo2UpqzcAj5fn8CHqw5yOisPgPBAT2K712FQq3DcXBRyRERKg8KNDQo3ciNk5xXw5fpE3l91gFOZhSEnLMCTUd2iubtNOO4uznauUESkYlO4sUHhRm6kc3lmZm9I5L34A5zMyAUgxN+DUd2iGdwmAg9XhRwRkWuhcGODwo2UhZx8M3M2JDIj/gAn0gtDTrCfO6O6RnNvu5oKOSIiV0nhxgaFGylLOflm5m5K4t2VBzielgNAkK87f+8azX3tauLpppAjIlISCjc2KNyIPeQWmPn2tyO8u+IAR1PPAVDNx52/d6nN0Jtq4uXmYucKRUTKN4UbGxRuxJ7yCix8t/kI01fs58jZwpBT1duNh7vU5oGbauHtrpAjInIpCjc2KNxIeZBvtjDv96P834r9JJzOBiDQy5WRnWszvGMkPgo5IiLFKNzYoHAj5UmB2cKCLceYvmI/h05lARDg5cpDnaIY3ikSPw9XO1coIlI+KNzYoHAj5VGB2cL/th1j2s/7OXiyMOT4ebjw4N+iGNEpCn9PhRwRqdwUbmxQuJHyzGwx+OF8yNmfkgmAr4cLIzpGEtMpiirebnauUETEPhRubFC4kYrAYjFYtOM405bvZ++JDAA8XZ25t10EIzvXJizA084VioiULYUbGxRupCKxWAyW7Ezm3ZUH2H40DQAXJxMDW4bxaNfa1AnytXOFIiJlQ+HGBoUbqYgMw2DN/tO8u3I/aw+ctrb3bBTMqG7RtKwZaMfqRERuPIUbGxRupKLbkpTKeysPsGRXMkX/ejvUrsqobtF0rlsNk8lk3wJFRG4AhRsbFG7EUexPyeD9+IPM+/0oBZbCf8ZNwvwY1bUOvZvUwNlJIUdEHIfCjQ0KN+JojqWe46NfDvHVhkTO5ZsBiKrmzd+71OaOVmG4u+j6VSJS8Snc2KBwI47qbFYen647zKy1h0nNzgcKL9I5snMU97WvpbMei0iFpnBjg8KNOLqs3ALmbEzio18OWq9E7ufhwvCOkcR0jKSqj7udKxQRuXoKNzYo3EhlkVdgYf6Wo7wXf8B61mMPVyfuaRPBw11qEx7oZecKRURKTuHGBoUbqWwsFoOfdhWeK2fbkcJz5Tg7mRjQPJRHu0VTL1jnyhGR8k/hxgaFG6msDMNg3YHTvLvyAKv3n7K292hYeK6c1rV0rhwRKb8UbmxQuBGBbUdSeS/+AIt3/HmunHZRVRjdLZqu9arrXDkiUu4o3NigcCPypwMnM/kg/iDf/X6EfHPhfwUNQ/wY1S2avk1q4OLsZOcKRUQKKdzYoHAjcrHktBw++uUgszckkp1XeK6cWlW9eKRLbQa1CsfDVefKERH7UrixQeFG5PJSs/P4bF0CM9cc4uz5c+VU93Xnob9FMbR9TXw9XO1coYhUVgo3NijciFxZdl4BX29M4sNVBzl2/lw5vh4uPHBTLUZ0iqK6r86VIyJlS+HGBoUbkZLLN1tYsOUY78UfYH9KJgDuLk4MbhPBI11qE1FF58oRkbKhcGODwo3I1bNYDJbtPsG7Kw+wJSkVKDxXTv9mITzaLZoGNfRvSURurKv5/rbroRBxcXG0bdsWX19fgoKCGDhwIHv37rW5zocffkjnzp0JDAwkMDCQHj16sGHDhjKqWKRycnIy0bNxDeaN7shXD99El3rVMVsM5m85Ru+pv/DgrI1sOnzG3mWKiAB2Djfx8fHExsayfv16li5dSn5+Pj179iQrK+uy66xcuZIhQ4awYsUK1q1bR0REBD179uTo0aNlWLlI5WQymegQXZXPHmzHD2P/Rr9mIZhM8POeFO56bx13v7eWn/ecoJJ1CItIOVOuhqVOnjxJUFAQ8fHxdOnSpUTrmM1mAgMDmT59OsOGDbvi8hqWEildh05l8cGqA/z3t6PkmS0ANKjhy6hu0fRrGqJz5YhIqagww1J/lZZWeN2bKlWqlHid7Oxs8vPzL7tObm4u6enpxW4iUnqiqnkTd2czfnmmO3/vUhtvN2f2JGfw+JwtdH9zJZ+vTyAn32zvMkWkEik3PTcWi4Xbb7+d1NRUVq9eXeL1Ro8ezZIlS9i5cyceHh4XPT9hwgQmTpx4Ubt6bkRujLTsfD5ff5hP1hzmTFYeANV83Hnwb5Hcf1Mt/HSuHBG5BhXyaKlRo0axePFiVq9eTXh4eInW+fe//80bb7zBypUradas2SWXyc3NJTc31/o4PT2diIgIhRuRG+xcnplvNiXxwaqDHE09B4CPuwt3tQ4npmMkkdW87VyhiFQkFS7cjBkzhgULFrBq1SqioqJKtM6UKVN45ZVXWLZsGW3atCnxvjTnRqRs5Zst/LDtGDNWHuCPE4XnyjGZ4Ob6QcR0iuRvdarpQp0ickUVJtwYhsHYsWOZN28eK1eupG7duiVa74033uDVV19lyZIl3HTTTVe1T4UbEfuwWAxW7z/FzDWHWLH3pLW9TpAPMR0jubNVGF5uLnasUETKswoTbkaPHs3s2bNZsGAB9evXt7b7+/vj6ekJwLBhwwgLCyMuLg6A119/nfHjxzN79mw6depkXcfHxwcfH58r7lPhRsT+Dp3K4tO1h5m7KYms8xfq9PNw4d52NXngplo687GIXKTChJvLdUXPnDmTmJgYALp160ZkZCSzZs0CIDIykoSEhIvWeemll5gwYcIV96lwI1J+ZOTk8+1vR/h07WEOn84GwMkEtzYKJqZjFDfVrqIhKxEBKlC4sQeFG5Hyx2IxWPlHCjPXHOaXfaes7Q1q+DKiUyQDWoTh4epsxwpFxN4UbmxQuBEp3/adyGDW2sN8t/ko586fHyfQy5Uh7WryQIdahPh72rlCEbEHhRsbFG5EKoa07Hy+3pTIp2sTrIeSOzuZ6N2kBiM6RtK6VqCGrEQqEYUbGxRuRCoW8/krks9cc4j1B/+8OGfTMH9iOkZyW/MQ3F00ZCXi6BRubFC4Eam4dh1L59O1h5m/5Si5BYXXsarm48Z97Wtxf/uaBPldfJZyEXEMCjc2KNyIVHxnsvL4akMin69LIDk9BwBXZxP9moYQ0ymKFhEB9i1QREqdwo0NCjcijiPfbGHJzmRmrTnMpoSz1vaWNQOI6RhJ36YhuOqq5CIOQeHGBoUbEce0/UgaM9ce4oetx8kzFw5ZBfu5c3/7WtzXviZVfdztXKGIXA+FGxsUbkQc28mMXGb/msgXvyZwMqPworluLk7c3jyUmI6RNAnzt3OFInItFG5sULgRqRzyCiws2n6cmWsOsfVImrW9XWQVYjpF0rNRMC4ashKpMBRubFC4Eal8NieeZdaawyzafpwCS+F/eaH+HjzQIZIh7SII8HKzc4UiciUKNzYo3IhUXslpOXz5awKzf03kdFYeAB6uTtzRMoyYjlHUr+Fr5wpF5HIUbmxQuBGRnHwz/9t6jJlrDrPreLq1vWN0VUZ0iuLmBkE4O+nsxyLlicKNDQo3IlLEMAw2Hj7LzDWHWLIzmfMjVkRU8WR4h0jubhOBv6erfYsUEUDhxiaFGxG5lKOp5/h8XQJfbUgk7Vw+AF5uzgxqFc7wjpHUCfKxc4UilZvCjQ0KNyJiy7k8M/O3HGXmmkP8cSLT2t6lXnVGdIqka93qOGnISqTMKdzYoHAjIiVhGAbrDpzmkzWHWb7nBEX/U0ZV82Z4h1rc0SpcQ1YiZUjhxgaFGxG5Womns/ls3WG+3pRERk4BUHiUVb+modzXPoJWNQMxmdSbI3IjKdzYoHAjItcqK7eA/24+whfrE4oNWdUN8mFIu5rc2SpM58wRuUEUbmxQuBGR62UYBpsTU/lqQyI/bDtGTn7htazcXJzo26QG97arSfuoKurNESlFCjc2KNyISGlKz8lnwe9H+WpDUrFz5tSu5s297SIY1CpcF+0UKQUKNzYo3IjIjWAYBtuPpvHVhkS+33KMrDwzAK7OJno2rsGQtjXpGF1VR1qJXCOFGxsUbkTkRsvMLeB/W48xZ0NisYt21qzixT1tI7i7TThBvh52rFCk4lG4sUHhRkTK0s5jaczZkMT834+SkVt4pJWLk4lbGgZxb7uadKlbXZd6ECkBhRsbFG5ExB6y8wpYuO04X21IZHNiqrU9LMCTwW0iGNw2nBB/T/sVKFLOKdzYoHAjIva2NzmDORsT+W7zUeulHpxM0L1+EEPa1aRb/eq4ODvZuUqR8kXhxgaFGxEpL3Lyzfy4I5nZGxLZcOiMtT3Yz5172kQwuG0E4YFedqxQpPxQuLFB4UZEyqP9KZl8vTGR/24+ypmsPABMJuhStzpD2kVwS8NgXNWbI5WYwo0NCjciUp7lFpj5aecJ5mxMZM3+09b2aj7u3N0mnHvbRlCrqrcdKxSxD4UbGxRuRKSiSDidxZyNSczddIRTmbnW9k51qnJv25r0bByMu4uzHSsUKTsKNzYo3IhIRZNvtrB89wm+2pDEqn0nrVcor+LtxqBWYdzbribR1X3sW6TIDaZwY4PCjYhUZElnsvlmUxLfbEriRPqfvTntoqpwX7ua9G5SAw9X9eaI41G4sUHhRkQcQYHZwsq9J/lqQyIr9qZgOf8/ub+nK3e2CmNIu5rUC/a1b5EipUjhxgaFGxFxNMfTzvHNxiN8symJo6nnrO2tawVyb9sIbmsWiqebenOkYlO4sUHhRkQcldlisGrfSeZsSGTZ7hTM57tzfD1cGNgijHvbRdA41N/OVYpcG4UbGxRuRKQySEnPYe5vR/h6YxKJZ7Kt7c3D/bm3XU36Nw/Fx93FjhWKXB2FGxsUbkSkMrFYDNYeOM1XGxP5aWcy+ebC//K93Zy5vUUog9tE0CIiAJNJF++U8k3hxgaFGxGprE5l5vLd5iN8tSGJQ6eyrO11g3y4u004d7QMp7qvux0rFLk8hRsbFG5EpLIzDINfD53h641JLN5xnJx8CwAuTia6NwhicJsIutWvrss9SLmicGODwo2IyJ/Sc/L5YetxvtmUxJakVGt7NR837mwVzt2tw6mrQ8qlHFC4sUHhRkTk0vadyGDub0f4bvMRTmXmWdtbRAQwuE0EtzUPwc/D1Y4VSmWmcGODwo2IiG35Zgsr9qQw97cj/Lznz0PKPVyd6NskhLvahHNTVFWcnDQJWcqOwo0NCjciIiWXkpHD/N+P8s2mI+xPybS2R1Tx5O7WEQxqHU5YgKcdK5TKQuHGBoUbEZGrZxgGW5JS+WbTEX7YeoyM3AIATCb4W51q3N0mgp6NgnVdK7lhFG5sULgREbk+5/LM/LjzON9sPMK6g6et7X4eLgxoEcbgNhE0CfPTuXOkVCnc2KBwIyJSepLOZDP3tyP897cjxa5r1aCGL3e3iWBgi1Cq+ujcOXL9FG5sULgRESl9RWdC/mZTEj/uTCavoPDcOa7OJno0DObuNuF0qVsdF507R66Rwo0NCjciIjdWWnY+3287xtxNSWw7kmZtD/J1Z1DrwnPn1K7uY8cKpSJSuLFB4UZEpOzsSU5n7qYjzPv9KGey/jx3TptagQxuE0HfZiG6gKeUyNV8f9u1fzAuLo62bdvi6+tLUFAQAwcOZO/evTbX2blzJ4MGDSIyMhKTycTUqVPLplgREblqDWr48eJtjVj/3C28d38rbm4QhJMJNiWc5Z//3Ua7V5fx1NytbDh0hkr2t7bcQHYNN/Hx8cTGxrJ+/XqWLl1Kfn4+PXv2JCsr67LrZGdnU7t2bf79739To0aNMqxWRESulZuLE72bhPBJTFvWPXcLz/RuQO1q3mTnmfn2tyMMfn8d3aes5P9W7Cc5Lcfe5UoFV66GpU6ePElQUBDx8fF06dLlistHRkYybtw4xo0bV+J9aFhKRKR8MAyDzYln+WbjEX7YdoysPDMATiboUq86d7eOoEejINxddO4cubrv73I10JmWVjjxrEqVKqW2zdzcXHJzc62P09PTS23bIiJy7UwmE61rVaF1rSqM79+IxTuS+WZTEhsOnWHl3pOs3HuSAC9XBrYI4+424TQO9bd3yVJBlJtwY7FYGDduHJ06daJJkyaltt24uDgmTpxYatsTEZHS5+3uwl2tw7mrdTiHT2Xx7W9H+Pa3IySn5zBr7WFmrT1M41A/BreJYECLUAK83OxdspRj5WZYatSoUSxevJjVq1cTHh5eonVKMix1qZ6biIiIK3Zrmc1m8vPzS1y/SEm4urri7KwudpGSMFsMftl3krm/HWHpzhPkmQvPnePm7MStjYMZ3CaCv9WphrMu4FkpVLhhqTFjxvDDDz+watWqEgebknJ3d8fdveRnxzQMg+TkZFJTU0u1DpEiAQEB1KhRQ6emF7kCZycT3eoH0a1+EGez8liw5ShzfzvCzmPpLNx2nIXbjhPi78GgVuHc0SqMaJ07R86za7gxDIOxY8cyb948Vq5cSVRUlD3LAbAGm6CgILy8vPQFJKXGMAyys7NJSUkBICQkxM4ViVQcgd5uxHSKIqZTFDuOpvHtb0eYv+Uox9NymL5iP9NX7KdpmD8DWoRye/NQgvw87F2y2JFdw01sbCyzZ89mwYIF+Pr6kpycDIC/vz+enp4ADBs2jLCwMOLi4gDIy8tj165d1vtHjx5ly5Yt+Pj4UKdOneuqx2w2W4NN1apVr2tbIpdS9LlOSUkhKChIQ1Qi16BJmD9Nwvx5rm8Dlu1K4dvfkli17xTbj6ax/Wgary3aTcfoagxoEUrvJjXw9XC1d8lSxuw65+ZyvSIzZ84kJiYGgG7duhEZGcmsWbMAOHz48CV7eLp27crKlSuvuE9bY3Y5OTkcOnSIyMhI65eQSGk7d+6c9XPs4aG/LkVKw+nMXBZtP878Lcf4LeGstd3NxYkeDYMY0CKMbvWr67DyCkyXX7ChJOFGXzpyI+lzJnJjJZ3JZsGWo8zfcoz9KZnWdj8PF/o1C+H25mG0j6qCkyYiVygV5vILUr5FRkZe1eUtVq5ciclk0mRsEbGriCpejLm5Lkuf6MLCx/7GI11qU8PPg/ScAr7akMSQD9fT6fWfiVu0m13H0nXZBweknpsLVNS/qK806fmll15iwoQJV73dkydP4u3tjZeXV4mWz8vL48yZMwQHB9/QidgrV66ke/funD17loCAgBu2nxulon7ORCoys8Xg10OnWfD7MRbtOE5GToH1ubpBPgxsGcbtzUOJqFKy/++k7FW4Q8Hl+hw/ftx6/+uvv2b8+PHFLkDq4/Pn4ZGGYWA2m3FxufJbX7169auqw83NTdf7EpFyydnJRMfoanSMrsakgY1ZseckC7YcZfmeFPalZDJ5yV4mL9lLm1qBDGgZRr+mIVTx1okCKyoNSzmAGjVqWG/+/v6YTCbr4z179uDr68vixYtp3bo17u7urF69mgMHDjBgwACCg4Px8fGhbdu2LFu2rNh2/zosZTKZ+Oijj7jjjjvw8vKibt26fP/999bn/zosNWvWLAICAliyZAkNGzbEx8eH3r17FwtjBQUFPPbYYwQEBFC1alWeeeYZhg8fzsCBA6/593H27FmGDRtGYGAgXl5e9OnTh3379lmfT0hIoH///gQGBuLt7U3jxo1ZtGiRdd2hQ4dSvXp1PD09qVu3LjNnzrzmWkSk/HF3caZ3kxrMuL81G5/vwRuDmtExuiqm81crf3H+Dtq9uowHZ21kwZajZOcVXHmjUq6o5+YKDMPgXL7ZLvv2dHUuteGdZ599lilTplC7dm0CAwNJSkqib9++vPrqq7i7u/PZZ5/Rv39/9u7dS82aNS+7nYkTJ/LGG28wefJkpk2bxtChQ0lISLjs9cCys7OZMmUKn3/+OU5OTtx///089dRTfPnllwC8/vrrfPnll8ycOZOGDRvyn//8h/nz59O9e/drfq0xMTHs27eP77//Hj8/P5555hn69u3Lrl27cHV1JTY2lry8PFatWoW3tze7du2y9m69+OKL7Nq1i8WLF1OtWjX279/PuXPnrrkWESnf/D1dGdw2gsFtIziRnsP/th5j/paj7Diazs97Uvh5Twpebs70alyD21uE0rlONVyc1S9Q3incXMG5fDONxi+xy753TeqFl1vpvEWTJk3i1ltvtT6uUqUKzZs3tz5++eWXmTdvHt9//z1jxoy57HZiYmIYMmQIAK+99hrvvPMOGzZsoHfv3pdcPj8/n/fee4/o6Gig8GzUkyZNsj4/bdo0nnvuOe644w4Apk+fbu1FuRZFoWbNmjV07NgRgC+//JKIiAjmz5/P3XffTWJiIoMGDaJp06YA1K5d27p+YmIiLVu2pE2bNkBh75WIVA7Bfh6M7FybkZ1rsz8lk+/PH3GVeCabeb8fZd7vR6nq7cZtzUIY0DKMlhEBOtFrOaVwU0kUfVkXyczMZMKECSxcuJDjx49TUFDAuXPnSExMtLmdZs2aWe97e3vj5+dnPePupXh5eVmDDRSelbdo+bS0NE6cOEG7du2szzs7O9O6dWssFstVvb4iu3fvxsXFhfbt21vbqlatSv369dm9ezcAjz32GKNGjeKnn36iR48eDBo0yPq6Ro0axaBBg9i8eTM9e/Zk4MCB1pAkIpVHnSAfnuxZnydurcfvSaks+P0oP2w7zumsPD5dl8Cn6xKoVdWLAc1Dub1FGHWCdOmH8kTh5go8XZ3ZNamX3fZdWry9vYs9fuqpp1i6dClTpkyhTp06eHp6ctddd5GXl2dzO66uxc/0aTKZbAaRSy1v7wP0Ro4cSa9evVi4cCE//fQTcXFxvPnmm4wdO5Y+ffqQkJDAokWLWLp0KbfccguxsbFMmTLFrjWLiH2YTCZa1QykVc1AXritEav3n+L7LcdYsjOZhNPZvPPzft75eT9NwvwY2CKM/s1DCdalH+xO4eYKTCZTqQ0NlSdr1qwhJibGOhyUmZnJ4cOHy7QGf39/goOD2bhxI126dAEKL4GxefNmWrRocU3bbNiwIQUFBfz666/WHpfTp0+zd+9eGjVqZF0uIiKCRx99lEcffZTnnnuODz/8kLFjxwKFR4kNHz6c4cOH07lzZ55++mmFGxHB1dmJ7vWD6F4/iOy8ApbuOsGCLcdY9cdJdhxNZ8fRdF5dtJuO0VUZ0DyM3k1r4KdLP9iF431rS4nUrVuX7777jv79+2MymXjxxReveSjoeowdO5a4uDjq1KlDgwYNmDZtGmfPni3ROPb27dvx9fW1PjaZTDRv3pwBAwbw8MMP8/777+Pr68uzzz5LWFgYAwYMAGDcuHH06dOHevXqcfbsWVasWEHDhg0BGD9+PK1bt6Zx48bk5ubyww8/WJ8TESni5ebCgBZhDGgRxpmsPBZuP86C34+yKeEsa/afZs3+07ywYAe3NCi89EP3Brr0Q1lSuKmk3nrrLR588EE6duxItWrVeOaZZ0hPTy/zOp555hmSk5MZNmwYzs7OPPLII/Tq1atEF5Qs6u0p4uzsTEFBATNnzuTxxx/ntttuIy8vjy5durBo0SLrEJnZbCY2NpYjR47g5+dH7969efvtt4HCc/U899xzHD58GE9PTzp37sycOXNK/4WLiMOo4u3GAzfV4oGbapF0Jpvvtx5j/u9H2ZeSyeIdySzekYyvhwt9m4QwoGUoN0VV1aUfbjCdofgCOnOs/VksFho2bMjgwYN5+eWX7V3ODaHPmYjjMwyD3cczWLDlKN9vPcbxtBzrczX8PLi9RSgDWoTSKMRPR1yVkM5QLBVGQkICP/30E127diU3N5fp06dz6NAh7rvvPnuXJiJyzUwmE41C/WgU6sczvRvw66EzfL/1KAu3HSc5PYcPVh3kg1UHqRPkw8AWoQxoEaZLP5Qi9dxcQH9Rl72kpCTuvfdeduzYgWEYNGnShH//+98XDTk5En3ORCqv3AIzK/cWXvph2e4U8gr+nOvYulYgtzULoU+TEGr46/+Gv7qanhuFmwvoS0fKgj5nIgKQnpPPjzuS+X7LMdYeOIXlgm/j1rUC6ds0hD5NahAa4Gm/IssRDUuJiIiUc34ergxuE8HgNhGkpOfww7bjLNp+nE0JZ/nt/O3lH3bRsmYAfZuE0KdpDcIDNXRVEuq5uYD+opayoM+ZiNiSnJbDjzuOs2hHMhsPn+HCb+nm4f70bRpC36YhlW6OjnpuREREKqga/h7EdIoiplMUKek5LNmZzMLtx9lw6Axbj6Sx9UgacYv30DTMnz5Na9CvaQi1qnpfecOViMKNiIhIORXk58EDHSJ5oEMkJzNyWbIzmUXbj7P+4Gm2H01j+9E03vhxL41C/OjXrHCOTu3qus6Vwo2IiEgFUN3XnftvqsX9N9XidGYuS3aeYPGO46w9cJpdx9PZdTydyUv20qCGr3XoqrJe0FPhRkREpIKp6uPOfe1rcl/7mpzJymPprmQWbk9m7f5T7EnOYE9yBm8t/YN6wT7WoFMv2PfKG3YQTvYuQMqPbt26MW7cOOvjyMhIpk6danMdk8nE/Pnzr3vfpbUdEZHKpoq3G/e0rclnD7Zj0ws9eOOuZnSvXx1XZxN/nMhk6rJ99Hx7FT3eiuetn/ayJzkdRz+WSOHGAfTv35/evXtf8rlffvkFk8nEtm3brnq7Gzdu5JFHHrne8oqZMGHCJa/4ffz4cfr06VOq+/qrWbNmERAQcEP3ISJiTwFebgxuE8HMEe3Y9PytvHl3c25pEISbsxP7UzJ55+f99J76C7e8Gc+UJXvZeSzNIYOOhqUcwEMPPcSgQYM4cuQI4eHhxZ6bOXMmbdq0oVmzZle93erVq5dWiVdUo0aNMtuXiEhl4O/lyqDW4QxqHU56Tj7Ld59g4bZkVu07ycFTWUxfsZ/pK/YTWdWLPk1D6Nc0hMahjnGtK/XcOIDbbruN6tWrM2vWrGLtmZmZzJ07l4ceeojTp08zZMgQwsLC8PLyomnTpnz11Vc2t/vXYal9+/bRpUsXPDw8aNSoEUuXLr1onWeeeYZ69erh5eVF7dq1efHFF8nPzwcKe04mTpzI1q1bMZlMmEwma81/HZbavn07N998M56enlStWpVHHnmEzMxM6/MxMTEMHDiQKVOmEBISQtWqVYmNjbXu61okJiYyYMAAfHx88PPzY/DgwZw4ccL6/NatW+nevTu+vr74+fnRunVrNm3aBBReI6t///4EBgbi7e1N48aNWbRo0TXXIiJSmvw8XLmjZTgfDW/Dby/04D/3tqBX42DcXZw4fDqbGSsPcNu01XSdvJK4xbvZmpRaoXt01HNzJYYB+dn22berF5QgQbu4uDBs2DBmzZrF888/b03dc+fOxWw2M2TIEDIzM2ndujXPPPMMfn5+LFy4kAceeIDo6GjatWt3xX1YLBbuvPNOgoOD+fXXX0lLSys2P6eIr68vs2bNIjQ0lO3bt/Pwww/j6+vLP//5T+655x527NjBjz/+yLJlywDw9/e/aBtZWVn06tWLDh06sHHjRlJSUhg5ciRjxowpFuBWrFhBSEgIK1asYP/+/dxzzz20aNGChx9++Iqv51KvryjYxMfHU1BQQGxsLPfccw8rV64EYOjQobRs2ZIZM2bg7OzMli1bcHV1BSA2Npa8vDxWrVqFt7c3u3btwsench6lICLlm6+HKwNahDGgRRhZuQX8vCeFRduPs2JvColnsnk//iDvxx8kLMCTvk1r0LdpCC0iAipUj47CzZXkZ8NrofbZ97+OgVvJTsz04IMPMnnyZOLj4+nWrRtQOCQ1aNAg/P398ff356mnnrIuP3bsWJYsWcI333xTonCzbNky9uzZw5IlSwgNLfx9vPbaaxfNk3nhhRes9yMjI3nqqaeYM2cO//znP/H09MTHxwcXFxebw1CzZ88mJyeHzz77DG/vwtc/ffp0+vfvz+uvv05wcDAAgYGBTJ8+HWdnZxo0aEC/fv1Yvnz5NYWb5cuXs337dg4dOkRERAQAn332GY0bN2bjxo20bduWxMREnn76aRo0aABA3bp1resnJiYyaNAgmjZtCkDt2rWvugYRkbLm7e5C/+ah9G8eSnZeASv2nGTRjuP8vDuFo6nn+PCXQ3z4yyFC/T3o0zSEvk1r0DIiECen8h10FG4cRIMGDejYsSOffPIJ3bp1Y//+/fzyyy9MmjQJALPZzGuvvcY333zD0aNHycvLIzc3Fy+vkp2+e/fu3URERFiDDUCHDh0uWu7rr7/mnXfe4cCBA2RmZlJQUHDF02Rfal/Nmze3BhuATp06YbFY2Lt3rzXcNG7cGGdnZ+syISEhbN++/ar2deE+IyIirMEGoFGjRgQEBLB7927atm3Lk08+yciRI/n888/p0aMHd999N9HR0QA89thjjBo1ip9++okePXowaNCga5rnJCJiL15uLvRrFkK/ZiGcyzMT/0cKC7cn8/PuExxLy+Hj1Yf4ePUhavh50LtJDfo1C6F1zfIZdBRursTVq7AHxV77vgoPPfQQY8eO5f/+7/+YOXMm0dHRdO3aFYDJkyfzn//8h6lTp9K0aVO8vb0ZN24ceXl5pVbuunXrGDp0KBMnTqRXr174+/szZ84c3nzzzVLbx4WKhoSKmEwmLBbLDdkXFB7pdd9997Fw4UIWL17MSy+9xJw5c7jjjjsYOXIkvXr1YuHChfz000/ExcXx5ptvMnbs2BtWj4jIjeLp5kzvJiH0bhJCTr6Z+D9Osnj7cZbtTiE5PYdZaw8za+1hgnzd6dOkBn2ahtA2sgrO5SToKNxciclU4qEhexs8eDCPP/44s2fP5rPPPmPUqFHWMdI1a9YwYMAA7r//fqBwjskff/xBo0aNSrTthg0bkpSUxPHjxwkJCQFg/fr1xZZZu3YttWrV4vnnn7e2JSQkFFvGzc0Ns9l8xX3NmjWLrKwsa+/NmjVrcHJyon79+iWq92oVvb6kpCRr782uXbtITU0t9juqV68e9erV44knnmDIkCHMnDmTO+64A4CIiAgeffRRHn30UZ577jk+/PBDhRsRqfA8XJ3p1bgGvRrXICffzOp9p1i0/ThLd50gJSOXT9cl8Om6BKr5uNO7STB9m4bQLrIKLs72O2ZJ4caB+Pj4cM899/Dcc8+Rnp5OTEyM9bm6devy7bffsnbtWgIDA3nrrbc4ceJEicNNjx49qFevHsOHD2fy5Mmkp6cXCzFF+0hMTGTOnDm0bduWhQsXMm/evGLLREZGcujQIbZs2UJ4eDi+vr64u7sXW2bo0KG89NJLDB8+nAkTJnDy5EnGjh3LAw88YB2SulZms5ktW7YUa3N3d6dHjx40bdqUoUOHMnXqVAoKChg9ejRdu3alTZs2nDt3jqeffpq77rqLqKgojhw5wsaNGxk0aBAA48aNo0+fPtSrV4+zZ8+yYsUKGjZseF21ioiUNx6uzvRoFEyPRsHkFphZs/8Ui7Yn89POZE5l5vLF+kS+WJ9IVDVvfv5HV7tNQtah4A7moYce4uzZs/Tq1avY/JgXXniBVq1a0atXL7p160aNGjUYOHBgibfr5OTEvHnzOHfuHO3atWPkyJG8+uqrxZa5/fbbeeKJJxgzZgwtWrRg7dq1vPjii8WWGTRoEL1796Z79+5Ur179koeje3l5sWTJEs6cOUPbtm256667uOWWW5g+ffrV/TIuITMzk5YtWxa79e/fH5PJxIIFCwgMDKRLly706NGD2rVr8/XXXwPg7OzM6dOnGTZsGPXq1WPw4MH06dOHiRMnAoWhKTY2loYNG9K7d2/q1avHu+++e931ioiUV+4uztzcIJgpdzdn0wu3MmtEWwa3CSfAy9XuR1eZjIp8IPs1SE9Px9/fn7S0tIsmuubk5HDo0CGioqLw8PCwU4Xi6PQ5ExFHlm+2kJFTQBVvt1Ldrq3v779Sz42IiIiUGldnp1IPNldL4UZEREQcisKNiIiIOBSFGxEREXEoCjciIiLiUBRuLqGSHUAmZUyfLxGRG0vh5gJFp/PPzrbTVcClUij6fP318hEiIlI6dIbiCzg7OxMQEEBKSgpQeDK5inSJdynfDMMgOzublJQUAgICil30U0RESo/CzV/UqFEDwBpwREpbQECA9XMmIiKlT+HmL0wmEyEhIQQFBZGfn2/vcsTBuLq6qsdGROQGU7i5DGdnZ30JiYiIVECaUCwiIiIOReFGREREHIrCjYiIiDiUSjfnpugEaunp6XauREREREqq6Hu7JCdCrXThJiMjA4CIiAg7VyIiIiJXKyMjA39/f5vLmIxKdi54i8XCsWPH8PX1LfUT9KWnpxMREUFSUhJ+fn6lum25eno/yhe9H+WL3o/yR++JbYZhkJGRQWhoKE5OtmfVVLqeGycnJ8LDw2/oPvz8/PTBLEf0fpQvej/KF70f5Y/ek8u7Uo9NEU0oFhEREYeicCMiIiIOReGmFLm7u/PSSy/h7u5u71IEvR/ljd6P8kXvR/mj96T0VLoJxSIiIuLY1HMjIiIiDkXhRkRERByKwo2IiIg4FIUbERERcSgKN6Xk//7v/4iMjMTDw4P27duzYcMGe5dUacXFxdG2bVt8fX0JCgpi4MCB7N27195lyXn//ve/MZlMjBs3zt6lVFpHjx7l/vvvp2rVqnh6etK0aVM2bdpk77IqJbPZzIsvvkhUVBSenp5ER0fz8ssvl+j6SXJ5Cjel4Ouvv+bJJ5/kpZdeYvPmzTRv3pxevXqRkpJi79Iqpfj4eGJjY1m/fj1Lly4lPz+fnj17kpWVZe/SKr2NGzfy/vvv06xZM3uXUmmdPXuWTp064erqyuLFi9m1axdvvvkmgYGB9i6tUnr99deZMWMG06dPZ/fu3bz++uu88cYbTJs2zd6lVWg6FLwUtG/fnrZt2zJ9+nSg8PpVERERjB07lmeffdbO1cnJkycJCgoiPj6eLl262LucSiszM5NWrVrx7rvv8sorr9CiRQumTp1q77IqnWeffZY1a9bwyy+/2LsUAW677TaCg4P5+OOPrW2DBg3C09OTL774wo6VVWzqublOeXl5/Pbbb/To0cPa5uTkRI8ePVi3bp0dK5MiaWlpAFSpUsXOlVRusbGx9OvXr9i/FSl733//PW3atOHuu+8mKCiIli1b8uGHH9q7rEqrY8eOLF++nD/++AOArVu3snr1avr06WPnyiq2SnfhzNJ26tQpzGYzwcHBxdqDg4PZs2ePnaqSIhaLhXHjxtGpUyeaNGli73IqrTlz5rB582Y2btxo71IqvYMHDzJjxgyefPJJ/vWvf7Fx40Yee+wx3NzcGD58uL3Lq3SeffZZ0tPTadCgAc7OzpjNZl599VWGDh1q79IqNIUbcWixsbHs2LGD1atX27uUSispKYnHH3+cpUuX4uHhYe9yKj2LxUKbNm147bXXAGjZsiU7duzgvffeU7ixg2+++YYvv/yS2bNn07hxY7Zs2cK4ceMIDQ3V+3EdFG6uU7Vq1XB2dubEiRPF2k+cOEGNGjXsVJUAjBkzhh9++IFVq1YRHh5u73Iqrd9++42UlBRatWplbTObzaxatYrp06eTm5uLs7OzHSusXEJCQmjUqFGxtoYNG/Lf//7XThVVbk8//TTPPvss9957LwBNmzYlISGBuLg4hZvroDk318nNzY3WrVuzfPlya5vFYmH58uV06NDBjpVVXoZhMGbMGObNm8fPP/9MVFSUvUuq1G655Ra2b9/Oli1brLc2bdowdOhQtmzZomBTxjp16nTRqRH++OMPatWqZaeKKrfs7GycnIp/FTs7O2OxWOxUkWNQz00pePLJJxk+fDht2rShXbt2TJ06laysLEaMGGHv0iql2NhYZs+ezYIFC/D19SU5ORkAf39/PD097Vxd5ePr63vRfCdvb2+qVq2qeVB28MQTT9CxY0dee+01Bg8ezIYNG/jggw/44IMP7F1apdS/f39effVVatasSePGjfn999956623ePDBB+1dWoWmQ8FLyfTp05k8eTLJycm0aNGCd955h/bt29u7rErJZDJdsn3mzJnExMSUbTFySd26ddOh4Hb0ww8/8Nxzz7Fv3z6ioqJ48sknefjhh+1dVqWUkZHBiy++yLx580hJSSE0NJQhQ4Ywfvx43Nzc7F1ehaVwIyIiIg5Fc25ERETEoSjciIiIiENRuBERERGHonAjIiIiDkXhRkRERByKwo2IiIg4FIUbERERcSgKNyJSKZlMJubPn2/vMkTkBlC4EZEyFxMTg8lkuujWu3dve5cmIg5A15YSEbvo3bs3M2fOLNbm7u5up2pExJGo50ZE7MLd3Z0aNWoUuwUGBgKFQ0YzZsygT58+eHp6Urt2bb799tti62/fvp2bb74ZT09PqlatyiOPPEJmZmaxZT755BMaN26Mu7s7ISEhjBkzptjzp06d4o477sDLy4u6devy/fffW587e/YsQ4cOpXr16nh6elK3bt2LwpiIlE8KNyJSLr344osMGjSIrVu3MnToUO699152794NQFZWFr169SIwMJCNGzcyd+5cli1bViy8zJgxg9jYWB555BG2b9/O999/T506dYrtY+LEiQwePJht27bRt29fhg4dypkzZ6z737VrF4sXL2b37t3MmDGDatWqld0vQESunSEiUsaGDx9uODs7G97e3sVur776qmEYhgEYjz76aLF12rdvb4waNcowDMP44IMPjMDAQCMzM9P6/MKFCw0nJycjOTnZMAzDCA0NNZ5//vnL1gAYL7zwgvVxZmamARiLFy82DMMw+vfvb4wYMaJ0XrCIlCnNuRERu+jevTszZswo1lalShXr/Q4dOhR7rkOHDmzZsgWA3bt307x5c7y9va3Pd+rUCYvFwt69ezGZTBw7doxbbrnFZg3NmjWz3vf29sbPz4+UlBQARo0axaBBg9i8eTM9e/Zk4MCBdOzY8Zpeq4iULYUbEbELb2/vi4aJSounp2eJlnN1dS322GQyYbFYAOjTpw8JCQksWrSIpUuXcssttxAbG8uUKVNKvV4RKV2acyMi5dL69esvetywYUMAGjZsyNatW8nKyrI+v2bNGpycnKhfvz6+vr5ERkayfPny66qhevXqDB8+nC+++IKpU6fywQcfXNf2RKRsqOdGROwiNzeX5OTkYm0uLi7WSbtz586lTZs2/O1vf+PLL79kw4YNfPzxxwAMHTqUl156ieHDhzNhwgROnjzJ2LFjeeCBBwgODgZgwoQJPProowQFBdGnTx8yMjJYs2YNY8eOLVF948ePp3Xr1jRu3Jjc3Fx++OEHa7gSkfJN4UZE7OLHH38kJCSkWFv9+vXZs2cPUHgk05w5cxg9ejQhISF89dVXNGrUCAAvLy+WLFnC448/Ttu2bfHy8mLQoEG89dZb1m0NHz6cnJwc3n77bZ566imqVavGXXfdVeL63NzceO655zh8+DCenp507tyZOXPmlMIrF5EbzWQYhmHvIkRELmQymZg3bx4DBw60dykiUgFpzo2IiIg4FIUbERERcSiacyMi5Y5Gy0XkeqjnRkRERByKwo2IiIg4FIUbERERcSgKNyIiIuJQFG5ERETEoSjciIiIiENRuBERERGHonAjIiIiDkXhRkRERBzK/wMK0VHm15q+MwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training history\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c39826-1839-4bf9-9f32-9fc66393e49c",
   "metadata": {},
   "source": [
    "### Generating Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "69ba8439-f79d-4b17-b107-f96d61e72dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoder model for inference\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Define decoder model for inference\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_inference_inputs = Input(shape=(None,))\n",
    "decoder_embedding_inference = decoder_embedding(decoder_inference_inputs)\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding_inference, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inference_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# Save token index mappings\n",
    "target_token_index = tokenizer.word_index\n",
    "reverse_target_token_index = {v: k for k, v in target_token_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "171a9727-3484-4968-a1e8-b26e3dd695d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate responses\n",
    "def generate_response(input_seq: np.ndarray, max_decoder_seq_length: int) -> str:\n",
    "    # Encode the input sequence to get the internal states\n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "\n",
    "    # Generate empty target sequence of length 1 with only the start token\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = tokenizer.word_index['<START>']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
    "\n",
    "        # Sample a token and add the corresponding character to the decoded sentence\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_token_index[sampled_token_index]\n",
    "        decoded_sentence += ' ' + sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length or find stop token\n",
    "        if (sampled_char == '<END>' or len(decoded_sentence.split()) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence.strip().replace('<START>', '').replace('<END>', '').strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0f56fc-ba3e-40fd-b965-ba93098dda02",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "02b68f62-8d82-4ea3-9061-cb798fcef3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed text: ['how are you doing today', 'what is your name', 'can you help me with my homework', 'what is the weather like', 'tell me a joke', 'who is the president of the united states', 'what is the capital of france', 'do you like pizza', 'what is your favorite color', 'goodbye']\n",
      "Tokenizer sequences: [[33, 17, 5, 208, 130], [18, 9, 24, 160], [21, 5, 101, 26, 39, 25, 1246], [18, 9, 6, 415, 30], [110, 26, 8, 2737], [152, 9, 6, 1324, 16, 6, 1130, 1020], [18, 9, 6, 2885, 16, 1721], [13, 5, 30, 1167], [18, 9, 24, 489, 511], [725]]\n",
      "Padded sequences: [[   0    0    0    0    0    0    0    0    0    0   33   17    5  208\n",
      "   130]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0   18    9   24\n",
      "   160]\n",
      " [   0    0    0    0    0    0    0    0   21    5  101   26   39   25\n",
      "  1246]\n",
      " [   0    0    0    0    0    0    0    0    0    0   18    9    6  415\n",
      "    30]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  110   26    8\n",
      "  2737]\n",
      " [   0    0    0    0    0    0    0  152    9    6 1324   16    6 1130\n",
      "  1020]\n",
      " [   0    0    0    0    0    0    0    0    0   18    9    6 2885   16\n",
      "  1721]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0   13    5   30\n",
      "  1167]\n",
      " [   0    0    0    0    0    0    0    0    0    0   18    9   24  489\n",
      "   511]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   725]]\n",
      "Input: How are you doing today?\n",
      "Response: i am fine\n",
      "--------------------------------------------------\n",
      "Input: What is your name?\n",
      "Response: my name is\n",
      "--------------------------------------------------\n",
      "Input: Can you help me with my homework?\n",
      "Response: sure what is the matter\n",
      "--------------------------------------------------\n",
      "Input: What is the weather like?\n",
      "Response: it is a piano solo i think it was a little dizzy\n",
      "--------------------------------------------------\n",
      "Input: Tell me a joke.\n",
      "Response: that 's a good idea\n",
      "--------------------------------------------------\n",
      "Input: Who is the president of the United States?\n",
      "Response: i like the thom birds best\n",
      "--------------------------------------------------\n",
      "Input: What is the capital of France?\n",
      "Response: they are reports of them\n",
      "--------------------------------------------------\n",
      "Input: Do you like pizza?\n",
      "Response: yes i do not know what i am going to do\n",
      "--------------------------------------------------\n",
      "Input: What is your favorite color?\n",
      "Response: i like peach blossom in my opinion\n",
      "--------------------------------------------------\n",
      "Input: Goodbye!\n",
      "Response: goodbye\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "initial_preprocessing = False  # Expects spaCy to detect and remove names from the text\n",
    "max_length = 15\n",
    "\n",
    "# Define ten examples to test the model\n",
    "test_examples = [\n",
    "    \"How are you doing today?\",\n",
    "    \"What is your name?\",\n",
    "    \"Can you help me with my homework?\",\n",
    "    \"What is the weather like?\",\n",
    "    \"Tell me a joke.\",\n",
    "    \"Who is the president of the United States?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Do you like pizza?\",\n",
    "    \"What is your favorite color?\",\n",
    "    \"Goodbye!\"\n",
    "]\n",
    "\n",
    "# Preprocess input text (assuming preprocess_text is defined elsewhere)\n",
    "input_text = [preprocess_text(text) for text in test_examples]\n",
    "print(f\"Preprocessed text: {input_text}\")\n",
    "\n",
    "# Tokenize and pad the test examples\n",
    "test_sequences = tokenizer.texts_to_sequences(input_text)\n",
    "print(f\"Tokenizer sequences: {test_sequences}\")\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=max_length, padding='pre', truncating='post')\n",
    "print(f\"Padded sequences: {padded_test_sequences}\")\n",
    "\n",
    "# Generate responses\n",
    "for test_seq in padded_test_sequences:\n",
    "    input_seq = np.array([test_seq])\n",
    "    response = generate_response(input_seq, max_length)\n",
    "    print(f\"Input: {test_examples[padded_test_sequences.tolist().index(test_seq.tolist())]}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf3485-ef0e-4ced-a681-daaa0eca5012",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "179e2505-888d-4941-b041-3ae9dc061fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'data_dd')\n",
    "file_path_h5 = os.path.join(data_dir, 's2s_model_dd.h5')\n",
    "model.save(file_path_h5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2069f0f9-6f2f-48b7-bc83-803df9ec8f3f",
   "metadata": {},
   "source": [
    "### Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "822f5419-ee91-466f-988a-16f5731eab30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: How are you doing today?\n",
      "Response: i am fine\n",
      "--------------------------------------------------\n",
      "Input: What is your name?\n",
      "Response: my name is\n",
      "--------------------------------------------------\n",
      "Input: Can you help me with my homework?\n",
      "Response: sure what is the matter\n",
      "--------------------------------------------------\n",
      "Input: What is the weather like?\n",
      "Response: it is a piano solo\n",
      "--------------------------------------------------\n",
      "Input: Tell me a joke.\n",
      "Response: that 's too bad\n",
      "--------------------------------------------------\n",
      "Input: Who is the president of the United States?\n",
      "Response: i like the thom birds best\n",
      "--------------------------------------------------\n",
      "Input: What is the capital of France?\n",
      "Response: they are reports of them\n",
      "--------------------------------------------------\n",
      "Input: Do you like pizza?\n",
      "Response: yes i do not know how to use it\n",
      "--------------------------------------------------\n",
      "Input: What is your favorite color?\n",
      "Response: i like it\n",
      "--------------------------------------------------\n",
      "Input: Goodbye!\n",
      "Response: goodbye\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def beam_search_decode(input_seq, beam_width=3, max_decoder_seq_length=15):\n",
    "    # Encode the input sequence to get the internal states\n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "\n",
    "    # Initialize the beams\n",
    "    start_token_index = tokenizer.word_index['<START>']\n",
    "    end_token_index = tokenizer.word_index['<END>']\n",
    "    beams = [(np.array([[start_token_index]]), states_value, 0.0)]  # (sequence, states, cumulative_probability)\n",
    "\n",
    "    for _ in range(max_decoder_seq_length):\n",
    "        all_candidates = []\n",
    "        for seq, states, score in beams:\n",
    "            if seq[0, -1] == end_token_index:\n",
    "                # If the beam already ended with the end token, add it to the candidates\n",
    "                all_candidates.append((seq, states, score))\n",
    "                continue\n",
    "            \n",
    "            # Predict the next token\n",
    "            output_tokens, h, c = decoder_model.predict([seq[:, -1:]] + states, verbose=0)\n",
    "            # Get the top beam_width predictions\n",
    "            top_k_indices = np.argsort(output_tokens[0, -1, :])[-beam_width:]\n",
    "            \n",
    "            # Create new beams for each prediction\n",
    "            for idx in top_k_indices:\n",
    "                new_seq = np.hstack([seq, np.array([[idx]])])\n",
    "                new_score = score + np.log(output_tokens[0, -1, idx])  # Use log to prevent underflow\n",
    "                all_candidates.append((new_seq, [h, c], new_score))\n",
    "        \n",
    "        # Select the top beam_width beams\n",
    "        beams = sorted(all_candidates, key=lambda x: x[2], reverse=True)[:beam_width]\n",
    "\n",
    "        # Check if all beams end with the end token\n",
    "        if all(seq[0, -1] == end_token_index for seq, _, _ in beams):\n",
    "            break\n",
    "\n",
    "    # Choose the best beam (highest score)\n",
    "    best_seq, _, _ = beams[0]\n",
    "    decoded_sentence = ' '.join([reverse_target_token_index[idx] for idx in best_seq[0] if idx != start_token_index and idx != end_token_index])\n",
    "    return decoded_sentence\n",
    "\n",
    "# Generate responses with beam search\n",
    "for test_seq in padded_test_sequences:\n",
    "    input_seq = np.array([test_seq])\n",
    "    response = beam_search_decode(input_seq, beam_width=3, max_decoder_seq_length=max_length)\n",
    "    print(f\"Input: {test_examples[padded_test_sequences.tolist().index(test_seq.tolist())]}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
