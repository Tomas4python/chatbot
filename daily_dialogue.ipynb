{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90199c4d-e447-46b3-9c7a-e633a3c02f09",
   "metadata": {},
   "source": [
    "# TESTING SIMPLE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14260f0-4962-42e2-8527-f5189a158c76",
   "metadata": {
    "id": "b0ZiOK0MWE7d"
   },
   "source": [
    "## GPU info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63f70056-2151-4051-8974-714b3e237e43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1715927490489,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "bhvFI7ztVLRy",
    "outputId": "e4fb02b2-7ebf-4f3e-b2a8-f9f189a25c47"
   },
   "outputs": [],
   "source": [
    "# gpu_info = !nvidia-smi\n",
    "# gpu_info = '\\n'.join(gpu_info)\n",
    "# if gpu_info.find('failed') >= 0:\n",
    "#   print('Not connected to a GPU')\n",
    "# else:\n",
    "#   print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a58ec6-07dc-4261-ae09-67ab59ed3fb6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23496,
     "status": "ok",
     "timestamp": 1715927517410,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "RMth1TfbAys6",
    "outputId": "766cb4f5-edfa-4cd7-ad09-825604e4c786"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d5d74b-5e87-4198-b112-9f91b7f65d0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4075,
     "status": "ok",
     "timestamp": 1715927650106,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "db509470-cb51-4c21-84ca-5f928073d68e",
    "outputId": "95b90920-4333-45db-f52c-0f88f1ec6a5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Name: /device:GPU:0\n",
      "Memory Limit: 4158652416 bytes\n",
      "Description: device: 0, name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_gpu_details():\n",
    "    devices = device_lib.list_local_devices()\n",
    "    for device in devices:\n",
    "        if device.device_type == 'GPU':\n",
    "            print(f\"Device Name: {device.name}\")\n",
    "            print(f\"Memory Limit: {device.memory_limit} bytes\")\n",
    "            print(f\"Description: {device.physical_device_desc}\")\n",
    "\n",
    "get_gpu_details()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72de3458-7b40-491d-a66f-d8355a455243",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1715927650106,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "6648d492-f4e0-40a8-a3ff-012fa0a0d293"
   },
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfe7f8ff-d35e-416c-8252-fc19fc3570e2",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1715927650106,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "8a0af574-6748-467b-8ba6-4da51794c5b9"
   },
   "outputs": [],
   "source": [
    "# !pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50d83013-6a91-4f74-bee7-da28b5c4f085",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1715927650106,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "7e4e2a98-f1d4-4f72-b95e-6a75f457e8e8"
   },
   "outputs": [],
   "source": [
    "# !pip install fastparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0510248e-c233-485a-b707-7a7eb9856939",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a59de0-817a-4089-9a14-0b45925b6298",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 15 # Length of input and target sequences, padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c8162-54ab-45da-b4f0-2d5472decdd8",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e231807-af60-4571-be49-655d80d8eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, Concatenate, Layer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e56f5d37-9a67-4e51-b0b8-404df9d734a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1715930801381,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "y5srV6bjOpFb",
    "outputId": "b74db1e8-1afe-4348-be1a-23c150492567"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\tomui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')  # Tokenizer\n",
    "nltk.download('wordnet')  # Lemmatizer\n",
    "nltk.download('stopwords')  # Stopwords\n",
    "nltk.download('omw-1.4') # Ensures multilingual contexts\n",
    "\n",
    "# Stopwords list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "initial_preprocessing = True\n",
    "\n",
    "# Load spaCy's English NLP model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b393a656-df75-4deb-af28-d8ba36f526eb",
   "metadata": {
    "id": "f5150d2f-999d-45ad-b2f2-b9b65b883e89"
   },
   "source": [
    "## Create prepocessing functions for initial text and later response generation preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75edfc9e-3560-4ab7-9992-963b8df9f537",
   "metadata": {
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1715930818326,
     "user": {
      "displayName": "Tomas Suslavicius",
      "userId": "14151871739646404131"
     },
     "user_tz": -180
    },
    "id": "f0e8f237-5274-43ee-8ac2-964dd9214ae5"
   },
   "outputs": [],
   "source": [
    "contractions = {\n",
    "    \"’\": \"'\",\n",
    "    \"‘\": \"'\",\n",
    "    \"“\": '\"',\n",
    "    \"”\": '\"',\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"n't\": \" not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"thats's\": \"that is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"that'll\": \"that will\",\n",
    "    \"there'll\": \"there will\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"ain't\": \"is not\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"daren't\": \"dare not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"usedn't\": \"used not\"\n",
    "}\n",
    "\n",
    "max_length_sentences = max_length - 2\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    # Normalize Unicode string to NFKD form, remove non-ASCII characters, and then decode it back to a UTF-8 string\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove spaces around apostrophes\n",
    "    text = re.sub(r\"\\s*'\\s*\", \"'\", text)\n",
    "    # Add a space before and after any punctuation mark (., !, or ?)\n",
    "    text = re.sub(r\"\\s*([.!?])\\s*\", r\" \\1 \", text)\n",
    "    # Correct contractions\n",
    "    for contraction, replacement in contractions.items():\n",
    "        text = re.sub(re.escape(contraction), replacement, text)\n",
    "    # Replace any sequence of characters that are not letters, basic punctuation\n",
    "    text = re.sub(r\"[^a-z.,'!? ]\", ' ', text)\n",
    "    # Replace any sequence of whitespace characters with a single space and remove leading and trailing whitespace\n",
    "    text = re.sub(r\"\\s+\", ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_names(text: str) -> str:\n",
    "    # Use spaCy to detect and remove names from the text\n",
    "    doc = nlp(text)\n",
    "    filtered_text = ' '.join([token.text for token in doc if token.ent_type_ != 'PERSON']) # Takes really long time, exlude from chatbot input preprocessing\n",
    "    return filtered_text\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # Normalize text\n",
    "    text = normalize_text(text)\n",
    "    # Remove names using spaCy's NER\n",
    "    if initial_preprocessing:\n",
    "        text = remove_names(text)\n",
    "    # # Remove punctuation\n",
    "    # text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stopwords and tokenize\n",
    "    # words = word_tokenize(text) # More intelligent splitting\n",
    "    # filtered_words = [word for word in words if word not in stop_words]\n",
    "    # # Lemmatize words\n",
    "    # lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    # Add <SOS> and <EOS> tokens, and join the list into a single string\n",
    "    # return ' '.join(['sofs'] + lemmatized_words + ['eofs'])\n",
    "        # Trim the text to the desired length\n",
    "    words = text.split()[:max_length_sentences]\n",
    "    trimmed_text = ' '.join(words)\n",
    "    return 'sofs ' + trimmed_text + ' eofs' # Chosen ['sofs', 'eofs'] because tokenizer removes everthing what is in <> or || and are not in dataset vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13a57e-6fc6-48ad-8326-becd2e579083",
   "metadata": {},
   "source": [
    "### Load data\n",
    "https://huggingface.co/datasets/daily_dialog/tree/refs%2Fconvert%2Fparquet/default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17b8c9fb-fe6c-4bca-a255-45e7c9a9a831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of data_dd/0000.parquet:\n",
      "                                              dialog  \\\n",
      "0  [Say , Jim , how about going for a few beers a...   \n",
      "1  [Can you do push-ups ? ,  Of course I can . It...   \n",
      "2  [Can you study with the radio on ? ,  No , I l...   \n",
      "3  [Are you all right ? ,  I will be all right so...   \n",
      "4  [Hey John , nice skates . Are they new ? ,  Ye...   \n",
      "\n",
      "                              act                         emotion  \n",
      "0  [3, 4, 2, 2, 2, 3, 4, 1, 3, 4]  [0, 0, 0, 0, 0, 0, 4, 4, 4, 4]  \n",
      "1              [2, 1, 2, 2, 1, 1]              [0, 0, 6, 0, 0, 0]  \n",
      "2                 [2, 1, 2, 1, 1]                 [0, 0, 0, 0, 0]  \n",
      "3                    [2, 1, 1, 1]                    [0, 0, 0, 0]  \n",
      "4     [2, 1, 2, 1, 1, 2, 1, 3, 4]     [0, 0, 0, 0, 0, 6, 0, 6, 0]   \n",
      "\n",
      "Contents of data_dd/default_validation_0000.parquet:\n",
      "                                              dialog  \\\n",
      "0  [Good morning , sir . Is there a bank near her...   \n",
      "1  [Good afternoon . This is Michelle Li speaking...   \n",
      "2  [What qualifications should a reporter have ? ...   \n",
      "3  [Hi , good morning , Miss ? what can I help yo...   \n",
      "4  [Excuse me , ma'am . Can you tell me where the...   \n",
      "\n",
      "                                                act  \\\n",
      "0                             [2, 1, 3, 2, 1, 2, 1]   \n",
      "1                    [2, 1, 1, 1, 1, 2, 3, 2, 3, 4]   \n",
      "2                                      [2, 1, 2, 1]   \n",
      "3  [2, 3, 2, 2, 1, 2, 1, 2, 1, 1, 1, 3, 2, 1, 3, 4]   \n",
      "4                          [3, 4, 2, 1, 2, 1, 1, 1]   \n",
      "\n",
      "                                            emotion  \n",
      "0                             [0, 0, 0, 0, 0, 0, 0]  \n",
      "1                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "2                                      [0, 0, 0, 0]  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]  \n",
      "4                          [0, 0, 0, 0, 0, 0, 4, 4]   \n",
      "\n",
      "Contents of data_dd/default_test_0000.parquet:\n",
      "                                              dialog  \\\n",
      "0  [Hey man , you wanna buy some weed ? ,  Some w...   \n",
      "1  [The taxi drivers are on strike again . ,  Wha...   \n",
      "2  [We've managed to reduce our energy consumptio...   \n",
      "3  [Believe it or not , tea is the most popular b...   \n",
      "4  [What are your personal weaknesses ? ,  I ’ m ...   \n",
      "\n",
      "                                          act  \\\n",
      "0        [3, 2, 3, 4, 3, 4, 3, 2, 3, 4, 2, 3]   \n",
      "1                                [1, 2, 1, 1]   \n",
      "2                       [1, 2, 1, 2, 1, 2, 1]   \n",
      "3  [1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 3, 4, 3]   \n",
      "4                    [2, 1, 2, 1, 2, 1, 2, 1]   \n",
      "\n",
      "                                      emotion  \n",
      "0        [0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0]  \n",
      "1                                [0, 0, 0, 0]  \n",
      "2                       [0, 0, 0, 0, 0, 0, 0]  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 4]  \n",
      "4                    [0, 0, 0, 0, 0, 0, 0, 4]   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_parquet_files(file_paths: Dict[str, str]) -> Dict[str, pd.DataFrame]:\n",
    "    dataframes = {}\n",
    "    for key, file_path in file_paths.items():\n",
    "        try:\n",
    "            df = pd.read_parquet(file_path)\n",
    "            dataframes[key] = df\n",
    "            print(f\"Contents of {file_path}:\")\n",
    "            print(df.head(), \"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while loading {file_path}: {e}\")\n",
    "    return dataframes\n",
    "\n",
    "file_paths = {\n",
    "    'train': 'data_dd/0000.parquet',\n",
    "    'validation': 'data_dd/default_validation_0000.parquet',\n",
    "    'test': 'data_dd/default_test_0000.parquet'\n",
    "}\n",
    "\n",
    "dataframes = load_parquet_files(file_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631dd47f-5120-435e-b7ae-27fde4ee31df",
   "metadata": {},
   "source": [
    "### Review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d739cdf-8425-416e-a55a-3e502c4b70d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation', 'test'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c79dba0-1db0-4f10-a3e2-65098b1c9d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11118 1000 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialog</th>\n",
       "      <th>act</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Say , Jim , how about going for a few beers a...</td>\n",
       "      <td>[3, 4, 2, 2, 2, 3, 4, 1, 3, 4]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 4, 4, 4, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Can you do push-ups ? ,  Of course I can . It...</td>\n",
       "      <td>[2, 1, 2, 2, 1, 1]</td>\n",
       "      <td>[0, 0, 6, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Can you study with the radio on ? ,  No , I l...</td>\n",
       "      <td>[2, 1, 2, 1, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Are you all right ? ,  I will be all right so...</td>\n",
       "      <td>[2, 1, 1, 1]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Hey John , nice skates . Are they new ? ,  Ye...</td>\n",
       "      <td>[2, 1, 2, 1, 1, 2, 1, 3, 4]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 6, 0, 6, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11113</th>\n",
       "      <td>[Hello , I bought a pen in your shop just befo...</td>\n",
       "      <td>[1, 1, 1, 2, 3, 2, 1, 4, 1]</td>\n",
       "      <td>[0, 4, 0, 0, 0, 0, 0, 0, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11114</th>\n",
       "      <td>[Do you have any seats available ? ,  Yes . Th...</td>\n",
       "      <td>[2, 1, 2, 1, 3, 4]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11115</th>\n",
       "      <td>[Uncle Ben , how did the Forbidden City get th...</td>\n",
       "      <td>[2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 3, 4]</td>\n",
       "      <td>[0, 0, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11116</th>\n",
       "      <td>[May I help you , sir ? ,  I want a pair of lo...</td>\n",
       "      <td>[2, 3, 4, 3]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11117</th>\n",
       "      <td>[Could I have the check , please ? ,  Okay . I...</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>[0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11118 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  dialog  \\\n",
       "0      [Say , Jim , how about going for a few beers a...   \n",
       "1      [Can you do push-ups ? ,  Of course I can . It...   \n",
       "2      [Can you study with the radio on ? ,  No , I l...   \n",
       "3      [Are you all right ? ,  I will be all right so...   \n",
       "4      [Hey John , nice skates . Are they new ? ,  Ye...   \n",
       "...                                                  ...   \n",
       "11113  [Hello , I bought a pen in your shop just befo...   \n",
       "11114  [Do you have any seats available ? ,  Yes . Th...   \n",
       "11115  [Uncle Ben , how did the Forbidden City get th...   \n",
       "11116  [May I help you , sir ? ,  I want a pair of lo...   \n",
       "11117  [Could I have the check , please ? ,  Okay . I...   \n",
       "\n",
       "                                                    act  \\\n",
       "0                        [3, 4, 2, 2, 2, 3, 4, 1, 3, 4]   \n",
       "1                                    [2, 1, 2, 2, 1, 1]   \n",
       "2                                       [2, 1, 2, 1, 1]   \n",
       "3                                          [2, 1, 1, 1]   \n",
       "4                           [2, 1, 2, 1, 1, 2, 1, 3, 4]   \n",
       "...                                                 ...   \n",
       "11113                       [1, 1, 1, 2, 3, 2, 1, 4, 1]   \n",
       "11114                                [2, 1, 2, 1, 3, 4]   \n",
       "11115  [2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 3, 4]   \n",
       "11116                                      [2, 3, 4, 3]   \n",
       "11117                                            [3, 4]   \n",
       "\n",
       "                                                emotion  \n",
       "0                        [0, 0, 0, 0, 0, 0, 4, 4, 4, 4]  \n",
       "1                                    [0, 0, 6, 0, 0, 0]  \n",
       "2                                       [0, 0, 0, 0, 0]  \n",
       "3                                          [0, 0, 0, 0]  \n",
       "4                           [0, 0, 0, 0, 0, 6, 0, 6, 0]  \n",
       "...                                                 ...  \n",
       "11113                       [0, 4, 0, 0, 0, 0, 0, 0, 4]  \n",
       "11114                                [0, 0, 0, 0, 0, 4]  \n",
       "11115  [0, 0, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0]  \n",
       "11116                                      [0, 0, 0, 0]  \n",
       "11117                                            [0, 0]  \n",
       "\n",
       "[11118 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = dataframes['train']\n",
    "val_df = dataframes['validation']\n",
    "test_df = dataframes['test']\n",
    "print(len(train_df),len(val_df), len(test_df))\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4defcd4b-38db-4f28-8cbc-3a27bd2db229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Can you tell me where the pots and pans are ? '\n",
      " ' Pots and pans are right over there . ' ' Oh , thank you . '\n",
      " ' Could I interest you in our store credit card ? '\n",
      " ' No , thanks . I already have credit cards . '\n",
      " ' But our credit card saves you 10 percent . '\n",
      " \" That's a nice discount . \"\n",
      " ' Here . Let me give you an application form . '\n",
      " \" Thank you , but I'm just browsing today . \"\n",
      " ' Okay . Enjoy your browsing . ']\n",
      "['Here is the fish counter . Look at the lobsters and crabs . Shall we have some ? '\n",
      " \" I'm allergic to these things , you know . \"\n",
      " ' Sorry , I forgot . I don ’ t like seafood , neither . '\n",
      " ' Let ’ s go over there and get some milk , a couple dozen eggs and some orange juice . '\n",
      " \" Let's get frozen juice . It is really good . We ’ Ve got enough food . Let ’ s go over to the check-out stand . \"\n",
      " ' OK . But just let me pick up a bottle of cooking wine and oil as we go by . ']\n",
      "['Good morning , may I speak with Professor Clark , please ? '\n",
      " ' You are speaking with Professor Clark . '\n",
      " ' Professor , I am Kalina from your morning literature class . '\n",
      " ' Yes , how can I help you ? '\n",
      " ' I ran my car into a tree yesterday and need to miss a few days of school . '\n",
      " ' Oh , my God ! I hope you are all right . '\n",
      " ' I have a concussion , but I will be OK . '\n",
      " ' How much school will you miss ? '\n",
      " ' I only need to take this week off . '\n",
      " ' I appreciate you calling and telling me that you won ’ t be in class . See you next week ! ']\n",
      "['Hello , Parker . How ’ s everything ? ' ' Can ’ t complain . And you ? '\n",
      " ' Business is booming . I understand you want to meet up with me next week . How ’ s your schedule looking ? '\n",
      " ' Let me see . I can come out and see you first thing Wednesday . '\n",
      " ' Great . ']\n",
      "['How come it is slow as a snail today ? '\n",
      " ' You mean the network connection ? '\n",
      " ' Yes , I wanted to look for some information on the company page just now . It took me almost one minute to open it . Then there is no response to any click . '\n",
      " ' I have the same question . I can ’ t send out mails . We ’ d better call the IT department and ask them to check it immediately . '\n",
      " ' Ok . ']\n",
      "['Honey , I need to have a talk with you . '\n",
      " ' Dad , I have to do my homework . '\n",
      " \" No , honey , why didn't you go to cram school last night ? \"\n",
      " \" Dad , I don't want to talk about it now . \"\n",
      " \" Honey , if you don't want to go to cram school , you should tell me the reason why . \"\n",
      " \" I'm sorry , dad . But I would rather stay at school than go to cram school . \"]\n",
      "[\"What's wrong with you , young man ? \"\n",
      " ' Doctor , I have a bad cough and a headache . '\n",
      " ' Do you have a fever ? ' \" I don't know , but I feel terrible . \"\n",
      " \" Let me examine you . Don't worry . It's nothing serious . \"\n",
      " ' Do you think I should lie in bed ? '\n",
      " ' Yes , stay in bed and drink a lot of water . Your fever will be gone in a day or two . '\n",
      " ' OK . Do you think I can play football tomorrow ? '\n",
      " ' Of course not . You need a good rest . ' \" OK , I'll listen to you . \"]\n",
      "['I want something sweet after dinner . ' ' What do you have in mind ? '\n",
      " ' A dessert sounds nice . ' ' What kind are you thinking of getting ? '\n",
      " ' I want to get some pie . ' ' What kind of pie do you want ? '\n",
      " ' I have no idea . ' ' Do you want to know what kind of pie I like ? '\n",
      " ' Sure , what kind do you like ? ' ' I love apple pie . '\n",
      " ' Oh , I love apple pie too . ' ' There you go . Problem solved . ']\n",
      "[\"Let's go now . \" \" I'll be with you in a minute . \"]\n"
     ]
    }
   ],
   "source": [
    "for dialogue in test_df['dialog'][101: 110]:\n",
    "    print(dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3388fda7-6bb1-47a7-9859-29e32e2f1ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    11118.000000\n",
      "mean         7.840439\n",
      "std          4.007963\n",
      "min          2.000000\n",
      "25%          4.000000\n",
      "50%          7.000000\n",
      "75%         10.000000\n",
      "max         35.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Cheking lengths of dialogs\n",
    "lengths = []\n",
    "for dialogue in train_df['dialog']:\n",
    "    length = len(dialogue)\n",
    "    lengths.append(length)\n",
    "\n",
    "lengths_series = pd.Series(lengths)\n",
    "print(lengths_series.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89db0e3-aa1c-4889-9e39-aabc148ee2ca",
   "metadata": {},
   "source": [
    "### Clean data\n",
    "For the first such project I will not use additional data provided such as 'act' and 'emotion'. I will use my own data split, therefore I will concatinate all data and take only dialogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "069c8ab6-7013-44c0-88ac-e194286af9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Say , Jim , how about going for a few beers a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Can you do push-ups ? ,  Of course I can . It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Can you study with the radio on ? ,  No , I l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Are you all right ? ,  I will be all right so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Hey John , nice skates . Are they new ? ,  Ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13113</th>\n",
       "      <td>[Frank ’ s getting married , do you believe th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13114</th>\n",
       "      <td>[OK . Come back into the classroom , class . ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13115</th>\n",
       "      <td>[Do you have any hobbies ? ,  Yes , I like col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13116</th>\n",
       "      <td>[Jenny , what's wrong with you ? Why do you ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13117</th>\n",
       "      <td>[What a nice day ! ,  yes . How about going ou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13118 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  dialog\n",
       "0      [Say , Jim , how about going for a few beers a...\n",
       "1      [Can you do push-ups ? ,  Of course I can . It...\n",
       "2      [Can you study with the radio on ? ,  No , I l...\n",
       "3      [Are you all right ? ,  I will be all right so...\n",
       "4      [Hey John , nice skates . Are they new ? ,  Ye...\n",
       "...                                                  ...\n",
       "13113  [Frank ’ s getting married , do you believe th...\n",
       "13114  [OK . Come back into the classroom , class . ,...\n",
       "13115  [Do you have any hobbies ? ,  Yes , I like col...\n",
       "13116  [Jenny , what's wrong with you ? Why do you ke...\n",
       "13117  [What a nice day ! ,  yes . How about going ou...\n",
       "\n",
       "[13118 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dialogs = pd.concat([train_df['dialog'], val_df['dialog'], test_df['dialog']], ignore_index=True)\n",
    "data = pd.DataFrame({'dialog': all_dialogs})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e04d54-13e8-444c-a08b-a16880b5abb1",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d06d6cf2-c98b-433b-9746-939d4b24ebfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of single dialog: <class 'numpy.ndarray'>\n",
      "The type of the sentence within dialog: <class 'str'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 13118 entries, 0 to 13117\n",
      "Series name: dialog\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "13118 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 102.6+ KB\n"
     ]
    }
   ],
   "source": [
    "print(f\"The type of single dialog: {type(data['dialog'][0])}\")\n",
    "print(f\"The type of the sentence within dialog: {type(data['dialog'][0][0])}\")\n",
    "data['dialog'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed01a1ad-359f-4db9-b2b5-9f810676cf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dialogs are ndarrays, my preprocessing funcions are for strings\n",
    "def preprocess_text_array(arr):\n",
    "    dialog = arr.tolist()\n",
    "    return [preprocess_text(text) for text in dialog]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b4a97ad-9000-42e9-ab96-8ee621bda8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialog</th>\n",
       "      <th>preprocessed_dialog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Say , Jim , how about going for a few beers a...</td>\n",
       "      <td>[sofs say , , how about going for a few beers ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Can you do push-ups ? ,  Of course I can . It...</td>\n",
       "      <td>[sofs can you do push ups ? eofs, sofs of cour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Can you study with the radio on ? ,  No , I l...</td>\n",
       "      <td>[sofs can you study with the radio on ? eofs, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Are you all right ? ,  I will be all right so...</td>\n",
       "      <td>[sofs are you all right ? eofs, sofs i will be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Hey John , nice skates . Are they new ? ,  Ye...</td>\n",
       "      <td>[sofs , nice skates . are they new ? eofs, sof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13113</th>\n",
       "      <td>[Frank ’ s getting married , do you believe th...</td>\n",
       "      <td>[sofs married , do you believe this ? eofs, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13114</th>\n",
       "      <td>[OK . Come back into the classroom , class . ,...</td>\n",
       "      <td>[sofs ok . come back into the classroom , clas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13115</th>\n",
       "      <td>[Do you have any hobbies ? ,  Yes , I like col...</td>\n",
       "      <td>[sofs do you have any hobbies ? eofs, sofs yes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13116</th>\n",
       "      <td>[Jenny , what's wrong with you ? Why do you ke...</td>\n",
       "      <td>[sofs , what 's wrong with you ? why do you ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13117</th>\n",
       "      <td>[What a nice day ! ,  yes . How about going ou...</td>\n",
       "      <td>[sofs what a nice day ! eofs, sofs yes . how a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13118 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  dialog  \\\n",
       "0      [Say , Jim , how about going for a few beers a...   \n",
       "1      [Can you do push-ups ? ,  Of course I can . It...   \n",
       "2      [Can you study with the radio on ? ,  No , I l...   \n",
       "3      [Are you all right ? ,  I will be all right so...   \n",
       "4      [Hey John , nice skates . Are they new ? ,  Ye...   \n",
       "...                                                  ...   \n",
       "13113  [Frank ’ s getting married , do you believe th...   \n",
       "13114  [OK . Come back into the classroom , class . ,...   \n",
       "13115  [Do you have any hobbies ? ,  Yes , I like col...   \n",
       "13116  [Jenny , what's wrong with you ? Why do you ke...   \n",
       "13117  [What a nice day ! ,  yes . How about going ou...   \n",
       "\n",
       "                                     preprocessed_dialog  \n",
       "0      [sofs say , , how about going for a few beers ...  \n",
       "1      [sofs can you do push ups ? eofs, sofs of cour...  \n",
       "2      [sofs can you study with the radio on ? eofs, ...  \n",
       "3      [sofs are you all right ? eofs, sofs i will be...  \n",
       "4      [sofs , nice skates . are they new ? eofs, sof...  \n",
       "...                                                  ...  \n",
       "13113  [sofs married , do you believe this ? eofs, so...  \n",
       "13114  [sofs ok . come back into the classroom , clas...  \n",
       "13115  [sofs do you have any hobbies ? eofs, sofs yes...  \n",
       "13116  [sofs , what 's wrong with you ? why do you ke...  \n",
       "13117  [sofs what a nice day ! eofs, sofs yes . how a...  \n",
       "\n",
       "[13118 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[:, \"preprocessed_dialog\"] = data.loc[:, \"dialog\"].apply(preprocess_text_array)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6073ee61-beb8-4c5f-b169-64660f0abfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sofs please excuse me , but i really have to be going . eofs', 'sofs yes , of course . it was nice to see you . eofs', 'sofs it was nice to see you , too . and please give my eofs']\n",
      "['sofs excuse me . is this seat taken ? eofs', 'sofs i am afraid so . eofs']\n",
      "['sofs what do you think of the coming match ? eofs', 'sofs winning is a piece of cake to me . eofs', 'sofs you are bragging again . eofs']\n",
      "['sofs what would you reckon the taxing increases ? eofs', 'sofs well , the state will benefit a lot , i suppose . eofs', 'sofs but what do most people think about it ? eofs', 'sofs ah , it s hard to say . eofs']\n",
      "['sofs are you still coming to my place for dinner tomorrow night ? eofs', 'sofs of course . is the dinner still on ? eofs', 'sofs yes , i was just wondering how you and your roommate were planning eofs', 'sofs we were planning on walking both ways since the weather is still nice eofs', \"sofs that 's what i thought you would do . listen , i live eofs\", 'sofs it can not be that bad . eofs', 'sofs i wish it was not , but there is actually a lot of eofs', 'sofs really ? i never would have guessed . the criminals must only come eofs', \"sofs do me a favor , and take a taxi . it 'd make eofs\", 'sofs ok , we will . how do you get around in the evenings eofs', 'sofs when i first moved in , i walked everywhere . but within a eofs', 'sofs has anything else happened to you ? eofs', 'sofs nothing else has happened to me , but i have seen quite a eofs', 'sofs well , we will be careful . thanks for letting me know . eofs']\n",
      "['sofs wonders whether likes him or not . eofs', 'sofs why does not he ask her ? eofs', 'sofs he is too scared to ask her . eofs', 'sofs he is a chicken guy . eofs']\n",
      "['sofs i do not understand why some parents keep beefing and complaining about their eofs', 'sofs yeah . mother has been building a fire under her since her neighbour eofs', 'sofs if i were , i would ask her if she had done that eofs', 'sofs she is as meek as a lamb . she never goes against anyone eofs']\n",
      "['sofs where is ? i can not find him anywhere . eofs', 'sofs have not you heard that he is in prison ? eofs', 'sofs what ? beg your pardon . eofs', 'sofs is in prison now . he was copped outstealing . eofs', 'sofs i just can not believe my ears ! eofs']\n",
      "['sofs what do you need ? eofs', 'sofs i need to use the internet . eofs', 'sofs you have your library card , right ? eofs', 'sofs yes , i do . eofs', 'sofs there is a wait right now to use the computers . eofs', 'sofs that s fine . eofs', 'sofs would you please write your name on this list ? eofs', 'sofs then what ? eofs', 'sofs i will call you when a computer is free . eofs', 'sofs how do i log on to the computer ? eofs', 'sofs use the number on the back of your library card . eofs', 'sofs thanks . i ll be sitting over there . eofs']\n"
     ]
    }
   ],
   "source": [
    "for dialogue in data['preprocessed_dialog'][121: 130]:\n",
    "    print(dialogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1662a9-d4df-47e1-aa98-9327309fa488",
   "metadata": {},
   "source": [
    "### Pairing messages - input with responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f7c7c5c-5ded-472c-a5dd-1823a6085050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create input-response pairs\n",
    "def create_pairs(dialogues):\n",
    "    input_responses = []\n",
    "    for dialogue in dialogues:\n",
    "        for i in range(len(dialogue) - 1):\n",
    "            input_responses.append((dialogue[i], dialogue[i + 1]))\n",
    "    return input_responses\n",
    "\n",
    "# Create input-response pairs\n",
    "pairs = create_pairs(data['preprocessed_dialog'])\n",
    "\n",
    "# Convert pairs to DataFrame\n",
    "pairs_df = pd.DataFrame(pairs, columns=['input', 'response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bd0c93a-101d-4ef5-af34-9324b1be7a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sofs say , , how about going for a few beers a...</td>\n",
       "      <td>sofs you know that is tempting but is really n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sofs you know that is tempting but is really n...</td>\n",
       "      <td>sofs what do you mean ? it will help us to rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sofs what do you mean ? it will help us to rel...</td>\n",
       "      <td>sofs do you really think so ? i do not . it wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sofs do you really think so ? i do not . it wi...</td>\n",
       "      <td>sofs i guess you are right . but what shall we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sofs i guess you are right . but what shall we...</td>\n",
       "      <td>sofs i suggest a walk over to the gym where we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89856</th>\n",
       "      <td>sofs why not go again to celebrate out one yea...</td>\n",
       "      <td>sofs are you kidding ? can you afford it ? do ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89857</th>\n",
       "      <td>sofs are you kidding ? can you afford it ? do ...</td>\n",
       "      <td>sofs never mind that , i will take care of it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89858</th>\n",
       "      <td>sofs never mind that , i will take care of it ...</td>\n",
       "      <td>sofs yeah , i think so . eofs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89859</th>\n",
       "      <td>sofs yeah , i think so . eofs</td>\n",
       "      <td>sofs ok . i will make the arrangements . it wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89860</th>\n",
       "      <td>sofs ok . i will make the arrangements . it wi...</td>\n",
       "      <td>sofs wonderful ! i will start packing our suit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89861 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input  \\\n",
       "0      sofs say , , how about going for a few beers a...   \n",
       "1      sofs you know that is tempting but is really n...   \n",
       "2      sofs what do you mean ? it will help us to rel...   \n",
       "3      sofs do you really think so ? i do not . it wi...   \n",
       "4      sofs i guess you are right . but what shall we...   \n",
       "...                                                  ...   \n",
       "89856  sofs why not go again to celebrate out one yea...   \n",
       "89857  sofs are you kidding ? can you afford it ? do ...   \n",
       "89858  sofs never mind that , i will take care of it ...   \n",
       "89859                      sofs yeah , i think so . eofs   \n",
       "89860  sofs ok . i will make the arrangements . it wi...   \n",
       "\n",
       "                                                response  \n",
       "0      sofs you know that is tempting but is really n...  \n",
       "1      sofs what do you mean ? it will help us to rel...  \n",
       "2      sofs do you really think so ? i do not . it wi...  \n",
       "3      sofs i guess you are right . but what shall we...  \n",
       "4      sofs i suggest a walk over to the gym where we...  \n",
       "...                                                  ...  \n",
       "89856  sofs are you kidding ? can you afford it ? do ...  \n",
       "89857  sofs never mind that , i will take care of it ...  \n",
       "89858                      sofs yeah , i think so . eofs  \n",
       "89859  sofs ok . i will make the arrangements . it wi...  \n",
       "89860  sofs wonderful ! i will start packing our suit...  \n",
       "\n",
       "[89861 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac0374a5-80f3-4d8f-9f8f-234190f4e2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    89861.000000\n",
      "mean        12.044235\n",
      "std          3.269560\n",
      "min          2.000000\n",
      "25%          9.000000\n",
      "50%         13.000000\n",
      "75%         15.000000\n",
      "max         15.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Cheking length of sentences\n",
    "lengths = []\n",
    "for sentence in pairs_df['input']:\n",
    "    length = len(sentence.split())\n",
    "    lengths.append(length)\n",
    "\n",
    "lengths_series = pd.Series(lengths)\n",
    "print(lengths_series.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2abf70bf-f25f-4102-9793-a6a8c7f244f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [input, response]\n",
      "Index: []\n",
      "sofs besides these , the center staff also works with community organizations which provide eofs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_monologues = pairs_df[pairs_df['input'].str.split().str.len() > 30]\n",
    "print(long_monologues)\n",
    "print(pairs_df['input'][89769])\n",
    "len(long_monologues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eed6d9-5b38-439b-8a95-bd835bb4d588",
   "metadata": {},
   "source": [
    "### Initialize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d16fef4-a5db-45e2-99c5-a71e9fecfcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 15 most frequent words:\n",
      " [('sofs', 179722), ('eofs', 179722), ('.', 141525), ('i', 80118), (',', 75544), ('you', 67065), ('?', 51858), ('the', 45147), ('to', 38350), ('a', 34425), ('it', 32654), ('is', 31942), ('that', 22683), ('do', 21815), ('have', 21121)]\n",
      "\n",
      "Last 100 words:\n",
      " [(\"frills'business\", 1), ('lifts', 1), ('drills', 1), ('ty', 1), ('grudge', 1), ('pilferage', 1), ('irritation', 1), ('antiseptic', 1), (\"the'great\", 1), (\"outdoors'is\", 1), ('crouch', 1), ('labyrinth', 1), ('burns', 1), ('targets', 1), ('huangguoshu', 1), ('communicational', 1), ('assiduously', 1), ('rarest', 1), ('codes', 1), ('dongle', 1), ('resolving', 1), ('multitasking', 1), ('constipation', 1), ('recarpeted', 1), ('uncite', 1), ('goodnight', 1), ('singers', 1), ('reunification', 1), ('yearning', 1), ('testimonials', 1), ('informing', 1), ('robson', 1), ('macchiato', 1), ('backers', 1), ('montezuma', 1), ('revenge', 1), ('thans', 1), ('ultra', 1), ('brushed', 1), ('titanium', 1), ('kaohsiung', 1), ('mousaka', 1), ('floral', 1), ('apologise', 1), ('distributed', 1), ('judgement', 1), ('transported', 1), ('workstations', 1), ('trucks', 1), ('outsmart', 1), ('thrilled', 1), ('tienda', 1), ('familiarise', 1), ('chunks', 1), ('upfront', 1), ('cctv', 1), ('discreet', 1), ('warned', 1), ('demographics', 1), ('byeb', 1), ('formalities', 1), ('chute', 1), ('draws', 1), ('neighborhoods', 1), ('redouble', 1), ('transplant', 1), ('fossil', 1), ('fuels', 1), ('distractions', 1), ('gustave', 1), ('hells', 1), ('canvas', 1), ('crescive', 1), ('lpt', 1), ('evades', 1), ('detection', 1), ('fortnightly', 1), ('trifle', 1), ('overtake', 1), ('nickels', 1), ('aider', 1), ('copywriters', 1), ('translations', 1), ('aspirins', 1), ('crashes', 1), ('streeter', 1), ('kleenex', 1), ('innovation', 1), ('sealed', 1), ('angel', 1), ('grownups', 1), ('telecommuting', 1), ('incarnate', 1), ('extraordinaire', 1), ('interact', 1), ('wined', 1), ('dined', 1), ('scuba', 1), ('hopeless', 1), ('aesthetics', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000, filters=' ')\n",
    "tokenizer.fit_on_texts(pairs_df['input'].tolist() + pairs_df['response'].tolist())\n",
    "\n",
    "# Sort the word_counts dictionary by frequency in descending order\n",
    "sorted_word_counts = OrderedDict(sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "print(f\"\\nTop 15 most frequent words:\\n {list(sorted_word_counts.items())[:15]}\")\n",
    "print(f\"\\nLast 100 words:\\n {list(sorted_word_counts.items())[-100:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a276c-6264-4bee-a0fb-067f04b6b4c7",
   "metadata": {},
   "source": [
    "### Filter rare words - 10000 vocabulary OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9dd5de7a-fe72-4754-92cc-76b214d7fe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words before filtering: 14190\n",
      "Total words after filtering: 9537\n",
      "\n",
      "Top 15 most frequent words:\n",
      " [('sofs', 179722), ('eofs', 179722), ('.', 141525), ('i', 80118), (',', 75544), ('you', 67065), ('?', 51858), ('the', 45147), ('to', 38350), ('a', 34425), ('it', 32654), ('is', 31942), ('that', 22683), ('do', 21815), ('have', 21121)]\n",
      "\n",
      "Last 100 words:\n",
      " [('trends', 3), ('chan', 3), ('greater', 3), ('workmanship', 3), ('creditor', 3), ('equity', 3), ('curd', 3), ('jeep', 3), ('theft', 3), ('danshui', 3), ('believer', 3), ('disaster', 3), ('truant', 3), ('clarity', 3), ('blanca', 3), ('frappuccino', 3), ('trail', 3), ('flames', 3), ('unfit', 3), ('trev', 3), ('pamphlets', 3), ('hasty', 3), ('expire', 3), ('overcoming', 3), ('banked', 3), ('unmarried', 3), ('scaring', 3), ('seniors', 3), ('alex', 3), ('stylus', 3), ('leaky', 3), ('advisor', 3), ('realise', 3), ('occasional', 3), ('introductory', 3), ('lithium', 3), ('bluemingdails', 3), ('spectator', 3), ('fang', 3), ('insight', 3), ('spotless', 3), ('prosperous', 3), ('al', 3), ('stefan', 3), ('skateboard', 3), ('cinemas', 3), ('offline', 3), ('dictation', 3), ('memorandum', 3), ('discoveries', 3), ('workable', 3), ('judging', 3), ('bristles', 3), ('ahem', 3), ('arranging', 3), ('gates', 3), ('inventors', 3), ('coating', 3), ('turnaround', 3), ('handcrafts', 3), ('interrupted', 3), ('firmly', 3), ('precision', 3), ('corresponding', 3), ('speedy', 3), (\"i'li\", 3), ('ministry', 3), ('weed', 3), ('scented', 3), ('talker', 3), ('frying', 3), ('kiwis', 3), ('typewriters', 3), ('metropolis', 3), ('certainty', 3), ('mentality', 3), ('ratings', 3), ('usher', 3), ('scottish', 3), ('liabilities', 3), ('herbal', 3), ('confiscated', 3), ('clears', 3), ('juices', 3), ('technoledge', 3), ('bicycles', 3), ('poem', 3), ('tutoring', 3), ('fortunes', 3), ('groves', 3), ('chessboard', 3), ('hurdle', 3), ('independently', 3), ('exhilarating', 3), ('whiter', 3), ('weeding', 3), ('salesmen', 3), ('cartridge', 3), ('honesty', 3), ('banged', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Just reviewing what are the last words in vocabulary - do they still usable and recognizable\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Set a frequency threshold\n",
    "threshold = 3\n",
    "\n",
    "# Filter out rare words\n",
    "filtered_words = {word: count for word, count in sorted_word_counts.items() if count >= threshold}\n",
    "\n",
    "# Display the number of words before and after filtering\n",
    "print(f\"Total words before filtering: {len(sorted_word_counts)}\")\n",
    "print(f\"Total words after filtering: {len(filtered_words)}\")\n",
    "# Display the sorted word counts\n",
    "print(f\"\\nTop 15 most frequent words:\\n {list(filtered_words.items())[:15]}\")\n",
    "print(f\"\\nLast 100 words:\\n {list(filtered_words.items())[-100:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb4a376-b87e-4630-a88c-12d22b9d3a2b",
   "metadata": {},
   "source": [
    "### Save the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca8be1de-b1f7-4d93-897b-6ef1908fc475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved to C:\\Users\\tomui\\Desktop\\daily_dialogue\\data_dd\\tokenizer_dd.pickle\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Determine the directory where the tokenizer will be saved\n",
    "data_dir = os.path.join(os.getcwd(), 'data_dd')\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Save the tokenizer using pickle\n",
    "tokenizer_path = os.path.join(data_dir, 'tokenizer_dd.pickle')\n",
    "with open(tokenizer_path, 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(f\"Tokenizer saved to {tokenizer_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5deb6d2-695b-4025-b6b1-72428d051053",
   "metadata": {},
   "source": [
    "### Load the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5428c87-4ce6-480a-bb32-48f7a6063e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from file\n",
    "data_dir = os.path.join(os.getcwd(), 'data_dd')\n",
    "tokenizer_path = os.path.join(data_dir, 'tokenizer_dd.pickle')\n",
    "with open(tokenizer_path, 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9df411-ff49-4a67-802e-bd0dbca3ba3f",
   "metadata": {},
   "source": [
    "### Converting to indices and input-target sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df1d74ff-4be8-4b14-8083-ddbce2032e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert texts to sequences\n",
    "input_sequences = tokenizer.texts_to_sequences(pairs_df['input'])\n",
    "target_sequences = tokenizer.texts_to_sequences(pairs_df['response'])\n",
    "\n",
    "# Pad sequences with pre-padding and truncating\n",
    "input_padded = pad_sequences(input_sequences, maxlen=max_length, padding='pre', truncating='post')\n",
    "target_padded = pad_sequences(target_sequences, maxlen=max_length, padding='pre', truncating='post')\n",
    "\n",
    "# Store numpy arrays directly in the DataFrame\n",
    "pairs_df['Padded_Input_Sequences'] = input_padded.tolist()\n",
    "pairs_df['Padded_Target_Sequences'] = target_padded.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "685d7313-700b-4e5f-a1c4-34d33b8e0155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>response</th>\n",
       "      <th>Padded_Input_Sequences</th>\n",
       "      <th>Padded_Target_Sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sofs say , , how about going for a few beers a...</td>\n",
       "      <td>sofs you know that is tempting but is really n...</td>\n",
       "      <td>[1, 138, 5, 5, 36, 41, 77, 22, 10, 211, 3423, ...</td>\n",
       "      <td>[1, 6, 49, 13, 12, 3335, 32, 12, 58, 16, 48, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sofs you know that is tempting but is really n...</td>\n",
       "      <td>sofs what do you mean ? it will help us to rel...</td>\n",
       "      <td>[1, 6, 49, 13, 12, 3335, 32, 12, 58, 16, 48, 2...</td>\n",
       "      <td>[0, 1, 19, 14, 6, 155, 7, 11, 25, 105, 95, 9, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sofs what do you mean ? it will help us to rel...</td>\n",
       "      <td>sofs do you really think so ? i do not . it wi...</td>\n",
       "      <td>[0, 1, 19, 14, 6, 155, 7, 11, 25, 105, 95, 9, ...</td>\n",
       "      <td>[1, 14, 6, 58, 46, 43, 7, 4, 14, 16, 3, 11, 25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sofs do you really think so ? i do not . it wi...</td>\n",
       "      <td>sofs i guess you are right . but what shall we...</td>\n",
       "      <td>[1, 14, 6, 58, 46, 43, 7, 4, 14, 16, 3, 11, 25...</td>\n",
       "      <td>[1, 4, 217, 6, 18, 55, 3, 32, 19, 304, 23, 14,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sofs i guess you are right . but what shall we...</td>\n",
       "      <td>sofs i suggest a walk over to the gym where we...</td>\n",
       "      <td>[1, 4, 217, 6, 18, 55, 3, 32, 19, 304, 23, 14,...</td>\n",
       "      <td>[1, 4, 639, 10, 434, 144, 9, 8, 978, 107, 23, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89856</th>\n",
       "      <td>sofs why not go again to celebrate out one yea...</td>\n",
       "      <td>sofs are you kidding ? can you afford it ? do ...</td>\n",
       "      <td>[1, 82, 16, 64, 213, 9, 1719, 89, 65, 222, 159...</td>\n",
       "      <td>[1, 18, 6, 563, 7, 24, 6, 994, 11, 7, 14, 6, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89857</th>\n",
       "      <td>sofs are you kidding ? can you afford it ? do ...</td>\n",
       "      <td>sofs never mind that , i will take care of it ...</td>\n",
       "      <td>[1, 18, 6, 563, 7, 24, 6, 994, 11, 7, 14, 6, 4...</td>\n",
       "      <td>[1, 177, 212, 13, 5, 4, 25, 76, 349, 20, 11, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89858</th>\n",
       "      <td>sofs never mind that , i will take care of it ...</td>\n",
       "      <td>sofs yeah , i think so . eofs</td>\n",
       "      <td>[1, 177, 212, 13, 5, 4, 25, 76, 349, 20, 11, 3...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 104, 5, 4, 46, 43, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89859</th>\n",
       "      <td>sofs yeah , i think so . eofs</td>\n",
       "      <td>sofs ok . i will make the arrangements . it wi...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 104, 5, 4, 46, 43, 3, 2]</td>\n",
       "      <td>[1, 70, 3, 4, 25, 110, 8, 3714, 3, 11, 25, 37,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89860</th>\n",
       "      <td>sofs ok . i will make the arrangements . it wi...</td>\n",
       "      <td>sofs wonderful ! i will start packing our suit...</td>\n",
       "      <td>[1, 70, 3, 4, 25, 110, 8, 3714, 3, 11, 25, 37,...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 381, 29, 4, 25, 284, 1732, 90,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89861 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input  \\\n",
       "0      sofs say , , how about going for a few beers a...   \n",
       "1      sofs you know that is tempting but is really n...   \n",
       "2      sofs what do you mean ? it will help us to rel...   \n",
       "3      sofs do you really think so ? i do not . it wi...   \n",
       "4      sofs i guess you are right . but what shall we...   \n",
       "...                                                  ...   \n",
       "89856  sofs why not go again to celebrate out one yea...   \n",
       "89857  sofs are you kidding ? can you afford it ? do ...   \n",
       "89858  sofs never mind that , i will take care of it ...   \n",
       "89859                      sofs yeah , i think so . eofs   \n",
       "89860  sofs ok . i will make the arrangements . it wi...   \n",
       "\n",
       "                                                response  \\\n",
       "0      sofs you know that is tempting but is really n...   \n",
       "1      sofs what do you mean ? it will help us to rel...   \n",
       "2      sofs do you really think so ? i do not . it wi...   \n",
       "3      sofs i guess you are right . but what shall we...   \n",
       "4      sofs i suggest a walk over to the gym where we...   \n",
       "...                                                  ...   \n",
       "89856  sofs are you kidding ? can you afford it ? do ...   \n",
       "89857  sofs never mind that , i will take care of it ...   \n",
       "89858                      sofs yeah , i think so . eofs   \n",
       "89859  sofs ok . i will make the arrangements . it wi...   \n",
       "89860  sofs wonderful ! i will start packing our suit...   \n",
       "\n",
       "                                  Padded_Input_Sequences  \\\n",
       "0      [1, 138, 5, 5, 36, 41, 77, 22, 10, 211, 3423, ...   \n",
       "1      [1, 6, 49, 13, 12, 3335, 32, 12, 58, 16, 48, 2...   \n",
       "2      [0, 1, 19, 14, 6, 155, 7, 11, 25, 105, 95, 9, ...   \n",
       "3      [1, 14, 6, 58, 46, 43, 7, 4, 14, 16, 3, 11, 25...   \n",
       "4      [1, 4, 217, 6, 18, 55, 3, 32, 19, 304, 23, 14,...   \n",
       "...                                                  ...   \n",
       "89856  [1, 82, 16, 64, 213, 9, 1719, 89, 65, 222, 159...   \n",
       "89857  [1, 18, 6, 563, 7, 24, 6, 994, 11, 7, 14, 6, 4...   \n",
       "89858  [1, 177, 212, 13, 5, 4, 25, 76, 349, 20, 11, 3...   \n",
       "89859  [0, 0, 0, 0, 0, 0, 0, 1, 104, 5, 4, 46, 43, 3, 2]   \n",
       "89860  [1, 70, 3, 4, 25, 110, 8, 3714, 3, 11, 25, 37,...   \n",
       "\n",
       "                                 Padded_Target_Sequences  \n",
       "0      [1, 6, 49, 13, 12, 3335, 32, 12, 58, 16, 48, 2...  \n",
       "1      [0, 1, 19, 14, 6, 155, 7, 11, 25, 105, 95, 9, ...  \n",
       "2      [1, 14, 6, 58, 46, 43, 7, 4, 14, 16, 3, 11, 25...  \n",
       "3      [1, 4, 217, 6, 18, 55, 3, 32, 19, 304, 23, 14,...  \n",
       "4      [1, 4, 639, 10, 434, 144, 9, 8, 978, 107, 23, ...  \n",
       "...                                                  ...  \n",
       "89856  [1, 18, 6, 563, 7, 24, 6, 994, 11, 7, 14, 6, 4...  \n",
       "89857  [1, 177, 212, 13, 5, 4, 25, 76, 349, 20, 11, 3...  \n",
       "89858  [0, 0, 0, 0, 0, 0, 0, 1, 104, 5, 4, 46, 43, 3, 2]  \n",
       "89859  [1, 70, 3, 4, 25, 110, 8, 3714, 3, 11, 25, 37,...  \n",
       "89860  [0, 0, 0, 0, 1, 381, 29, 4, 25, 284, 1732, 90,...  \n",
       "\n",
       "[89861 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43feafb6-2b5a-4710-b070-258258f63e30",
   "metadata": {},
   "source": [
    "### Checking if conversion was successfull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26d4a424-9e01-44d3-a6a5-4df7eb46bf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text: sofs no , so it is usually boring to join my friends in the eofs \n",
      "Reconstructed Text: sofs no , so it is usually boring to join my friends in the eofs\n",
      "\n",
      "Original Text: sofs what kind of things would you like to see on the menu ? eofs \n",
      "Reconstructed Text: sofs what kind of things would you like to see on the menu ? eofs\n",
      "\n",
      "Original Text: sofs what kind of things would you like to see on the menu ? eofs \n",
      "Reconstructed Text: sofs what kind of things would you like to see on the menu ? eofs\n",
      "\n",
      "Original Text: sofs maybe a fruit salad and a few different hot sandwiches at least . eofs \n",
      "Reconstructed Text: sofs maybe a fruit salad and a few different hot sandwiches at least . eofs\n",
      "\n",
      "Original Text: sofs maybe a fruit salad and a few different hot sandwiches at least . eofs \n",
      "Reconstructed Text: sofs maybe a fruit salad and a few different hot sandwiches at least . eofs\n",
      "\n",
      "Original Text: sofs that should not be too difficult . since this is a small neighborhood eofs \n",
      "Reconstructed Text: sofs that should not be too difficult . since this is a small neighborhood eofs\n",
      "\n",
      "Original Text: sofs that should not be too difficult . since this is a small neighborhood eofs \n",
      "Reconstructed Text: sofs that should not be too difficult . since this is a small neighborhood eofs\n",
      "\n",
      "Original Text: sofs let us try it ! eofs \n",
      "Reconstructed Text: sofs let us try it ! eofs\n",
      "\n",
      "Original Text: sofs darling , i have news for you . and his wife , evelyn eofs \n",
      "Reconstructed Text: sofs darling , i have news for you . and his wife , evelyn eofs\n",
      "\n",
      "Original Text: sofs really ? i thought his wife couldn t have a baby . eofs \n",
      "Reconstructed Text: sofs really ? i thought his wife couldn t have a baby . eofs\n"
     ]
    }
   ],
   "source": [
    "def sequences_to_text(sequence):\n",
    "    index_to_word = {index: word for word, index in tokenizer.word_index.items()}\n",
    "    # Directly map sequence of indices back to words\n",
    "    return ' '.join(index_to_word.get(idx, '') for idx in sequence if idx != 0)\n",
    "\n",
    "# Print original and reverse-tokenized text for entries\n",
    "for index, row in pairs_df[1130:1135].iterrows():\n",
    "    print(\"\\nOriginal Text:\", row['input'], \n",
    "          \"\\nReconstructed Text:\", sequences_to_text(row['Padded_Input_Sequences']))\n",
    "    print(\"\\nOriginal Text:\", row['response'], \n",
    "          \"\\nReconstructed Text:\", sequences_to_text(row['Padded_Target_Sequences']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f96c6b5-a666-499a-b578-9639350909e3",
   "metadata": {},
   "source": [
    "## Saving the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3dddbeaa-4b10-44e1-a6f9-f3f2056ce421",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving the DataFrame\n",
    "data_dir = os.path.join(os.getcwd(), 'data_dd')\n",
    "file_path_parquet = os.path.join(data_dir, 'training_df_dd.parquet')\n",
    "pairs_df.to_parquet(file_path_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e0ec68-2723-436d-a101-ad18df9f0a63",
   "metadata": {},
   "source": [
    "## Loading the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d8036e60-f3a3-4beb-9f96-da6496797413",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>response</th>\n",
       "      <th>Padded_Input_Sequences</th>\n",
       "      <th>Padded_Target_Sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sofs say , , how about going for a few beers a...</td>\n",
       "      <td>sofs you know that is tempting but is really n...</td>\n",
       "      <td>[1, 138, 5, 5, 36, 41, 77, 22, 10, 211, 3423, ...</td>\n",
       "      <td>[1, 6, 49, 13, 12, 3335, 32, 12, 58, 16, 48, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sofs you know that is tempting but is really n...</td>\n",
       "      <td>sofs what do you mean ? it will help us to rel...</td>\n",
       "      <td>[1, 6, 49, 13, 12, 3335, 32, 12, 58, 16, 48, 2...</td>\n",
       "      <td>[0, 1, 19, 14, 6, 155, 7, 11, 25, 105, 95, 9, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sofs what do you mean ? it will help us to rel...</td>\n",
       "      <td>sofs do you really think so ? i do not . it wi...</td>\n",
       "      <td>[0, 1, 19, 14, 6, 155, 7, 11, 25, 105, 95, 9, ...</td>\n",
       "      <td>[1, 14, 6, 58, 46, 43, 7, 4, 14, 16, 3, 11, 25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sofs do you really think so ? i do not . it wi...</td>\n",
       "      <td>sofs i guess you are right . but what shall we...</td>\n",
       "      <td>[1, 14, 6, 58, 46, 43, 7, 4, 14, 16, 3, 11, 25...</td>\n",
       "      <td>[1, 4, 217, 6, 18, 55, 3, 32, 19, 304, 23, 14,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sofs i guess you are right . but what shall we...</td>\n",
       "      <td>sofs i suggest a walk over to the gym where we...</td>\n",
       "      <td>[1, 4, 217, 6, 18, 55, 3, 32, 19, 304, 23, 14,...</td>\n",
       "      <td>[1, 4, 639, 10, 434, 144, 9, 8, 978, 107, 23, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sofs i suggest a walk over to the gym where we...</td>\n",
       "      <td>sofs that 's a good idea . i hear mary and sal...</td>\n",
       "      <td>[1, 4, 639, 10, 434, 144, 9, 8, 978, 107, 23, ...</td>\n",
       "      <td>[1, 13, 34, 10, 48, 169, 3, 4, 229, 476, 17, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sofs that 's a good idea . i hear mary and sal...</td>\n",
       "      <td>sofs sounds great to me ! if they are willing ...</td>\n",
       "      <td>[1, 13, 34, 10, 48, 169, 3, 4, 229, 476, 17, 5...</td>\n",
       "      <td>[1, 142, 100, 9, 30, 29, 63, 61, 18, 1155, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sofs sounds great to me ! if they are willing ...</td>\n",
       "      <td>sofs good . let us go now . eofs</td>\n",
       "      <td>[1, 142, 100, 9, 30, 29, 63, 61, 18, 1155, 5, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 48, 3, 75, 95, 64, 80, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sofs good . let us go now . eofs</td>\n",
       "      <td>sofs all right . eofs</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 48, 3, 75, 95, 64, 80, 3...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 53, 55, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sofs can you do push ups ? eofs</td>\n",
       "      <td>sofs of course i can . it is a piece of cake !...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 24, 6, 14, 1595, 2298...</td>\n",
       "      <td>[1, 20, 119, 4, 24, 3, 11, 12, 10, 762, 20, 93...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  sofs say , , how about going for a few beers a...   \n",
       "1  sofs you know that is tempting but is really n...   \n",
       "2  sofs what do you mean ? it will help us to rel...   \n",
       "3  sofs do you really think so ? i do not . it wi...   \n",
       "4  sofs i guess you are right . but what shall we...   \n",
       "5  sofs i suggest a walk over to the gym where we...   \n",
       "6  sofs that 's a good idea . i hear mary and sal...   \n",
       "7  sofs sounds great to me ! if they are willing ...   \n",
       "8                   sofs good . let us go now . eofs   \n",
       "9                    sofs can you do push ups ? eofs   \n",
       "\n",
       "                                            response  \\\n",
       "0  sofs you know that is tempting but is really n...   \n",
       "1  sofs what do you mean ? it will help us to rel...   \n",
       "2  sofs do you really think so ? i do not . it wi...   \n",
       "3  sofs i guess you are right . but what shall we...   \n",
       "4  sofs i suggest a walk over to the gym where we...   \n",
       "5  sofs that 's a good idea . i hear mary and sal...   \n",
       "6  sofs sounds great to me ! if they are willing ...   \n",
       "7                   sofs good . let us go now . eofs   \n",
       "8                              sofs all right . eofs   \n",
       "9  sofs of course i can . it is a piece of cake !...   \n",
       "\n",
       "                              Padded_Input_Sequences  \\\n",
       "0  [1, 138, 5, 5, 36, 41, 77, 22, 10, 211, 3423, ...   \n",
       "1  [1, 6, 49, 13, 12, 3335, 32, 12, 58, 16, 48, 2...   \n",
       "2  [0, 1, 19, 14, 6, 155, 7, 11, 25, 105, 95, 9, ...   \n",
       "3  [1, 14, 6, 58, 46, 43, 7, 4, 14, 16, 3, 11, 25...   \n",
       "4  [1, 4, 217, 6, 18, 55, 3, 32, 19, 304, 23, 14,...   \n",
       "5  [1, 4, 639, 10, 434, 144, 9, 8, 978, 107, 23, ...   \n",
       "6  [1, 13, 34, 10, 48, 169, 3, 4, 229, 476, 17, 5...   \n",
       "7  [1, 142, 100, 9, 30, 29, 63, 61, 18, 1155, 5, ...   \n",
       "8  [0, 0, 0, 0, 0, 0, 1, 48, 3, 75, 95, 64, 80, 3...   \n",
       "9  [0, 0, 0, 0, 0, 0, 0, 1, 24, 6, 14, 1595, 2298...   \n",
       "\n",
       "                             Padded_Target_Sequences  \n",
       "0  [1, 6, 49, 13, 12, 3335, 32, 12, 58, 16, 48, 2...  \n",
       "1  [0, 1, 19, 14, 6, 155, 7, 11, 25, 105, 95, 9, ...  \n",
       "2  [1, 14, 6, 58, 46, 43, 7, 4, 14, 16, 3, 11, 25...  \n",
       "3  [1, 4, 217, 6, 18, 55, 3, 32, 19, 304, 23, 14,...  \n",
       "4  [1, 4, 639, 10, 434, 144, 9, 8, 978, 107, 23, ...  \n",
       "5  [1, 13, 34, 10, 48, 169, 3, 4, 229, 476, 17, 5...  \n",
       "6  [1, 142, 100, 9, 30, 29, 63, 61, 18, 1155, 5, ...  \n",
       "7  [0, 0, 0, 0, 0, 0, 1, 48, 3, 75, 95, 64, 80, 3...  \n",
       "8    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 53, 55, 3, 2]  \n",
       "9  [1, 20, 119, 4, 24, 3, 11, 12, 10, 762, 20, 93...  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the DataFrame\n",
    "data_dir = os.path.join(os.getcwd(), 'data_dd')\n",
    "file_path_parquet = os.path.join(data_dir, 'training_df_dd.parquet')\n",
    "training_data_final = pd.read_parquet(file_path_parquet)\n",
    "\n",
    "training_data_final.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ad6aab19-4df8-461d-9dc1-393d1157a83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14190"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3fbffc-0d50-455a-8d1a-2608dad745e3",
   "metadata": {},
   "source": [
    "### Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "39ff9d2f-e6b8-4c47-aeb5-5af9071ad308",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = np.array(training_data_final['Padded_Input_Sequences'].tolist())\n",
    "target_sequences = np.array(training_data_final['Padded_Target_Sequences'].tolist())\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "input_train, input_val, target_train, target_val = train_test_split(input_sequences, target_sequences, test_size=0.1, random_state=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076347eb-91fb-4830-a259-27e6241faa67",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a5feee9c-140d-4bf3-bd4b-fd7f576ce7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_16 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)        (None, None, 256)    2560000     ['input_15[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)        (None, None, 256)    2560000     ['input_16[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_6 (LSTM)                  [(None, 256),        525312      ['embedding_6[0][0]']            \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_7 (LSTM)                  [(None, None, 256),  525312      ['embedding_7[0][0]',            \n",
      "                                 (None, 256),                     'lstm_6[0][1]',                 \n",
      "                                 (None, 256)]                     'lstm_6[0][2]']                 \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, None, 10000)  2570000     ['lstm_7[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,740,624\n",
      "Trainable params: 8,740,624\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "latent_dim = 256\n",
    "num_encoder_tokens = np.max(input_sequences) + 1\n",
    "num_decoder_tokens = np.max(target_sequences) + 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding = tf.keras.layers.Embedding(num_encoder_tokens, latent_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = tf.keras.layers.Embedding(num_decoder_tokens, latent_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Define checkpoint callback\n",
    "checkpoint = ModelCheckpoint('seq2seq_dd_model.h5', save_best_only=False, monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe71e88b-5724-4c14-ada4-e7f7f2ceb383",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86cf1fe-85e1-4142-8c54-04d178b499e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1264/1264 [==============================] - 30s 21ms/step - loss: 3.6790 - val_loss: 3.1208\n",
      "Epoch 2/25\n",
      "1264/1264 [==============================] - 27s 21ms/step - loss: 2.9906 - val_loss: 2.9120\n",
      "Epoch 3/25\n",
      "1264/1264 [==============================] - 31s 24ms/step - loss: 2.8071 - val_loss: 2.8135\n",
      "Epoch 4/25\n",
      "1264/1264 [==============================] - 34s 27ms/step - loss: 2.6855 - val_loss: 2.7554\n",
      "Epoch 5/25\n",
      "1264/1264 [==============================] - 52s 41ms/step - loss: 2.5889 - val_loss: 2.7164\n",
      "Epoch 6/25\n",
      "1264/1264 [==============================] - 79s 62ms/step - loss: 2.5060 - val_loss: 2.6870\n",
      "Epoch 7/25\n",
      "1264/1264 [==============================] - 114s 90ms/step - loss: 2.4316 - val_loss: 2.6654\n",
      "Epoch 8/25\n",
      "1264/1264 [==============================] - 136s 107ms/step - loss: 2.3644 - val_loss: 2.6531\n",
      "Epoch 9/25\n",
      " 729/1264 [================>.............] - ETA: 55s - loss: 2.2963"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "history = model.fit(\n",
    "    [input_train, target_train[:, :-1]],\n",
    "    target_train[:, 1:, np.newaxis],\n",
    "    batch_size=64,\n",
    "    epochs=25,\n",
    "    validation_data=([input_val, target_val[:, :-1]], target_val[:, 1:, np.newaxis]),\n",
    "    # callbacks=[checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda22363-777c-4bac-b55c-b8ba47fd7175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c39826-1839-4bf9-9f32-9fc66393e49c",
   "metadata": {},
   "source": [
    "### Generating Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba8439-f79d-4b17-b107-f96d61e72dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoder model for inference\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Define decoder model for inference\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# Save token index mappings\n",
    "target_token_index = tokenizer.word_index\n",
    "reverse_target_token_index = {v: k for k, v in target_token_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171a9727-3484-4968-a1e8-b26e3dd695d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate responses\n",
    "def generate_response(input_seq: np.ndarray, max_decoder_seq_length: int) -> str:\n",
    "    # Encode the input sequence to get the internal states\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1 with only the start token\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = tokenizer.word_index['sofs']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token and add the corresponding character to the decoded sentence\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_token_index[sampled_token_index]\n",
    "        decoded_sentence += ' ' + sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length or find stop token\n",
    "        if (sampled_char == 'eofs' or len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence.strip('sofs').strip('eofs').strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0f56fc-ba3e-40fd-b965-ba93098dda02",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b68f62-8d82-4ea3-9061-cb798fcef3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_preprocessing = False  # Expects spaCy to detect and remove names from the text\n",
    "max_length = 15\n",
    "\n",
    "# Define ten examples to test the model\n",
    "test_examples = [\n",
    "    \"How are you doing today?\",\n",
    "    \"What is your name?\",\n",
    "    \"Can you help me with my homework?\",\n",
    "    \"What is the weather like?\",\n",
    "    \"Tell me a joke.\",\n",
    "    \"Who is the president of the United States?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Do you like pizza?\",\n",
    "    \"What is your favorite color?\",\n",
    "    \"Goodbye!\"\n",
    "]\n",
    "\n",
    "# Preprocess input text (assuming preprocess_text is defined elsewhere)\n",
    "input_text = [preprocess_text(text) for text in test_examples]\n",
    "print(f\"Preprocessed text: {input_text}\")\n",
    "\n",
    "# Tokenize and pad the test examples\n",
    "test_sequences = tokenizer.texts_to_sequences(input_text)\n",
    "print(f\"Tokenizer sequences: {test_sequences}\")\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=max_length, padding='pre', truncating='post')\n",
    "print(f\"Padded sequences: {padded_test_sequences}\")\n",
    "\n",
    "# Generate responses\n",
    "for test_seq in padded_test_sequences:\n",
    "    input_seq = np.array([test_seq])\n",
    "    response = generate_response(input_seq, max_length)\n",
    "    print(f\"Input: {test_examples[padded_test_sequences.tolist().index(test_seq.tolist())]}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
